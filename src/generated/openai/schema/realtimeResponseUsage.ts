/**
 * Generated by orval v7.9.0 üç∫
 * Do not edit manually.
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 * OpenAPI spec version: 2.3.0
 */
import type { RealtimeResponseUsageInputTokenDetails } from './realtimeResponseUsageInputTokenDetails';
import type { RealtimeResponseUsageOutputTokenDetails } from './realtimeResponseUsageOutputTokenDetails';

/**
 * Usage statistics for the Response, this will correspond to billing. A 
Realtime API session will maintain a conversation context and append new 
Items to the Conversation, thus output from previous turns (text and 
audio tokens) will become the input for later turns.

 */
export type RealtimeResponseUsage = {
  /** The total number of tokens in the Response including input and output 
text and audio tokens.
 */
  total_tokens?: number;
  /** The number of input tokens used in the Response, including text and 
audio tokens.
 */
  input_tokens?: number;
  /** The number of output tokens sent in the Response, including text and 
audio tokens.
 */
  output_tokens?: number;
  /** Details about the input tokens used in the Response. Cached tokens are tokens from previous turns in the conversation that are included as context for the current response. Cached tokens here are counted as a subset of input tokens, meaning input tokens will include cached and uncached tokens. */
  input_token_details?: RealtimeResponseUsageInputTokenDetails;
  /** Details about the output tokens used in the Response. */
  output_token_details?: RealtimeResponseUsageOutputTokenDetails;
};
