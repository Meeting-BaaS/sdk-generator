/**
 * Generated by orval v7.17.0 üç∫
 * Do not edit manually.
 * OpenAI API
 * The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.
 * OpenAPI spec version: 2.3.0
 */
import type { CreateChatCompletionStreamResponseChoicesItem } from "./createChatCompletionStreamResponseChoicesItem"
import type { ServiceTier } from "./serviceTier"
import type { CreateChatCompletionStreamResponseObject } from "./createChatCompletionStreamResponseObject"
import type { CompletionUsage } from "./completionUsage"

/**
 * Represents a streamed chunk of a chat completion response returned
by the model, based on the provided input. 
[Learn more](https://platform.openai.com/docs/guides/streaming-responses).

 */
export interface CreateChatCompletionStreamResponse {
  /** A unique identifier for the chat completion. Each chunk has the same ID. */
  id: string
  /** A list of chat completion choices. Can contain more than one elements if `n` is greater than 1. Can also be empty for the
last chunk if you set `stream_options: {"include_usage": true}`.
 */
  choices: CreateChatCompletionStreamResponseChoicesItem[]
  /** The Unix timestamp (in seconds) of when the chat completion was created. Each chunk has the same timestamp. */
  created: number
  /** The model to generate the completion. */
  model: string
  service_tier?: ServiceTier
  /**
   * This fingerprint represents the backend configuration that the model runs with.
Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.

   * @deprecated
   */
  system_fingerprint?: string
  /** The object type, which is always `chat.completion.chunk`. */
  object: CreateChatCompletionStreamResponseObject
  /**
   * An optional field that will only be present when you set
`stream_options: {"include_usage": true}` in your request. When present, it
contains a null value **except for the last chunk** which contains the
token usage statistics for the entire request.

**NOTE:** If the stream is interrupted or cancelled, you may not
receive the final usage chunk which contains the total token usage for
the request.

   * @nullable
   */
  usage?: CompletionUsage
}
