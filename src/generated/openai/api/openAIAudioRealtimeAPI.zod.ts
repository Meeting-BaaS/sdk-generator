/**
 * Generated by orval v7.9.0 üç∫
 * Do not edit manually.
 * OpenAI Audio & Realtime API
 * OpenAI Audio API - Transcription, Translation, Speech, and Realtime streaming endpoints. Filtered from the official OpenAI API spec (Stainless-hosted).
 * OpenAPI spec version: 2.3.0
 */
import {
  z as zod
} from 'zod';

/**
 * Generates audio from the input text.
 * @summary Create speech
 */
export const createSpeechBodyInputMax = 4096;
export const createSpeechBodyInstructionsMax = 4096;
export const createSpeechBodyResponseFormatDefault = "mp3";export const createSpeechBodySpeedDefault = 1;
export const createSpeechBodySpeedMin = 0.25;

export const createSpeechBodySpeedMax = 4;
export const createSpeechBodyStreamFormatDefault = "audio";

export const createSpeechBody = zod.object({
  "model": zod.string().or(zod.enum(['tts-1', 'tts-1-hd', 'gpt-4o-mini-tts', 'gpt-4o-mini-tts-2025-12-15'])).describe('One of the available [TTS models](https://platform.openai.com/docs/models#tts): `tts-1`, `tts-1-hd`, `gpt-4o-mini-tts`, or `gpt-4o-mini-tts-2025-12-15`.\n'),
  "input": zod.string().max(createSpeechBodyInputMax).describe('The text to generate audio for. The maximum length is 4096 characters.'),
  "instructions": zod.string().max(createSpeechBodyInstructionsMax).optional().describe('Control the voice of your generated audio with additional instructions. Does not work with `tts-1` or `tts-1-hd`.'),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])),
  "response_format": zod.enum(['mp3', 'opus', 'aac', 'flac', 'wav', 'pcm']).default(createSpeechBodyResponseFormatDefault).describe('The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`.'),
  "speed": zod.number().min(createSpeechBodySpeedMin).max(createSpeechBodySpeedMax).default(createSpeechBodySpeedDefault).describe('The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is the default.'),
  "stream_format": zod.enum(['sse', 'audio']).default(createSpeechBodyStreamFormatDefault).describe('The format to stream the audio in. Supported formats are `sse` and `audio`. `sse` is not supported for `tts-1` or `tts-1-hd`.')
})


/**
 * Transcribes audio into the input language.
 * @summary Create transcription
 */
export const createTranscriptionBodyResponseFormatDefault = "json";export const createTranscriptionBodyTemperatureDefault = 0;export const createTranscriptionBodyTimestampGranularitiesDefault: ('word' | 'segment')[] = ["segment"];export const createTranscriptionBodyStreamDefaultOne = false;export const createTranscriptionBodyChunkingStrategyDefaultTwo = "auto";export const createTranscriptionBodyChunkingStrategyPrefixPaddingMsDefault = 300;export const createTranscriptionBodyChunkingStrategySilenceDurationMsDefault = 200;export const createTranscriptionBodyChunkingStrategyThresholdDefault = 0.5;export const createTranscriptionBodyKnownSpeakerNamesMax = 4;
export const createTranscriptionBodyKnownSpeakerReferencesMax = 4;


export const createTranscriptionBody = zod.object({
  "file": zod.instanceof(File).describe('The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n'),
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-transcribe', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe-diarize'])).describe('ID of the model to use. The options are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `whisper-1` (which is powered by our open source Whisper V2 model), and `gpt-4o-transcribe-diarize`.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format will improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. This field is not supported when using `gpt-4o-transcribe-diarize`.\n'),
  "response_format": zod.enum(['json', 'text', 'srt', 'verbose_json', 'vtt', 'diarized_json']).default(createTranscriptionBodyResponseFormatDefault).describe('The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, `vtt`, or `diarized_json`. For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`, the only supported format is `json`. For `gpt-4o-transcribe-diarize`, the supported formats are `json`, `text`, and `diarized_json`, with `diarized_json` required to receive speaker annotations.\n'),
  "temperature": zod.number().optional().describe('The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.\n'),
  "include": zod.array(zod.enum(['logprobs'])).optional().describe('Additional information to include in the transcription response.\n`logprobs` will return the log probabilities of the tokens in the\nresponse to understand the model\'s confidence in the transcription.\n`logprobs` only works with response_format set to `json` and only with\nthe models `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `gpt-4o-mini-transcribe-2025-12-15`. This field is not supported when using `gpt-4o-transcribe-diarize`.\n'),
  "timestamp_granularities": zod.array(zod.enum(['word', 'segment'])).default(createTranscriptionBodyTimestampGranularitiesDefault).describe('The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.\nThis option is not available for `gpt-4o-transcribe-diarize`.\n'),
  "stream": zod.boolean().describe('If set to true, the model response data will be streamed to the client\nas it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).\nSee the [Streaming section of the Speech-to-Text guide](https://platform.openai.com/docs/guides/speech-to-text?lang=curl#streaming-transcriptions)\nfor more information.\n\nNote: Streaming is not supported for the `whisper-1` model and will be ignored.\n').or(zod.null()).optional(),
  "chunking_strategy": zod.enum(['auto']).describe('Automatically set chunking parameters based on the audio. Must be set to `\"auto\"`.\n').or(zod.object({
  "type": zod.enum(['server_vad']).describe('Must be set to `server_vad` to enable manual chunking using server side VAD.'),
  "prefix_padding_ms": zod.number().default(createTranscriptionBodyChunkingStrategyPrefixPaddingMsDefault).describe('Amount of audio to include before the VAD detected speech (in \nmilliseconds).\n'),
  "silence_duration_ms": zod.number().default(createTranscriptionBodyChunkingStrategySilenceDurationMsDefault).describe('Duration of silence to detect speech stop (in milliseconds).\nWith shorter values the model will respond more quickly, \nbut may jump in on short pauses from the user.\n'),
  "threshold": zod.number().default(createTranscriptionBodyChunkingStrategyThresholdDefault).describe('Sensitivity threshold (0.0 to 1.0) for voice activity detection. A \nhigher threshold will require louder audio to activate the model, and \nthus might perform better in noisy environments.\n')
})).describe('Controls how the audio is cut into chunks. When set to `\"auto\"`, the server first normalizes loudness and then uses voice activity detection (VAD) to choose boundaries. `server_vad` object can be provided to tweak VAD detection parameters manually. If unset, the audio is transcribed as a single block. Required when using `gpt-4o-transcribe-diarize` for inputs longer than 30 seconds. ').or(zod.null()).optional(),
  "known_speaker_names": zod.array(zod.string()).max(createTranscriptionBodyKnownSpeakerNamesMax).optional().describe('Optional list of speaker names that correspond to the audio samples provided in `known_speaker_references[]`. Each entry should be a short identifier (for example `customer` or `agent`). Up to 4 speakers are supported.\n'),
  "known_speaker_references": zod.array(zod.string()).max(createTranscriptionBodyKnownSpeakerReferencesMax).optional().describe('Optional list of audio samples (as [data URLs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs)) that contain known speaker references matching `known_speaker_names[]`. Each sample must be between 2 and 10 seconds, and can use any of the same input audio formats supported by `file`.\n')
})

export const createTranscriptionResponse = zod.union( [zod.object({
  "text": zod.string().describe('The transcribed text.'),
  "logprobs": zod.array(zod.object({
  "token": zod.string().optional().describe('The token in the transcription.'),
  "logprob": zod.number().optional().describe('The log probability of the token.'),
  "bytes": zod.array(zod.number()).optional().describe('The bytes of the token.')
})).optional().describe('The log probabilities of the tokens in the transcription. Only returned with the models `gpt-4o-transcribe` and `gpt-4o-mini-transcribe` if `logprobs` is added to the `include` array.\n'),
  "usage": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['tokens']).describe('The type of the usage object. Always `tokens` for this variant.'),
  "input_tokens": zod.number().describe('Number of input tokens billed for this request.'),
  "input_token_details": zod.object({
  "text_tokens": zod.number().optional().describe('Number of text tokens billed for this request.'),
  "audio_tokens": zod.number().optional().describe('Number of audio tokens billed for this request.')
}).optional().describe('Details about the input tokens billed for this request.'),
  "output_tokens": zod.number().describe('Number of output tokens generated.'),
  "total_tokens": zod.number().describe('Total number of tokens used (input + output).')
}).describe('Usage statistics for models billed by token usage.'),zod.object({
  "type": zod.enum(['duration']).describe('The type of the usage object. Always `duration` for this variant.'),
  "seconds": zod.number().describe('Duration of the input audio in seconds.')
}).describe('Usage statistics for models billed by audio input duration.')]).optional().describe('Token usage statistics for the request.')
}).describe('Represents a transcription response returned by model, based on the provided input.'),zod.object({
  "task": zod.enum(['transcribe']).describe('The type of task that was run. Always `transcribe`.'),
  "duration": zod.number().describe('Duration of the input audio in seconds.'),
  "text": zod.string().describe('The concatenated transcript text for the entire audio input.'),
  "segments": zod.array(zod.object({
  "type": zod.enum(['transcript.text.segment']).describe('The type of the segment. Always `transcript.text.segment`.\n'),
  "id": zod.string().describe('Unique identifier for the segment.'),
  "start": zod.number().describe('Start timestamp of the segment in seconds.'),
  "end": zod.number().describe('End timestamp of the segment in seconds.'),
  "text": zod.string().describe('Transcript text for this segment.'),
  "speaker": zod.string().describe('Speaker label for this segment. When known speakers are provided, the label matches `known_speaker_names[]`. Otherwise speakers are labeled sequentially using capital letters (`A`, `B`, ...).\n')
}).describe('A segment of diarized transcript text with speaker metadata.')).describe('Segments of the transcript annotated with timestamps and speaker labels.'),
  "usage": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['tokens']).describe('The type of the usage object. Always `tokens` for this variant.'),
  "input_tokens": zod.number().describe('Number of input tokens billed for this request.'),
  "input_token_details": zod.object({
  "text_tokens": zod.number().optional().describe('Number of text tokens billed for this request.'),
  "audio_tokens": zod.number().optional().describe('Number of audio tokens billed for this request.')
}).optional().describe('Details about the input tokens billed for this request.'),
  "output_tokens": zod.number().describe('Number of output tokens generated.'),
  "total_tokens": zod.number().describe('Total number of tokens used (input + output).')
}).describe('Usage statistics for models billed by token usage.'),zod.object({
  "type": zod.enum(['duration']).describe('The type of the usage object. Always `duration` for this variant.'),
  "seconds": zod.number().describe('Duration of the input audio in seconds.')
}).describe('Usage statistics for models billed by audio input duration.')]).optional().describe('Token or duration usage statistics for the request.')
}).describe('Represents a diarized transcription response returned by the model, including the combined transcript and speaker-segment annotations.\n'),zod.object({
  "language": zod.string().describe('The language of the input audio.'),
  "duration": zod.number().describe('The duration of the input audio.'),
  "text": zod.string().describe('The transcribed text.'),
  "words": zod.array(zod.object({
  "word": zod.string().describe('The text content of the word.'),
  "start": zod.number().describe('Start time of the word in seconds.'),
  "end": zod.number().describe('End time of the word in seconds.')
})).optional().describe('Extracted words and their corresponding timestamps.'),
  "segments": zod.array(zod.object({
  "id": zod.number().describe('Unique identifier of the segment.'),
  "seek": zod.number().describe('Seek offset of the segment.'),
  "start": zod.number().describe('Start time of the segment in seconds.'),
  "end": zod.number().describe('End time of the segment in seconds.'),
  "text": zod.string().describe('Text content of the segment.'),
  "tokens": zod.array(zod.number()).describe('Array of token IDs for the text content.'),
  "temperature": zod.number().describe('Temperature parameter used for generating the segment.'),
  "avg_logprob": zod.number().describe('Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.'),
  "compression_ratio": zod.number().describe('Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.'),
  "no_speech_prob": zod.number().describe('Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent.')
})).optional().describe('Segments of the transcribed text and their corresponding details.'),
  "usage": zod.object({
  "type": zod.enum(['duration']).describe('The type of the usage object. Always `duration` for this variant.'),
  "seconds": zod.number().describe('Duration of the input audio in seconds.')
}).optional().describe('Usage statistics for models billed by audio input duration.')
}).describe('Represents a verbose json transcription response returned by model, based on the provided input.')])


/**
 * Translates audio into English.
 * @summary Create translation
 */
export const createTranslationBodyResponseFormatDefault = "json";export const createTranslationBodyTemperatureDefault = 0;

export const createTranslationBody = zod.object({
  "file": zod.instanceof(File).describe('The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.\n'),
  "model": zod.string().or(zod.enum(['whisper-1'])).describe('ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2 model) is currently available.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should be in English.\n'),
  "response_format": zod.enum(['json', 'text', 'srt', 'verbose_json', 'vtt']).default(createTranslationBodyResponseFormatDefault).describe('The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.\n'),
  "temperature": zod.number().optional().describe('The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.\n')
})

export const createTranslationResponse = zod.object({
  "text": zod.string()
}).or(zod.object({
  "language": zod.string().describe('The language of the output translation (always `english`).'),
  "duration": zod.number().describe('The duration of the input audio.'),
  "text": zod.string().describe('The translated text.'),
  "segments": zod.array(zod.object({
  "id": zod.number().describe('Unique identifier of the segment.'),
  "seek": zod.number().describe('Seek offset of the segment.'),
  "start": zod.number().describe('Start time of the segment in seconds.'),
  "end": zod.number().describe('End time of the segment in seconds.'),
  "text": zod.string().describe('Text content of the segment.'),
  "tokens": zod.array(zod.number()).describe('Array of token IDs for the text content.'),
  "temperature": zod.number().describe('Temperature parameter used for generating the segment.'),
  "avg_logprob": zod.number().describe('Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.'),
  "compression_ratio": zod.number().describe('Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.'),
  "no_speech_prob": zod.number().describe('Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent.')
})).optional().describe('Segments of the translated text and their corresponding details.')
}))


/**
 * Creates a custom voice.
 * @summary Create voice
 */
export const createVoiceBody = zod.object({
  "name": zod.string().describe('The name of the new voice.'),
  "audio_sample": zod.instanceof(File).describe('The sample audio recording file. Maximum size is 10 MiB.\n\nSupported MIME types:\n`audio/mpeg`, `audio/wav`, `audio/x-wav`, `audio/ogg`, `audio/aac`, `audio/flac`, `audio/webm`, `audio/mp4`.\n'),
  "consent": zod.string().describe('The consent recording ID (for example, `cons_1234`).')
})

export const createVoiceResponse = zod.object({
  "object": zod.enum(['audio.voice']).describe('The object type, which is always `audio.voice`.'),
  "id": zod.string().describe('The voice identifier, which can be referenced in API endpoints.'),
  "name": zod.string().describe('The name of the voice.'),
  "created_at": zod.number().describe('The Unix timestamp (in seconds) for when the voice was created.')
}).describe('A custom voice that can be used for audio output.')


/**
 * Create a Realtime client secret with an associated session configuration.

 * @summary Create client secret
 */
export const createRealtimeClientSecretBodyExpiresAfterAnchorDefault = "created_at";export const createRealtimeClientSecretBodyExpiresAfterSecondsDefault = 600;
export const createRealtimeClientSecretBodyExpiresAfterSecondsMin = 10;

export const createRealtimeClientSecretBodyExpiresAfterSecondsMax = 7200;
export const createRealtimeClientSecretBodySessionOutputModalitiesDefault: ('text' | 'audio')[] = ["audio"];export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionTypeDefault = "server_vad";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefault = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefault = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinOne = 5000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxOne = 30000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionEagernessDefault = "auto";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultOne = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultOne = true;export const createRealtimeClientSecretBodySessionAudioOutputSpeedDefault = 1;
export const createRealtimeClientSecretBodySessionAudioOutputSpeedMin = 0.25;

export const createRealtimeClientSecretBodySessionAudioOutputSpeedMax = 1.5;
export const createRealtimeClientSecretBodySessionTracingDefaultOne = "auto";export const createRealtimeClientSecretBodySessionToolsItemRequireApprovalDefaultOne = "always";export const createRealtimeClientSecretBodySessionToolChoiceDefault = "auto";export const createRealtimeClientSecretBodySessionTruncationRetentionRatioMin = 0;

export const createRealtimeClientSecretBodySessionTruncationRetentionRatioMax = 1;
export const createRealtimeClientSecretBodySessionTruncationTokenLimitsPostInstructionsMin = 0;
export const createRealtimeClientSecretBodySessionPromptVariablesTypeDefault = "input_text";export const createRealtimeClientSecretBodySessionPromptVariablesTypeDefaultOne = "input_image";export const createRealtimeClientSecretBodySessionPromptVariablesTypeDefaultTwo = "input_file";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionTypeDefaultTwo = "server_vad";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultTwo = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultTwo = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinFour = 5000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxFour = 30000;
export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionEagernessDefaultOne = "auto";export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultThree = true;export const createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultThree = true;

export const createRealtimeClientSecretBody = zod.object({
  "expires_after": zod.object({
  "anchor": zod.enum(['created_at']).default(createRealtimeClientSecretBodyExpiresAfterAnchorDefault).describe('The anchor point for the client secret expiration, meaning that `seconds` will be added to the `created_at` time of the client secret to produce an expiration timestamp. Only `created_at` is currently supported.\n'),
  "seconds": zod.number().min(createRealtimeClientSecretBodyExpiresAfterSecondsMin).max(createRealtimeClientSecretBodyExpiresAfterSecondsMax).default(createRealtimeClientSecretBodyExpiresAfterSecondsDefault).describe('The number of seconds from the anchor point to the expiration. Select a value between `10` and `7200` (2 hours). This default to 600 seconds (10 minutes) if not specified.\n')
}).optional().describe('Configuration for the client secret expiration. Expiration refers to the time after which\na client secret will no longer be valid for creating sessions. The session itself may\ncontinue after that time once started. A secret can be used to create multiple sessions\nuntil it expires.\n'),
  "session": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['realtime']).describe('The type of session to create. Always `realtime` for the Realtime API.\n'),
  "output_modalities": zod.array(zod.enum(['text', 'audio'])).default(createRealtimeClientSecretBodySessionOutputModalitiesDefault).describe('The set of modalities the model can respond with. It defaults to `[\"audio\"]`, indicating\nthat the model will respond with audio plus a transcript. `[\"text\"]` can be used to make\nthe model respond with text only. It is not possible to request both `text` and `audio` at the same time.\n'),
  "model": zod.string().or(zod.enum(['gpt-realtime', 'gpt-realtime-2025-08-28', 'gpt-4o-realtime-preview', 'gpt-4o-realtime-preview-2024-10-01', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-4o-realtime-preview-2025-06-03', 'gpt-4o-mini-realtime-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-realtime-mini', 'gpt-realtime-mini-2025-10-06', 'gpt-realtime-mini-2025-12-15', 'gpt-audio-mini', 'gpt-audio-mini-2025-10-06', 'gpt-audio-mini-2025-12-15'])).optional().describe('The Realtime model used for this session.\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\n\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize'])).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefault).describe('Whether or not to automatically generate a response when a VAD stop event occurs. If `interrupt_response` is set to `false` this may fail to create a response if the model is already responding.\n\nIf both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.\n'),
  "interrupt_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefault).describe('Whether or not to automatically interrupt (cancel) any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs. If `true` then the response will be cancelled, otherwise it will continue until complete.\n\nIf both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.\n'),
  "idle_timeout_ms": zod.number().min(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinOne).max(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxOne).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionEagernessDefault).describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultOne).describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultOne).describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().min(createRealtimeClientSecretBodySessionAudioOutputSpeedMin).max(createRealtimeClientSecretBodySessionAudioOutputSpeedMax).default(createRealtimeClientSecretBodySessionAudioOutputSpeedDefault).describe('The speed of the model\'s spoken response as a multiple of the original speed.\n1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.\n\nThis parameter is a post-processing adjustment to the audio after it is generated, it\'s\nalso possible to prompt the model to speak faster or slower.\n')
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "tracing": zod.enum(['auto']).describe('Enables tracing and sets default values for tracing configuration options. Always `auto`.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the Traces Dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the Traces Dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the Traces Dashboard.\n')
}).describe('Granular configuration for tracing.\n')).nullish().describe('Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "tools": zod.array(zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
}),zod.object({
  "type": zod.enum(['mcp']).describe('The type of the MCP tool. Always `mcp`.'),
  "server_label": zod.string().describe('A label for this MCP server, used to identify it in tool calls.\n'),
  "server_url": zod.string().optional().describe('The URL for the MCP server. One of `server_url` or `connector_id` must be\nprovided.\n'),
  "connector_id": zod.enum(['connector_dropbox', 'connector_gmail', 'connector_googlecalendar', 'connector_googledrive', 'connector_microsoftteams', 'connector_outlookcalendar', 'connector_outlookemail', 'connector_sharepoint']).optional().describe('Identifier for service connectors, like those available in ChatGPT. One of\n`server_url` or `connector_id` must be provided. Learn more about service\nconnectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n\nCurrently supported `connector_id` values are:\n\n- Dropbox: `connector_dropbox`\n- Gmail: `connector_gmail`\n- Google Calendar: `connector_googlecalendar`\n- Google Drive: `connector_googledrive`\n- Microsoft Teams: `connector_microsoftteams`\n- Outlook Calendar: `connector_outlookcalendar`\n- Outlook Email: `connector_outlookemail`\n- SharePoint: `connector_sharepoint`\n'),
  "authorization": zod.string().optional().describe('An OAuth access token that can be used with a remote MCP server, either\nwith a custom MCP server URL or a service connector. Your application\nmust handle the OAuth authorization flow and provide the token here.\n'),
  "server_description": zod.string().optional().describe('Optional description of the MCP server, used to provide more context.\n'),
  "headers": zod.record(zod.string(), zod.string()).describe('Optional HTTP headers to send to the MCP server. Use for authentication\nor other purposes.\n').or(zod.null()).optional(),
  "allowed_tools": zod.array(zod.string()).describe('A string array of allowed tool names').or(zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).describe('A filter object to specify which tools are allowed.\n')).describe('List of allowed tool names or a filter object.\n').or(zod.null()).optional(),
  "require_approval": zod.object({
  "always": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n'),
  "never": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n')
}).describe('Specify which of the MCP server\'s tools require approval. Can be\n`always`, `never`, or a filter object associated with tools\nthat require approval.\n').or(zod.enum(['always', 'never']).describe('Specify a single approval policy for all tools. One of `always` or\n`never`. When set to `always`, all tools will require approval. When\nset to `never`, all tools will not require approval.\n')).describe('Specify which of the MCP server\'s tools require approval.').or(zod.null()).optional()
}).describe('Give the model access to additional tools via remote Model Context Protocol\n(MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n')])).optional().describe('Tools available to the model.'),
  "tool_choice": zod.enum(['none', 'auto', 'required']).describe('Controls which (if any) tool is called by the model.\n\n`none` means the model will not call any tool and instead generates a message.\n\n`auto` means the model can pick between generating a message or calling one or\nmore tools.\n\n`required` means the model must call one or more tools.\n').or(zod.object({
  "type": zod.enum(['function']).describe('For function calling, the type is always `function`.'),
  "name": zod.string().describe('The name of the function to call.')
}).describe('Use this option to force the model to call a specific function.\n')).or(zod.object({
  "type": zod.enum(['mcp']).describe('For MCP tools, the type is always `mcp`.'),
  "server_label": zod.string().describe('The label of the MCP server to use.\n'),
  "name": zod.string().describe('The name of the tool to call on the server.\n').or(zod.null()).optional()
}).describe('Use this option to force the model to call a specific tool on a remote MCP server.\n')).default(createRealtimeClientSecretBodySessionToolChoiceDefault).describe('How the model chooses tools. Provide one of the string modes or force a specific\nfunction/MCP tool.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(createRealtimeClientSecretBodySessionTruncationRetentionRatioMin).max(createRealtimeClientSecretBodySessionTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (`0.0` - `1.0`) when the conversation exceeds the input token limit. Setting this to `0.8` means that messages will be dropped until 80% of the maximum allowed tokens are used. This helps reduce the frequency of truncations and improve cache rates.\n'),
  "token_limits": zod.object({
  "post_instructions": zod.number().min(createRealtimeClientSecretBodySessionTruncationTokenLimitsPostInstructionsMin).optional().describe('Maximum tokens allowed in the conversation after instructions (which including tool definitions). For example, setting this to 5,000 would mean that truncation would occur when the conversation exceeds 5,000 tokens after instructions. This cannot be higher than the model\'s context window size minus the maximum output tokens.')
}).optional().describe('Optional custom token limits for this truncation strategy. If not provided, the model\'s default token limits will be used.')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('When the number of tokens in a conversation exceeds the model\'s input token limit, the conversation be truncated, meaning messages (starting from the oldest) will not be included in the model\'s context. A 32k context model with 4,096 max output tokens can only include 28,224 tokens in the context before truncation occurs.\n\nClients can configure truncation behavior to truncate with a lower max token limit, which is an effective way to control token usage and cost.\n\nTruncation will reduce the number of cached tokens on the next turn (busting the cache), since messages are dropped from the beginning of the context. However, clients can also configure truncation to retain messages up to a fraction of the maximum context size, which will reduce the need for future truncations and thus improve the cache rate.\n\nTruncation can be disabled entirely, which means the server will never truncate but would instead return an error if the conversation exceeds the model\'s input token limit.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('Realtime session object configuration.'),zod.object({
  "type": zod.enum(['transcription']).describe('The type of session to create. Always `transcription` for transcription sessions.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize'])).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultTwo).describe('Whether or not to automatically generate a response when a VAD stop event occurs. If `interrupt_response` is set to `false` this may fail to create a response if the model is already responding.\n\nIf both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.\n'),
  "interrupt_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultTwo).describe('Whether or not to automatically interrupt (cancel) any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs. If `true` then the response will be cancelled, otherwise it will continue until complete.\n\nIf both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.\n'),
  "idle_timeout_ms": zod.number().min(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMinFour).max(createRealtimeClientSecretBodySessionAudioInputTurnDetectionIdleTimeoutMsMaxFour).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionEagernessDefaultOne).describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionCreateResponseDefaultThree).describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().default(createRealtimeClientSecretBodySessionAudioInputTurnDetectionInterruptResponseDefaultThree).describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n')
}).describe('Realtime transcription session object configuration.')]).optional().describe('Session configuration to use for the client secret. Choose either a realtime\nsession or a transcription session.\n')
}).describe('Create a session and client secret for the Realtime API. The request can specify\neither a realtime or a transcription session configuration.\n[Learn more about the Realtime API](https://platform.openai.com/docs/guides/realtime).\n')

export const createRealtimeClientSecretResponseSessionOutputModalitiesDefault: ('text' | 'audio')[] = ["audio"];export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionTypeDefault = "server_vad";export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionCreateResponseDefault = true;export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionInterruptResponseDefault = true;export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMinOne = 5000;
export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMaxOne = 30000;
export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionEagernessDefault = "auto";export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionCreateResponseDefaultOne = true;export const createRealtimeClientSecretResponseSessionAudioInputTurnDetectionInterruptResponseDefaultOne = true;export const createRealtimeClientSecretResponseSessionAudioOutputSpeedDefault = 1;
export const createRealtimeClientSecretResponseSessionAudioOutputSpeedMin = 0.25;

export const createRealtimeClientSecretResponseSessionAudioOutputSpeedMax = 1.5;
export const createRealtimeClientSecretResponseSessionTracingDefaultTwo = "auto";export const createRealtimeClientSecretResponseSessionToolsItemRequireApprovalDefaultOne = "always";export const createRealtimeClientSecretResponseSessionToolChoiceDefault = "auto";export const createRealtimeClientSecretResponseSessionTruncationRetentionRatioMin = 0;

export const createRealtimeClientSecretResponseSessionTruncationRetentionRatioMax = 1;
export const createRealtimeClientSecretResponseSessionTruncationTokenLimitsPostInstructionsMin = 0;
export const createRealtimeClientSecretResponseSessionPromptVariablesTypeDefault = "input_text";export const createRealtimeClientSecretResponseSessionPromptVariablesTypeDefaultOne = "input_image";export const createRealtimeClientSecretResponseSessionPromptVariablesTypeDefaultTwo = "input_file";

export const createRealtimeClientSecretResponse = zod.object({
  "value": zod.string().describe('The generated client secret value.'),
  "expires_at": zod.number().describe('Expiration timestamp for the client secret, in seconds since epoch.'),
  "session": zod.discriminatedUnion('type', [zod.object({
  "client_secret": zod.object({
  "value": zod.string().describe('Ephemeral key usable in client environments to authenticate connections to the Realtime API. Use this in client-side environments rather than a standard API token, which should only be used server-side.\n'),
  "expires_at": zod.number().describe('Timestamp for when the token expires. Currently, all tokens expire\nafter one minute.\n')
}).describe('Ephemeral key returned by the API.'),
  "type": zod.enum(['realtime']).describe('The type of session to create. Always `realtime` for the Realtime API.\n'),
  "output_modalities": zod.array(zod.enum(['text', 'audio'])).default(createRealtimeClientSecretResponseSessionOutputModalitiesDefault).describe('The set of modalities the model can respond with. It defaults to `[\"audio\"]`, indicating\nthat the model will respond with audio plus a transcript. `[\"text\"]` can be used to make\nthe model respond with text only. It is not possible to request both `text` and `audio` at the same time.\n'),
  "model": zod.string().or(zod.enum(['gpt-realtime', 'gpt-realtime-2025-08-28', 'gpt-4o-realtime-preview', 'gpt-4o-realtime-preview-2024-10-01', 'gpt-4o-realtime-preview-2024-12-17', 'gpt-4o-realtime-preview-2025-06-03', 'gpt-4o-mini-realtime-preview', 'gpt-4o-mini-realtime-preview-2024-12-17', 'gpt-realtime-mini', 'gpt-realtime-mini-2025-10-06', 'gpt-realtime-mini-2025-12-15', 'gpt-audio-mini', 'gpt-audio-mini-2025-10-06', 'gpt-audio-mini-2025-12-15'])).optional().describe('The Realtime model used for this session.\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\n\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize'])).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "turn_detection": zod.discriminatedUnion('type', [zod.object({
  "type": zod.string().describe('Type of turn detection, `server_vad` to turn on simple Server VAD.\n'),
  "threshold": zod.number().optional().describe('Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n'),
  "create_response": zod.boolean().default(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionCreateResponseDefault).describe('Whether or not to automatically generate a response when a VAD stop event occurs. If `interrupt_response` is set to `false` this may fail to create a response if the model is already responding.\n\nIf both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.\n'),
  "interrupt_response": zod.boolean().default(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionInterruptResponseDefault).describe('Whether or not to automatically interrupt (cancel) any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs. If `true` then the response will be cancelled, otherwise it will continue until complete.\n\nIf both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.\n'),
  "idle_timeout_ms": zod.number().min(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMinOne).max(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionIdleTimeoutMsMaxOne).describe('Optional timeout after which a model response will be triggered automatically. This is\nuseful for situations in which a long pause from the user is unexpected, such as a phone\ncall. The model will effectively prompt the user to continue the conversation based\non the current context.\n\nThe timeout value will be applied after the last model response\'s audio has finished playing,\ni.e. it\'s set to the `response.done` time plus audio playback duration.\n\nAn `input_audio_buffer.timeout_triggered` event (plus events\nassociated with the Response) will be emitted when the timeout is reached.\nIdle timeout is currently only supported for `server_vad` mode.\n').or(zod.null()).optional()
}).describe('Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.'),zod.object({
  "type": zod.string().describe('Type of turn detection, `semantic_vad` to turn on Semantic VAD.\n'),
  "eagerness": zod.enum(['low', 'medium', 'high', 'auto']).default(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionEagernessDefault).describe('Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.\n'),
  "create_response": zod.boolean().default(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionCreateResponseDefaultOne).describe('Whether or not to automatically generate a response when a VAD stop event occurs.\n'),
  "interrupt_response": zod.boolean().default(createRealtimeClientSecretResponseSessionAudioInputTurnDetectionInterruptResponseDefaultOne).describe('Whether or not to automatically interrupt any ongoing response with output to the default\nconversation (i.e. `conversation` of `auto`) when a VAD start event occurs.\n')
}).describe('Server-side semantic turn detection which uses a model to determine when the user has finished speaking.')]).describe('Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.\n\nServer VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n\nSemantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with \"uhhm\", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.\n').or(zod.null()).optional()
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().min(createRealtimeClientSecretResponseSessionAudioOutputSpeedMin).max(createRealtimeClientSecretResponseSessionAudioOutputSpeedMax).default(createRealtimeClientSecretResponseSessionAudioOutputSpeedDefault).describe('The speed of the model\'s spoken response as a multiple of the original speed.\n1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.\n\nThis parameter is a post-processing adjustment to the audio after it is generated, it\'s\nalso possible to prompt the model to speak faster or slower.\n')
}).optional()
}).optional().describe('Configuration for input and output audio.\n'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n\n`item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "tracing": zod.enum(['auto']).describe('Enables tracing and sets default values for tracing configuration options. Always `auto`.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the Traces Dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the Traces Dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the Traces Dashboard.\n')
}).describe('Granular configuration for tracing.\n')).describe('Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n').or(zod.null()).optional(),
  "tools": zod.array(zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
}).or(zod.object({
  "type": zod.enum(['mcp']).describe('The type of the MCP tool. Always `mcp`.'),
  "server_label": zod.string().describe('A label for this MCP server, used to identify it in tool calls.\n'),
  "server_url": zod.string().optional().describe('The URL for the MCP server. One of `server_url` or `connector_id` must be\nprovided.\n'),
  "connector_id": zod.enum(['connector_dropbox', 'connector_gmail', 'connector_googlecalendar', 'connector_googledrive', 'connector_microsoftteams', 'connector_outlookcalendar', 'connector_outlookemail', 'connector_sharepoint']).optional().describe('Identifier for service connectors, like those available in ChatGPT. One of\n`server_url` or `connector_id` must be provided. Learn more about service\nconnectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).\n\nCurrently supported `connector_id` values are:\n\n- Dropbox: `connector_dropbox`\n- Gmail: `connector_gmail`\n- Google Calendar: `connector_googlecalendar`\n- Google Drive: `connector_googledrive`\n- Microsoft Teams: `connector_microsoftteams`\n- Outlook Calendar: `connector_outlookcalendar`\n- Outlook Email: `connector_outlookemail`\n- SharePoint: `connector_sharepoint`\n'),
  "authorization": zod.string().optional().describe('An OAuth access token that can be used with a remote MCP server, either\nwith a custom MCP server URL or a service connector. Your application\nmust handle the OAuth authorization flow and provide the token here.\n'),
  "server_description": zod.string().optional().describe('Optional description of the MCP server, used to provide more context.\n'),
  "headers": zod.record(zod.string(), zod.string()).describe('Optional HTTP headers to send to the MCP server. Use for authentication\nor other purposes.\n').or(zod.null()).optional(),
  "allowed_tools": zod.array(zod.string()).describe('A string array of allowed tool names').or(zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).describe('A filter object to specify which tools are allowed.\n')).describe('List of allowed tool names or a filter object.\n').or(zod.null()).optional(),
  "require_approval": zod.object({
  "always": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n'),
  "never": zod.object({
  "tool_names": zod.array(zod.string()).optional().describe('List of allowed tool names.'),
  "read_only": zod.boolean().optional().describe('Indicates whether or not a tool modifies data or is read-only. If an\nMCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),\nit will match this filter.\n')
}).optional().describe('A filter object to specify which tools are allowed.\n')
}).describe('Specify which of the MCP server\'s tools require approval. Can be\n`always`, `never`, or a filter object associated with tools\nthat require approval.\n').or(zod.enum(['always', 'never']).describe('Specify a single approval policy for all tools. One of `always` or\n`never`. When set to `always`, all tools will require approval. When\nset to `never`, all tools will not require approval.\n')).describe('Specify which of the MCP server\'s tools require approval.').or(zod.null()).optional()
}).describe('Give the model access to additional tools via remote Model Context Protocol\n(MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).\n'))).optional().describe('Tools available to the model.'),
  "tool_choice": zod.enum(['none', 'auto', 'required']).describe('Controls which (if any) tool is called by the model.\n\n`none` means the model will not call any tool and instead generates a message.\n\n`auto` means the model can pick between generating a message or calling one or\nmore tools.\n\n`required` means the model must call one or more tools.\n').or(zod.object({
  "type": zod.enum(['function']).describe('For function calling, the type is always `function`.'),
  "name": zod.string().describe('The name of the function to call.')
}).describe('Use this option to force the model to call a specific function.\n')).or(zod.object({
  "type": zod.enum(['mcp']).describe('For MCP tools, the type is always `mcp`.'),
  "server_label": zod.string().describe('The label of the MCP server to use.\n'),
  "name": zod.string().describe('The name of the tool to call on the server.\n').or(zod.null()).optional()
}).describe('Use this option to force the model to call a specific tool on a remote MCP server.\n')).default(createRealtimeClientSecretResponseSessionToolChoiceDefault).describe('How the model chooses tools. Provide one of the string modes or force a specific\nfunction/MCP tool.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(createRealtimeClientSecretResponseSessionTruncationRetentionRatioMin).max(createRealtimeClientSecretResponseSessionTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (`0.0` - `1.0`) when the conversation exceeds the input token limit. Setting this to `0.8` means that messages will be dropped until 80% of the maximum allowed tokens are used. This helps reduce the frequency of truncations and improve cache rates.\n'),
  "token_limits": zod.object({
  "post_instructions": zod.number().min(createRealtimeClientSecretResponseSessionTruncationTokenLimitsPostInstructionsMin).optional().describe('Maximum tokens allowed in the conversation after instructions (which including tool definitions). For example, setting this to 5,000 would mean that truncation would occur when the conversation exceeds 5,000 tokens after instructions. This cannot be higher than the model\'s context window size minus the maximum output tokens.')
}).optional().describe('Optional custom token limits for this truncation strategy. If not provided, the model\'s default token limits will be used.')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('When the number of tokens in a conversation exceeds the model\'s input token limit, the conversation be truncated, meaning messages (starting from the oldest) will not be included in the model\'s context. A 32k context model with 4,096 max output tokens can only include 28,224 tokens in the context before truncation occurs.\n\nClients can configure truncation behavior to truncate with a lower max token limit, which is an effective way to control token usage and cost.\n\nTruncation will reduce the number of cached tokens on the next turn (busting the cache), since messages are dropped from the beginning of the context. However, clients can also configure truncation to retain messages up to a fraction of the maximum context size, which will reduce the need for future truncations and thus improve the cache rate.\n\nTruncation can be disabled entirely, which means the server will never truncate but would instead return an error if the conversation exceeds the model\'s input token limit.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('A new Realtime session configuration, with an ephemeral key. Default TTL\nfor keys is one minute.\n'),zod.object({
  "type": zod.enum(['transcription']).describe('The type of session. Always `transcription` for transcription sessions.\n'),
  "id": zod.string().describe('Unique identifier for the session that looks like `sess_1234567890abcdef`.\n'),
  "object": zod.string().describe('The object type. Always `realtime.transcription_session`.'),
  "expires_at": zod.number().optional().describe('Expiration timestamp for the session, in seconds since epoch.'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n- `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize'])).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n')
}).optional()
}).optional().describe('Configuration for input audio for the session.\n')
}).describe('A Realtime transcription session configuration object.\n')]).describe('The session configuration for either a realtime or transcription session.\n')
}).describe('Response from creating a session and client secret for the Realtime API.\n')


/**
 * Create an ephemeral API token for use in client-side applications with the
Realtime API. Can be configured with the same session parameters as the
`session.update` client event.

It responds with a session object, plus a `client_secret` key which contains
a usable ephemeral API token that can be used to authenticate browser clients
for the Realtime API.

 * @summary Create session
 */
export const createRealtimeSessionBodySpeedDefault = 1;
export const createRealtimeSessionBodySpeedMin = 0.25;

export const createRealtimeSessionBodySpeedMax = 1.5;
export const createRealtimeSessionBodyTracingDefaultOne = "auto";export const createRealtimeSessionBodyTruncationRetentionRatioMin = 0;

export const createRealtimeSessionBodyTruncationRetentionRatioMax = 1;
export const createRealtimeSessionBodyTruncationTokenLimitsPostInstructionsMin = 0;
export const createRealtimeSessionBodyPromptVariablesTypeDefault = "input_text";export const createRealtimeSessionBodyPromptVariablesTypeDefaultOne = "input_image";export const createRealtimeSessionBodyPromptVariablesTypeDefaultTwo = "input_file";

export const createRealtimeSessionBody = zod.object({
  "client_secret": zod.object({
  "value": zod.string().describe('Ephemeral key usable in client environments to authenticate connections\nto the Realtime API. Use this in client-side environments rather than\na standard API token, which should only be used server-side.\n'),
  "expires_at": zod.number().describe('Timestamp for when the token expires. Currently, all tokens expire\nafter one minute.\n')
}).describe('Ephemeral key returned by the API.'),
  "modalities": zod.any().optional().describe('The set of modalities the model can respond with. To disable audio,\nset this to [\"text\"].\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good responses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion into your voice\", \"laugh frequently\"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.\nNote that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.\n'),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "input_audio_format": zod.string().optional().describe('The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n'),
  "output_audio_format": zod.string().optional().describe('The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n'),
  "input_audio_transcription": zod.object({
  "model": zod.string().optional().describe('The model to use for transcription.\n')
}).optional().describe('Configuration for input audio transcription, defaults to off and can be\nset to `null` to turn off once on. Input audio transcription is not native\nto the model, since the model consumes audio directly. Transcription runs\nasynchronously and should be treated as rough guidance\nrather than the representation understood by the model.\n'),
  "speed": zod.number().min(createRealtimeSessionBodySpeedMin).max(createRealtimeSessionBodySpeedMax).default(createRealtimeSessionBodySpeedDefault).describe('The speed of the model\'s spoken response. 1.0 is the default speed. 0.25 is\nthe minimum speed. 1.5 is the maximum speed. This value can only be changed\nin between model turns, not while a response is in progress.\n'),
  "tracing": zod.enum(['auto']).describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the traces dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the traces dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the traces dashboard.\n')
}).describe('Granular configuration for tracing.\n')).optional().describe('Configuration options for tracing. Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n'),
  "tools": zod.array(zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
})).optional().describe('Tools (functions) available to the model.'),
  "tool_choice": zod.string().optional().describe('How the model chooses tools. Options are `auto`, `none`, `required`, or\nspecify a function.\n'),
  "temperature": zod.number().optional().describe('Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.\n'),
  "max_response_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n'),
  "truncation": zod.enum(['auto', 'disabled']).describe('The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.').or(zod.object({
  "type": zod.enum(['retention_ratio']).describe('Use retention ratio truncation.'),
  "retention_ratio": zod.number().min(createRealtimeSessionBodyTruncationRetentionRatioMin).max(createRealtimeSessionBodyTruncationRetentionRatioMax).describe('Fraction of post-instruction conversation tokens to retain (`0.0` - `1.0`) when the conversation exceeds the input token limit. Setting this to `0.8` means that messages will be dropped until 80% of the maximum allowed tokens are used. This helps reduce the frequency of truncations and improve cache rates.\n'),
  "token_limits": zod.object({
  "post_instructions": zod.number().min(createRealtimeSessionBodyTruncationTokenLimitsPostInstructionsMin).optional().describe('Maximum tokens allowed in the conversation after instructions (which including tool definitions). For example, setting this to 5,000 would mean that truncation would occur when the conversation exceeds 5,000 tokens after instructions. This cannot be higher than the model\'s context window size minus the maximum output tokens.')
}).optional().describe('Optional custom token limits for this truncation strategy. If not provided, the model\'s default token limits will be used.')
}).describe('Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.')).optional().describe('When the number of tokens in a conversation exceeds the model\'s input token limit, the conversation be truncated, meaning messages (starting from the oldest) will not be included in the model\'s context. A 32k context model with 4,096 max output tokens can only include 28,224 tokens in the context before truncation occurs.\n\nClients can configure truncation behavior to truncate with a lower max token limit, which is an effective way to control token usage and cost.\n\nTruncation will reduce the number of cached tokens on the next turn (busting the cache), since messages are dropped from the beginning of the context. However, clients can also configure truncation to retain messages up to a fraction of the maximum context size, which will reduce the need for future truncations and thus improve the cache rate.\n\nTruncation can be disabled entirely, which means the server will never truncate but would instead return an error if the conversation exceeds the model\'s input token limit.\n'),
  "prompt": zod.object({
  "id": zod.string().describe('The unique identifier of the prompt template to use.'),
  "version": zod.string().describe('Optional version of the prompt template.').or(zod.null()).optional(),
  "variables": zod.record(zod.string(), zod.string().or(zod.object({
  "type": zod.enum(['input_text']).describe('The type of the input item. Always `input_text`.'),
  "text": zod.string().describe('The text input to the model.')
}).describe('A text input to the model.')).or(zod.object({
  "type": zod.enum(['input_image']).describe('The type of the input item. Always `input_image`.'),
  "image_url": zod.string().describe('The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.').or(zod.null()).optional(),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "detail": zod.enum(['low', 'high', 'auto'])
}).describe('An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).')).or(zod.object({
  "type": zod.enum(['input_file']).describe('The type of the input item. Always `input_file`.'),
  "file_id": zod.string().describe('The ID of the file to be sent to the model.').or(zod.null()).optional(),
  "filename": zod.string().optional().describe('The name of the file to be sent to the model.'),
  "file_url": zod.string().optional().describe('The URL of the file to be sent to the model.'),
  "file_data": zod.string().optional().describe('The content of the file to be sent to the model.\n')
}).describe('A file input to the model.'))).describe('Optional map of values to substitute in for variables in your\nprompt. The substitution values can either be strings, or other\nResponse input types like images or files.\n').or(zod.null()).optional()
}).describe('Reference to a prompt template and its variables.\n[Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).\n').or(zod.null()).optional()
}).describe('A new Realtime session configuration, with an ephemeral key. Default TTL\nfor keys is one minute.\n')

export const createRealtimeSessionResponseTracingDefaultOne = "auto";

export const createRealtimeSessionResponse = zod.object({
  "id": zod.string().optional().describe('Unique identifier for the session that looks like `sess_1234567890abcdef`.\n'),
  "object": zod.string().optional().describe('The object type. Always `realtime.session`.'),
  "expires_at": zod.number().optional().describe('Expiration timestamp for the session, in seconds since epoch.'),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('Additional fields to include in server outputs.\n- `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.\n'),
  "model": zod.string().optional().describe('The Realtime model used for this session.'),
  "output_modalities": zod.any().optional().describe('The set of modalities the model can respond with. To disable audio,\nset this to [\"text\"].\n'),
  "instructions": zod.string().optional().describe('The default system instructions (i.e. system message) prepended to model\ncalls. This field allows the client to guide the model on desired\nresponses. The model can be instructed on response content and format,\n(e.g. \"be extremely succinct\", \"act friendly\", \"here are examples of good\nresponses\") and on audio behavior (e.g. \"talk quickly\", \"inject emotion\ninto your voice\", \"laugh frequently\"). The instructions are not guaranteed\nto be followed by the model, but they provide guidance to the model on the\ndesired behavior.\n\nNote that the server sets default instructions which will be used if this\nfield is not set and are visible in the `session.created` event at the\nstart of the session.\n'),
  "audio": zod.object({
  "input": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "transcription": zod.object({
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize'])).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional(),
  "prefix_padding_ms": zod.number().optional(),
  "silence_duration_ms": zod.number().optional()
}).optional().describe('Configuration for turn detection.\n')
}).optional(),
  "output": zod.object({
  "format": zod.discriminatedUnion('type', [zod.object({
  "type": zod.enum(['audio/pcm']).optional().describe('The audio format. Always `audio/pcm`.'),
  "rate": zod.literal(24000).optional().describe('The sample rate of the audio. Always `24000`.')
}).describe('The PCM audio format. Only a 24kHz sample rate is supported.'),zod.object({
  "type": zod.enum(['audio/pcmu']).optional().describe('The audio format. Always `audio/pcmu`.')
}).describe('The G.711 Œº-law format.'),zod.object({
  "type": zod.enum(['audio/pcma']).optional().describe('The audio format. Always `audio/pcma`.')
}).describe('The G.711 A-law format.')]).optional(),
  "voice": zod.string().or(zod.enum(['alloy', 'ash', 'ballad', 'coral', 'echo', 'sage', 'shimmer', 'verse', 'marin', 'cedar'])).optional(),
  "speed": zod.number().optional()
}).optional()
}).optional().describe('Configuration for input and output audio for the session.\n'),
  "tracing": zod.enum(['auto']).describe('Default tracing mode for the session.\n').or(zod.object({
  "workflow_name": zod.string().optional().describe('The name of the workflow to attach to this trace. This is used to\nname the trace in the traces dashboard.\n'),
  "group_id": zod.string().optional().describe('The group id to attach to this trace to enable filtering and\ngrouping in the traces dashboard.\n'),
  "metadata": zod.object({

}).optional().describe('The arbitrary metadata to attach to this trace to enable\nfiltering in the traces dashboard.\n')
}).describe('Granular configuration for tracing.\n')).optional().describe('Configuration options for tracing. Set to null to disable tracing. Once\ntracing is enabled for a session, the configuration cannot be modified.\n\n`auto` will create a trace for the session with default values for the\nworkflow name, group id, and metadata.\n'),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n'),
  "tools": zod.array(zod.object({
  "type": zod.enum(['function']).optional().describe('The type of the tool, i.e. `function`.'),
  "name": zod.string().optional().describe('The name of the function.'),
  "description": zod.string().optional().describe('The description of the function, including guidance on when and how\nto call it, and guidance about what to tell the user when calling\n(if anything).\n'),
  "parameters": zod.object({

}).optional().describe('Parameters of the function in JSON Schema.')
})).optional().describe('Tools (functions) available to the model.'),
  "tool_choice": zod.string().optional().describe('How the model chooses tools. Options are `auto`, `none`, `required`, or\nspecify a function.\n'),
  "max_output_tokens": zod.number().or(zod.enum(['inf'])).optional().describe('Maximum number of output tokens for a single assistant response,\ninclusive of tool calls. Provide an integer between 1 and 4096 to\nlimit output tokens, or `inf` for the maximum available tokens for a\ngiven model. Defaults to `inf`.\n')
}).describe('A Realtime session configuration object.\n')


/**
 * Create an ephemeral API token for use in client-side applications with the
Realtime API specifically for realtime transcriptions. 
Can be configured with the same session parameters as the `transcription_session.update` client event.

It responds with a session object, plus a `client_secret` key which contains
a usable ephemeral API token that can be used to authenticate browser clients
for the Realtime API.

 * @summary Create transcription session
 */
export const createRealtimeTranscriptionSessionBodyInputAudioFormatDefault = "pcm16";

export const createRealtimeTranscriptionSessionBody = zod.object({
  "turn_detection": zod.object({
  "type": zod.enum(['server_vad']).optional().describe('Type of turn detection. Only `server_vad` is currently supported for transcription sessions.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.\n'),
  "input_audio_noise_reduction": zod.object({
  "type": zod.enum(['near_field', 'far_field']).optional().describe('Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.\n')
}).optional().describe('Configuration for input audio noise reduction. This can be set to `null` to turn off.\nNoise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.\nFiltering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.\n'),
  "input_audio_format": zod.enum(['pcm16', 'g711_ulaw', 'g711_alaw']).default(createRealtimeTranscriptionSessionBodyInputAudioFormatDefault).describe('The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\nFor `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate,\nsingle channel (mono), and little-endian byte order.\n'),
  "input_audio_transcription": zod.object({
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize'])).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "include": zod.array(zod.enum(['item.input_audio_transcription.logprobs'])).optional().describe('The set of items to include in the transcription. Current available items are:\n`item.input_audio_transcription.logprobs`\n')
}).describe('Realtime transcription session object configuration.')

export const createRealtimeTranscriptionSessionResponse = zod.object({
  "client_secret": zod.object({
  "value": zod.string().describe('Ephemeral key usable in client environments to authenticate connections\nto the Realtime API. Use this in client-side environments rather than\na standard API token, which should only be used server-side.\n'),
  "expires_at": zod.number().describe('Timestamp for when the token expires. Currently, all tokens expire\nafter one minute.\n')
}).describe('Ephemeral key returned by the API. Only present when the session is\ncreated on the server via REST API.\n'),
  "modalities": zod.any().optional().describe('The set of modalities the model can respond with. To disable audio,\nset this to [\"text\"].\n'),
  "input_audio_format": zod.string().optional().describe('The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.\n'),
  "input_audio_transcription": zod.object({
  "model": zod.string().or(zod.enum(['whisper-1', 'gpt-4o-mini-transcribe', 'gpt-4o-mini-transcribe-2025-12-15', 'gpt-4o-transcribe', 'gpt-4o-transcribe-diarize'])).optional().describe('The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.\n'),
  "language": zod.string().optional().describe('The language of the input audio. Supplying the input language in\n[ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format\nwill improve accuracy and latency.\n'),
  "prompt": zod.string().optional().describe('An optional text to guide the model\'s style or continue a previous audio\nsegment.\nFor `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).\nFor `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example \"expect words related to technology\".\n')
}).optional(),
  "turn_detection": zod.object({
  "type": zod.string().optional().describe('Type of turn detection, only `server_vad` is currently supported.\n'),
  "threshold": zod.number().optional().describe('Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A\nhigher threshold will require louder audio to activate the model, and\nthus might perform better in noisy environments.\n'),
  "prefix_padding_ms": zod.number().optional().describe('Amount of audio to include before the VAD detected speech (in\nmilliseconds). Defaults to 300ms.\n'),
  "silence_duration_ms": zod.number().optional().describe('Duration of silence to detect speech stop (in milliseconds). Defaults\nto 500ms. With shorter values the model will respond more quickly,\nbut may jump in on short pauses from the user.\n')
}).optional().describe('Configuration for turn detection. Can be set to `null` to turn off. Server\nVAD means that the model will detect the start and end of speech based on\naudio volume and respond at the end of user speech.\n')
}).describe('A new Realtime transcription session configuration.\n\nWhen a session is created on the server via REST API, the session object\nalso contains an ephemeral key. Default TTL for keys is 10 minutes. This\nproperty is not present when a session is updated via the WebSocket API.\n')
