openapi: 3.1.0
info:
  title: OpenAI Audio & Realtime API
  description: OpenAI Audio API - Transcription, Translation, Speech, and Realtime streaming endpoints. Filtered from the official OpenAI API spec (Stainless-hosted).
  version: 2.3.0
  termsOfService: https://openai.com/policies/terms-of-use
  contact:
    name: OpenAI Support
    url: https://help.openai.com/
  license:
    name: MIT
    url: https://github.com/openai/openai-openapi/blob/master/LICENSE
servers:
  - url: https://api.openai.com/v1
security:
  - ApiKeyAuth: []
tags:
  - name: Audio
    description: Turn audio into text or text into audio.
paths:
  /audio/speech:
    post:
      operationId: createSpeech
      tags:
        - Audio
      summary: Create speech
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateSpeechRequest'
      responses:
        '200':
          description: OK
          headers:
            Transfer-Encoding:
              schema:
                type: string
              description: chunked
          content:
            application/octet-stream:
              schema:
                type: string
                format: binary
            text/event-stream:
              schema:
                $ref: '#/components/schemas/CreateSpeechResponseStreamEvent'
      x-oaiMeta:
        name: Create speech
        group: audio
        returns: The audio file content or a [stream of audio events](https://platform.openai.com/docs/api-reference/audio/speech-audio-delta-event).
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/audio/speech \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -d '{
                    "model": "gpt-4o-mini-tts",
                    "input": "The quick brown fox jumped over the lazy dog.",
                    "voice": "alloy"
                  }' \
                  --output speech.mp3
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                speech = client.audio.speech.create(
                    input="input",
                    model="string",
                    voice="ash",
                )
                print(speech)
                content = speech.read()
                print(content)
              javascript: |
                import fs from "fs";
                import path from "path";
                import OpenAI from "openai";

                const openai = new OpenAI();

                const speechFile = path.resolve("./speech.mp3");

                async function main() {
                  const mp3 = await openai.audio.speech.create({
                    model: "gpt-4o-mini-tts",
                    voice: "alloy",
                    input: "Today is a wonderful day to build something people love!",
                  });
                  console.log(speechFile);
                  const buffer = Buffer.from(await mp3.arrayBuffer());
                  await fs.promises.writeFile(speechFile, buffer);
                }
                main();
              csharp: |
                using System;
                using System.IO;

                using OpenAI.Audio;

                AudioClient client = new(
                    model: "gpt-4o-mini-tts",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                BinaryData speech = client.GenerateSpeech(
                    text: "The quick brown fox jumped over the lazy dog.",
                    voice: GeneratedSpeechVoice.Alloy
                );

                using FileStream stream = File.OpenWrite("speech.mp3");
                speech.ToStream().CopyTo(stream);
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const speech = await client.audio.speech.create({
                  input: 'input',
                  model: 'string',
                  voice: 'ash',
                });

                console.log(speech);

                const content = await speech.blob();
                console.log(content);
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  speech, err := client.Audio.Speech.New(context.TODO(), openai.AudioSpeechNewParams{
                    Input: "input",
                    Model: openai.SpeechModel("string"),
                    Voice: openai.AudioSpeechNewParamsVoiceAsh,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", speech)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.core.http.HttpResponse;
                import com.openai.models.audio.speech.SpeechCreateParams;
                import com.openai.models.audio.speech.SpeechModel;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        SpeechCreateParams params = SpeechCreateParams.builder()
                            .input("input")
                            .model(SpeechModel.of("string"))
                            .voice(SpeechCreateParams.Voice.ASH)
                            .build();
                        HttpResponse speech = client.audio().speech().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                speech = openai.audio.speech.create(input: "input", model: :string, voice: :ash)

                puts(speech)
          - title: SSE Stream Format
            request:
              curl: |
                curl https://api.openai.com/v1/audio/speech \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: application/json" \
                  -d '{
                    "model": "gpt-4o-mini-tts",
                    "input": "The quick brown fox jumped over the lazy dog.",
                    "voice": "alloy",
                    "stream_format": "sse"
                  }'
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const speech = await client.audio.speech.create({
                  input: 'input',
                  model: 'string',
                  voice: 'ash',
                });

                console.log(speech);

                const content = await speech.blob();
                console.log(content);
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                speech = client.audio.speech.create(
                    input="input",
                    model="string",
                    voice="ash",
                )
                print(speech)
                content = speech.read()
                print(content)
              go: |
                package main

                import (
                  "context"
                  "fmt"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  speech, err := client.Audio.Speech.New(context.TODO(), openai.AudioSpeechNewParams{
                    Input: "input",
                    Model: openai.SpeechModel("string"),
                    Voice: openai.AudioSpeechNewParamsVoiceAsh,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", speech)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.core.http.HttpResponse;
                import com.openai.models.audio.speech.SpeechCreateParams;
                import com.openai.models.audio.speech.SpeechModel;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        SpeechCreateParams params = SpeechCreateParams.builder()
                            .input("input")
                            .model(SpeechModel.of("string"))
                            .voice(SpeechCreateParams.Voice.ASH)
                            .build();
                        HttpResponse speech = client.audio().speech().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                speech = openai.audio.speech.create(input: "input", model: :string, voice: :ash)

                puts(speech)
      description: Generates audio from the input text.
      parameters: []
  /audio/transcriptions:
    post:
      operationId: createTranscription
      tags:
        - Audio
      summary: Create transcription
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/CreateTranscriptionRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                anyOf:
                  - $ref: '#/components/schemas/CreateTranscriptionResponseJson'
                  - $ref: '#/components/schemas/CreateTranscriptionResponseDiarizedJson'
                    x-stainless-skip:
                      - go
                  - $ref: '#/components/schemas/CreateTranscriptionResponseVerboseJson'
                discriminator:
                  propertyName: task
            text/event-stream:
              schema:
                $ref: '#/components/schemas/CreateTranscriptionResponseStreamEvent'
      x-oaiMeta:
        name: Create transcription
        group: audio
        returns: The [transcription object](https://platform.openai.com/docs/api-reference/audio/json-object), a [diarized transcription object](https://platform.openai.com/docs/api-reference/audio/diarized-json-object), a [verbose transcription object](https://platform.openai.com/docs/api-reference/audio/verbose-json-object), or a [stream of transcript events](https://platform.openai.com/docs/api-reference/audio/transcript-text-delta-event).
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F model="gpt-4o-transcribe"
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                transcription = client.audio.transcriptions.create(
                    file=b"raw file contents",
                    model="gpt-4o-transcribe",
                )
                print(transcription)
              javascript: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const transcription = await openai.audio.transcriptions.create({
                    file: fs.createReadStream("audio.mp3"),
                    model: "gpt-4o-transcribe",
                  });

                  console.log(transcription.text);
                }
                main();
              csharp: |
                using System;

                using OpenAI.Audio;
                string audioFilePath = "audio.mp3";

                AudioClient client = new(
                    model: "gpt-4o-transcribe",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                AudioTranscription transcription = client.TranscribeAudio(audioFilePath);

                Console.WriteLine($"{transcription.Text}");
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const transcription = await client.audio.transcriptions.create({
                  file: fs.createReadStream('speech.mp3'),
                  model: 'gpt-4o-transcribe',
                });

                console.log(transcription);
              go: |
                package main

                import (
                  "bytes"
                  "context"
                  "fmt"
                  "io"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  transcription, err := client.Audio.Transcriptions.New(context.TODO(), openai.AudioTranscriptionNewParams{
                    File: io.Reader(bytes.NewBuffer([]byte("some file contents"))),
                    Model: openai.AudioModelGPT4oTranscribe,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", transcription)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.audio.AudioModel;
                import com.openai.models.audio.transcriptions.TranscriptionCreateParams;
                import com.openai.models.audio.transcriptions.TranscriptionCreateResponse;
                import java.io.ByteArrayInputStream;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        TranscriptionCreateParams params = TranscriptionCreateParams.builder()
                            .file(ByteArrayInputStream("some content".getBytes()))
                            .model(AudioModel.GPT_4O_TRANSCRIBE)
                            .build();
                        TranscriptionCreateResponse transcription = client.audio().transcriptions().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                transcription = openai.audio.transcriptions.create(file: Pathname(__FILE__), model: :"gpt-4o-transcribe")

                puts(transcription)
            response: |
              {
                "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that.",
                "usage": {
                  "type": "tokens",
                  "input_tokens": 14,
                  "input_token_details": {
                    "text_tokens": 0,
                    "audio_tokens": 14
                  },
                  "output_tokens": 45,
                  "total_tokens": 59
                }
              }
          - title: Diarization
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/meeting.wav" \
                  -F model="gpt-4o-transcribe-diarize" \
                  -F response_format="diarized_json" \
                  -F chunking_strategy=auto \
                  -F 'known_speaker_names[]=agent' \
                  -F 'known_speaker_references[]=data:audio/wav;base64,AAA...'
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                transcription = client.audio.transcriptions.create(
                    file=b"raw file contents",
                    model="gpt-4o-transcribe",
                )
                print(transcription)
              javascript: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                const speakerRef = fs.readFileSync("agent.wav").toString("base64");

                const transcript = await openai.audio.transcriptions.create({
                  file: fs.createReadStream("meeting.wav"),
                  model: "gpt-4o-transcribe-diarize",
                  response_format: "diarized_json",
                  chunking_strategy: "auto",
                  extra_body: {
                    known_speaker_names: ["agent"],
                    known_speaker_references: [`data:audio/wav;base64,${speakerRef}`],
                  },
                });

                console.log(transcript.segments);
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const transcription = await client.audio.transcriptions.create({
                  file: fs.createReadStream('speech.mp3'),
                  model: 'gpt-4o-transcribe',
                });

                console.log(transcription);
              go: |
                package main

                import (
                  "bytes"
                  "context"
                  "fmt"
                  "io"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  transcription, err := client.Audio.Transcriptions.New(context.TODO(), openai.AudioTranscriptionNewParams{
                    File: io.Reader(bytes.NewBuffer([]byte("some file contents"))),
                    Model: openai.AudioModelGPT4oTranscribe,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", transcription)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.audio.AudioModel;
                import com.openai.models.audio.transcriptions.TranscriptionCreateParams;
                import com.openai.models.audio.transcriptions.TranscriptionCreateResponse;
                import java.io.ByteArrayInputStream;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        TranscriptionCreateParams params = TranscriptionCreateParams.builder()
                            .file(ByteArrayInputStream("some content".getBytes()))
                            .model(AudioModel.GPT_4O_TRANSCRIBE)
                            .build();
                        TranscriptionCreateResponse transcription = client.audio().transcriptions().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                transcription = openai.audio.transcriptions.create(file: Pathname(__FILE__), model: :"gpt-4o-transcribe")

                puts(transcription)
            response: |
              {
                "task": "transcribe",
                "duration": 27.4,
                "text": "Agent: Thanks for calling OpenAI support.\nA: Hi, I'm trying to enable diarization.\nAgent: Happy to walk you through the steps.",
                "segments": [
                  {
                    "type": "transcript.text.segment",
                    "id": "seg_001",
                    "start": 0.0,
                    "end": 4.7,
                    "text": "Thanks for calling OpenAI support.",
                    "speaker": "agent"
                  },
                  {
                    "type": "transcript.text.segment",
                    "id": "seg_002",
                    "start": 4.7,
                    "end": 11.8,
                    "text": "Hi, I'm trying to enable diarization.",
                    "speaker": "A"
                  },
                  {
                    "type": "transcript.text.segment",
                    "id": "seg_003",
                    "start": 12.1,
                    "end": 18.5,
                    "text": "Happy to walk you through the steps.",
                    "speaker": "agent"
                  }
                ],
                "usage": {
                  "type": "duration",
                  "seconds": 27
                }
              }
          - title: Streaming
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F model="gpt-4o-mini-transcribe" \
                  -F stream=true
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                transcription = client.audio.transcriptions.create(
                    file=b"raw file contents",
                    model="gpt-4o-transcribe",
                )
                print(transcription)
              javascript: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                const stream = await openai.audio.transcriptions.create({
                  file: fs.createReadStream("audio.mp3"),
                  model: "gpt-4o-mini-transcribe",
                  stream: true,
                });

                for await (const event of stream) {
                  console.log(event);
                }
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const transcription = await client.audio.transcriptions.create({
                  file: fs.createReadStream('speech.mp3'),
                  model: 'gpt-4o-transcribe',
                });

                console.log(transcription);
              go: |
                package main

                import (
                  "bytes"
                  "context"
                  "fmt"
                  "io"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  transcription, err := client.Audio.Transcriptions.New(context.TODO(), openai.AudioTranscriptionNewParams{
                    File: io.Reader(bytes.NewBuffer([]byte("some file contents"))),
                    Model: openai.AudioModelGPT4oTranscribe,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", transcription)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.audio.AudioModel;
                import com.openai.models.audio.transcriptions.TranscriptionCreateParams;
                import com.openai.models.audio.transcriptions.TranscriptionCreateResponse;
                import java.io.ByteArrayInputStream;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        TranscriptionCreateParams params = TranscriptionCreateParams.builder()
                            .file(ByteArrayInputStream("some content".getBytes()))
                            .model(AudioModel.GPT_4O_TRANSCRIBE)
                            .build();
                        TranscriptionCreateResponse transcription = client.audio().transcriptions().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                transcription = openai.audio.transcriptions.create(file: Pathname(__FILE__), model: :"gpt-4o-transcribe")

                puts(transcription)
            response: |
              data: {"type":"transcript.text.delta","delta":"I","logprobs":[{"token":"I","logprob":-0.00007588794,"bytes":[73]}]}

              data: {"type":"transcript.text.delta","delta":" see","logprobs":[{"token":" see","logprob":-3.1281633e-7,"bytes":[32,115,101,101]}]}

              data: {"type":"transcript.text.delta","delta":" skies","logprobs":[{"token":" skies","logprob":-2.3392786e-6,"bytes":[32,115,107,105,101,115]}]}

              data: {"type":"transcript.text.delta","delta":" of","logprobs":[{"token":" of","logprob":-3.1281633e-7,"bytes":[32,111,102]}]}

              data: {"type":"transcript.text.delta","delta":" blue","logprobs":[{"token":" blue","logprob":-1.0280384e-6,"bytes":[32,98,108,117,101]}]}

              data: {"type":"transcript.text.delta","delta":" and","logprobs":[{"token":" and","logprob":-0.0005108566,"bytes":[32,97,110,100]}]}

              data: {"type":"transcript.text.delta","delta":" clouds","logprobs":[{"token":" clouds","logprob":-1.9361265e-7,"bytes":[32,99,108,111,117,100,115]}]}

              data: {"type":"transcript.text.delta","delta":" of","logprobs":[{"token":" of","logprob":-1.9361265e-7,"bytes":[32,111,102]}]}

              data: {"type":"transcript.text.delta","delta":" white","logprobs":[{"token":" white","logprob":-7.89631e-7,"bytes":[32,119,104,105,116,101]}]}

              data: {"type":"transcript.text.delta","delta":",","logprobs":[{"token":",","logprob":-0.0014890312,"bytes":[44]}]}

              data: {"type":"transcript.text.delta","delta":" the","logprobs":[{"token":" the","logprob":-0.0110956915,"bytes":[32,116,104,101]}]}

              data: {"type":"transcript.text.delta","delta":" bright","logprobs":[{"token":" bright","logprob":0.0,"bytes":[32,98,114,105,103,104,116]}]}

              data: {"type":"transcript.text.delta","delta":" blessed","logprobs":[{"token":" blessed","logprob":-0.000045848617,"bytes":[32,98,108,101,115,115,101,100]}]}

              data: {"type":"transcript.text.delta","delta":" days","logprobs":[{"token":" days","logprob":-0.000010802739,"bytes":[32,100,97,121,115]}]}

              data: {"type":"transcript.text.delta","delta":",","logprobs":[{"token":",","logprob":-0.00001700133,"bytes":[44]}]}

              data: {"type":"transcript.text.delta","delta":" the","logprobs":[{"token":" the","logprob":-0.0000118755715,"bytes":[32,116,104,101]}]}

              data: {"type":"transcript.text.delta","delta":" dark","logprobs":[{"token":" dark","logprob":-5.5122365e-7,"bytes":[32,100,97,114,107]}]}

              data: {"type":"transcript.text.delta","delta":" sacred","logprobs":[{"token":" sacred","logprob":-5.4385737e-6,"bytes":[32,115,97,99,114,101,100]}]}

              data: {"type":"transcript.text.delta","delta":" nights","logprobs":[{"token":" nights","logprob":-4.00813e-6,"bytes":[32,110,105,103,104,116,115]}]}

              data: {"type":"transcript.text.delta","delta":",","logprobs":[{"token":",","logprob":-0.0036910512,"bytes":[44]}]}

              data: {"type":"transcript.text.delta","delta":" and","logprobs":[{"token":" and","logprob":-0.0031903093,"bytes":[32,97,110,100]}]}

              data: {"type":"transcript.text.delta","delta":" I","logprobs":[{"token":" I","logprob":-1.504853e-6,"bytes":[32,73]}]}

              data: {"type":"transcript.text.delta","delta":" think","logprobs":[{"token":" think","logprob":-4.3202e-7,"bytes":[32,116,104,105,110,107]}]}

              data: {"type":"transcript.text.delta","delta":" to","logprobs":[{"token":" to","logprob":-1.9361265e-7,"bytes":[32,116,111]}]}

              data: {"type":"transcript.text.delta","delta":" myself","logprobs":[{"token":" myself","logprob":-1.7432603e-6,"bytes":[32,109,121,115,101,108,102]}]}

              data: {"type":"transcript.text.delta","delta":",","logprobs":[{"token":",","logprob":-0.29254505,"bytes":[44]}]}

              data: {"type":"transcript.text.delta","delta":" what","logprobs":[{"token":" what","logprob":-0.016815351,"bytes":[32,119,104,97,116]}]}

              data: {"type":"transcript.text.delta","delta":" a","logprobs":[{"token":" a","logprob":-3.1281633e-7,"bytes":[32,97]}]}

              data: {"type":"transcript.text.delta","delta":" wonderful","logprobs":[{"token":" wonderful","logprob":-2.1008714e-6,"bytes":[32,119,111,110,100,101,114,102,117,108]}]}

              data: {"type":"transcript.text.delta","delta":" world","logprobs":[{"token":" world","logprob":-8.180258e-6,"bytes":[32,119,111,114,108,100]}]}

              data: {"type":"transcript.text.delta","delta":".","logprobs":[{"token":".","logprob":-0.014231676,"bytes":[46]}]}

              data: {"type":"transcript.text.done","text":"I see skies of blue and clouds of white, the bright blessed days, the dark sacred nights, and I think to myself, what a wonderful world.","logprobs":[{"token":"I","logprob":-0.00007588794,"bytes":[73]},{"token":" see","logprob":-3.1281633e-7,"bytes":[32,115,101,101]},{"token":" skies","logprob":-2.3392786e-6,"bytes":[32,115,107,105,101,115]},{"token":" of","logprob":-3.1281633e-7,"bytes":[32,111,102]},{"token":" blue","logprob":-1.0280384e-6,"bytes":[32,98,108,117,101]},{"token":" and","logprob":-0.0005108566,"bytes":[32,97,110,100]},{"token":" clouds","logprob":-1.9361265e-7,"bytes":[32,99,108,111,117,100,115]},{"token":" of","logprob":-1.9361265e-7,"bytes":[32,111,102]},{"token":" white","logprob":-7.89631e-7,"bytes":[32,119,104,105,116,101]},{"token":",","logprob":-0.0014890312,"bytes":[44]},{"token":" the","logprob":-0.0110956915,"bytes":[32,116,104,101]},{"token":" bright","logprob":0.0,"bytes":[32,98,114,105,103,104,116]},{"token":" blessed","logprob":-0.000045848617,"bytes":[32,98,108,101,115,115,101,100]},{"token":" days","logprob":-0.000010802739,"bytes":[32,100,97,121,115]},{"token":",","logprob":-0.00001700133,"bytes":[44]},{"token":" the","logprob":-0.0000118755715,"bytes":[32,116,104,101]},{"token":" dark","logprob":-5.5122365e-7,"bytes":[32,100,97,114,107]},{"token":" sacred","logprob":-5.4385737e-6,"bytes":[32,115,97,99,114,101,100]},{"token":" nights","logprob":-4.00813e-6,"bytes":[32,110,105,103,104,116,115]},{"token":",","logprob":-0.0036910512,"bytes":[44]},{"token":" and","logprob":-0.0031903093,"bytes":[32,97,110,100]},{"token":" I","logprob":-1.504853e-6,"bytes":[32,73]},{"token":" think","logprob":-4.3202e-7,"bytes":[32,116,104,105,110,107]},{"token":" to","logprob":-1.9361265e-7,"bytes":[32,116,111]},{"token":" myself","logprob":-1.7432603e-6,"bytes":[32,109,121,115,101,108,102]},{"token":",","logprob":-0.29254505,"bytes":[44]},{"token":" what","logprob":-0.016815351,"bytes":[32,119,104,97,116]},{"token":" a","logprob":-3.1281633e-7,"bytes":[32,97]},{"token":" wonderful","logprob":-2.1008714e-6,"bytes":[32,119,111,110,100,101,114,102,117,108]},{"token":" world","logprob":-8.180258e-6,"bytes":[32,119,111,114,108,100]},{"token":".","logprob":-0.014231676,"bytes":[46]}],"usage":{"input_tokens":14,"input_token_details":{"text_tokens":0,"audio_tokens":14},"output_tokens":45,"total_tokens":59}}
          - title: Logprobs
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F "include[]=logprobs" \
                  -F model="gpt-4o-transcribe" \
                  -F response_format="json"
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                transcription = client.audio.transcriptions.create(
                    file=b"raw file contents",
                    model="gpt-4o-transcribe",
                )
                print(transcription)
              javascript: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const transcription = await openai.audio.transcriptions.create({
                    file: fs.createReadStream("audio.mp3"),
                    model: "gpt-4o-transcribe",
                    response_format: "json",
                    include: ["logprobs"]
                  });

                  console.log(transcription);
                }
                main();
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const transcription = await client.audio.transcriptions.create({
                  file: fs.createReadStream('speech.mp3'),
                  model: 'gpt-4o-transcribe',
                });

                console.log(transcription);
              go: |
                package main

                import (
                  "bytes"
                  "context"
                  "fmt"
                  "io"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  transcription, err := client.Audio.Transcriptions.New(context.TODO(), openai.AudioTranscriptionNewParams{
                    File: io.Reader(bytes.NewBuffer([]byte("some file contents"))),
                    Model: openai.AudioModelGPT4oTranscribe,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", transcription)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.audio.AudioModel;
                import com.openai.models.audio.transcriptions.TranscriptionCreateParams;
                import com.openai.models.audio.transcriptions.TranscriptionCreateResponse;
                import java.io.ByteArrayInputStream;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        TranscriptionCreateParams params = TranscriptionCreateParams.builder()
                            .file(ByteArrayInputStream("some content".getBytes()))
                            .model(AudioModel.GPT_4O_TRANSCRIBE)
                            .build();
                        TranscriptionCreateResponse transcription = client.audio().transcriptions().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                transcription = openai.audio.transcriptions.create(file: Pathname(__FILE__), model: :"gpt-4o-transcribe")

                puts(transcription)
            response: |
              {
                "text": "Hey, my knee is hurting and I want to see the doctor tomorrow ideally.",
                "logprobs": [
                  { "token": "Hey", "logprob": -1.0415299, "bytes": [72, 101, 121] },
                  { "token": ",", "logprob": -9.805982e-5, "bytes": [44] },
                  { "token": " my", "logprob": -0.00229799, "bytes": [32, 109, 121] },
                  {
                    "token": " knee",
                    "logprob": -4.7159858e-5,
                    "bytes": [32, 107, 110, 101, 101]
                  },
                  { "token": " is", "logprob": -0.043909557, "bytes": [32, 105, 115] },
                  {
                    "token": " hurting",
                    "logprob": -1.1041146e-5,
                    "bytes": [32, 104, 117, 114, 116, 105, 110, 103]
                  },
                  { "token": " and", "logprob": -0.011076359, "bytes": [32, 97, 110, 100] },
                  { "token": " I", "logprob": -5.3193703e-6, "bytes": [32, 73] },
                  {
                    "token": " want",
                    "logprob": -0.0017156356,
                    "bytes": [32, 119, 97, 110, 116]
                  },
                  { "token": " to", "logprob": -7.89631e-7, "bytes": [32, 116, 111] },
                  { "token": " see", "logprob": -5.5122365e-7, "bytes": [32, 115, 101, 101] },
                  { "token": " the", "logprob": -0.0040786397, "bytes": [32, 116, 104, 101] },
                  {
                    "token": " doctor",
                    "logprob": -2.3392786e-6,
                    "bytes": [32, 100, 111, 99, 116, 111, 114]
                  },
                  {
                    "token": " tomorrow",
                    "logprob": -7.89631e-7,
                    "bytes": [32, 116, 111, 109, 111, 114, 114, 111, 119]
                  },
                  {
                    "token": " ideally",
                    "logprob": -0.5800861,
                    "bytes": [32, 105, 100, 101, 97, 108, 108, 121]
                  },
                  { "token": ".", "logprob": -0.00011093382, "bytes": [46] }
                ],
                "usage": {
                  "type": "tokens",
                  "input_tokens": 14,
                  "input_token_details": {
                    "text_tokens": 0,
                    "audio_tokens": 14
                  },
                  "output_tokens": 45,
                  "total_tokens": 59
                }
              }
          - title: Word timestamps
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F "timestamp_granularities[]=word" \
                  -F model="whisper-1" \
                  -F response_format="verbose_json"
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                transcription = client.audio.transcriptions.create(
                    file=b"raw file contents",
                    model="gpt-4o-transcribe",
                )
                print(transcription)
              javascript: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const transcription = await openai.audio.transcriptions.create({
                    file: fs.createReadStream("audio.mp3"),
                    model: "whisper-1",
                    response_format: "verbose_json",
                    timestamp_granularities: ["word"]
                  });

                  console.log(transcription.text);
                }
                main();
              csharp: |
                using System;

                using OpenAI.Audio;

                string audioFilePath = "audio.mp3";

                AudioClient client = new(
                    model: "whisper-1",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                AudioTranscriptionOptions options = new()
                {
                    ResponseFormat = AudioTranscriptionFormat.Verbose,
                    TimestampGranularities = AudioTimestampGranularities.Word,
                };

                AudioTranscription transcription = client.TranscribeAudio(audioFilePath, options);

                Console.WriteLine($"{transcription.Text}");
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const transcription = await client.audio.transcriptions.create({
                  file: fs.createReadStream('speech.mp3'),
                  model: 'gpt-4o-transcribe',
                });

                console.log(transcription);
              go: |
                package main

                import (
                  "bytes"
                  "context"
                  "fmt"
                  "io"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  transcription, err := client.Audio.Transcriptions.New(context.TODO(), openai.AudioTranscriptionNewParams{
                    File: io.Reader(bytes.NewBuffer([]byte("some file contents"))),
                    Model: openai.AudioModelGPT4oTranscribe,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", transcription)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.audio.AudioModel;
                import com.openai.models.audio.transcriptions.TranscriptionCreateParams;
                import com.openai.models.audio.transcriptions.TranscriptionCreateResponse;
                import java.io.ByteArrayInputStream;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        TranscriptionCreateParams params = TranscriptionCreateParams.builder()
                            .file(ByteArrayInputStream("some content".getBytes()))
                            .model(AudioModel.GPT_4O_TRANSCRIBE)
                            .build();
                        TranscriptionCreateResponse transcription = client.audio().transcriptions().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                transcription = openai.audio.transcriptions.create(file: Pathname(__FILE__), model: :"gpt-4o-transcribe")

                puts(transcription)
            response: |
              {
                "task": "transcribe",
                "language": "english",
                "duration": 8.470000267028809,
                "text": "The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball.",
                "words": [
                  {
                    "word": "The",
                    "start": 0.0,
                    "end": 0.23999999463558197
                  },
                  ...
                  {
                    "word": "volleyball",
                    "start": 7.400000095367432,
                    "end": 7.900000095367432
                  }
                ],
                "usage": {
                  "type": "duration",
                  "seconds": 9
                }
              }
          - title: Segment timestamps
            request:
              curl: |
                curl https://api.openai.com/v1/audio/transcriptions \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -H "Content-Type: multipart/form-data" \
                  -F file="@/path/to/file/audio.mp3" \
                  -F "timestamp_granularities[]=segment" \
                  -F model="whisper-1" \
                  -F response_format="verbose_json"
              python: |-
                import os
                from openai import OpenAI

                client = OpenAI(
                    api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
                )
                transcription = client.audio.transcriptions.create(
                    file=b"raw file contents",
                    model="gpt-4o-transcribe",
                )
                print(transcription)
              javascript: |
                import fs from "fs";
                import OpenAI from "openai";

                const openai = new OpenAI();

                async function main() {
                  const transcription = await openai.audio.transcriptions.create({
                    file: fs.createReadStream("audio.mp3"),
                    model: "whisper-1",
                    response_format: "verbose_json",
                    timestamp_granularities: ["segment"]
                  });

                  console.log(transcription.text);
                }
                main();
              csharp: |
                using System;

                using OpenAI.Audio;

                string audioFilePath = "audio.mp3";

                AudioClient client = new(
                    model: "whisper-1",
                    apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
                );

                AudioTranscriptionOptions options = new()
                {
                    ResponseFormat = AudioTranscriptionFormat.Verbose,
                    TimestampGranularities = AudioTimestampGranularities.Segment,
                };

                AudioTranscription transcription = client.TranscribeAudio(audioFilePath, options);

                Console.WriteLine($"{transcription.Text}");
              node.js: |-
                import OpenAI from 'openai';

                const client = new OpenAI({
                  apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
                });

                const transcription = await client.audio.transcriptions.create({
                  file: fs.createReadStream('speech.mp3'),
                  model: 'gpt-4o-transcribe',
                });

                console.log(transcription);
              go: |
                package main

                import (
                  "bytes"
                  "context"
                  "fmt"
                  "io"

                  "github.com/openai/openai-go"
                  "github.com/openai/openai-go/option"
                )

                func main() {
                  client := openai.NewClient(
                    option.WithAPIKey("My API Key"),
                  )
                  transcription, err := client.Audio.Transcriptions.New(context.TODO(), openai.AudioTranscriptionNewParams{
                    File: io.Reader(bytes.NewBuffer([]byte("some file contents"))),
                    Model: openai.AudioModelGPT4oTranscribe,
                  })
                  if err != nil {
                    panic(err.Error())
                  }
                  fmt.Printf("%+v\n", transcription)
                }
              java: |-
                package com.openai.example;

                import com.openai.client.OpenAIClient;
                import com.openai.client.okhttp.OpenAIOkHttpClient;
                import com.openai.models.audio.AudioModel;
                import com.openai.models.audio.transcriptions.TranscriptionCreateParams;
                import com.openai.models.audio.transcriptions.TranscriptionCreateResponse;
                import java.io.ByteArrayInputStream;

                public final class Main {
                    private Main() {}

                    public static void main(String[] args) {
                        OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                        TranscriptionCreateParams params = TranscriptionCreateParams.builder()
                            .file(ByteArrayInputStream("some content".getBytes()))
                            .model(AudioModel.GPT_4O_TRANSCRIBE)
                            .build();
                        TranscriptionCreateResponse transcription = client.audio().transcriptions().create(params);
                    }
                }
              ruby: |-
                require "openai"

                openai = OpenAI::Client.new(api_key: "My API Key")

                transcription = openai.audio.transcriptions.create(file: Pathname(__FILE__), model: :"gpt-4o-transcribe")

                puts(transcription)
            response: |
              {
                "task": "transcribe",
                "language": "english",
                "duration": 8.470000267028809,
                "text": "The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball.",
                "segments": [
                  {
                    "id": 0,
                    "seek": 0,
                    "start": 0.0,
                    "end": 3.319999933242798,
                    "text": " The beach was a popular spot on a hot summer day.",
                    "tokens": [
                      50364, 440, 7534, 390, 257, 3743, 4008, 322, 257, 2368, 4266, 786, 13, 50530
                    ],
                    "temperature": 0.0,
                    "avg_logprob": -0.2860786020755768,
                    "compression_ratio": 1.2363636493682861,
                    "no_speech_prob": 0.00985979475080967
                  },
                  ...
                ],
                "usage": {
                  "type": "duration",
                  "seconds": 9
                }
              }
      description: Transcribes audio into the input language.
      parameters: []
  /audio/translations:
    post:
      operationId: createTranslation
      tags:
        - Audio
      summary: Create translation
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/CreateTranslationRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                anyOf:
                  - $ref: '#/components/schemas/CreateTranslationResponseJson'
                  - $ref: '#/components/schemas/CreateTranslationResponseVerboseJson'
                    x-stainless-skip:
                      - go
      x-oaiMeta:
        name: Create translation
        group: audio
        returns: The translated text.
        examples:
          response: |
            {
              "text": "Hello, my name is Wolfgang and I come from Germany. Where are you heading today?"
            }
          request:
            curl: |
              curl https://api.openai.com/v1/audio/translations \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: multipart/form-data" \
                -F file="@/path/to/file/german.m4a" \
                -F model="whisper-1"
            python: |-
              import os
              from openai import OpenAI

              client = OpenAI(
                  api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
              )
              translation = client.audio.translations.create(
                  file=b"raw file contents",
                  model="whisper-1",
              )
              print(translation)
            javascript: |
              import fs from "fs";
              import OpenAI from "openai";

              const openai = new OpenAI();

              async function main() {
                  const translation = await openai.audio.translations.create({
                      file: fs.createReadStream("speech.mp3"),
                      model: "whisper-1",
                  });

                  console.log(translation.text);
              }
              main();
            csharp: |
              using System;

              using OpenAI.Audio;

              string audioFilePath = "audio.mp3";

              AudioClient client = new(
                  model: "whisper-1",
                  apiKey: Environment.GetEnvironmentVariable("OPENAI_API_KEY")
              );

              AudioTranscription transcription = client.TranscribeAudio(audioFilePath);

              Console.WriteLine($"{transcription.Text}");
            node.js: |-
              import OpenAI from 'openai';

              const client = new OpenAI({
                apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
              });

              const translation = await client.audio.translations.create({
                file: fs.createReadStream('speech.mp3'),
                model: 'whisper-1',
              });

              console.log(translation);
            go: |
              package main

              import (
                "bytes"
                "context"
                "fmt"
                "io"

                "github.com/openai/openai-go"
                "github.com/openai/openai-go/option"
              )

              func main() {
                client := openai.NewClient(
                  option.WithAPIKey("My API Key"),
                )
                translation, err := client.Audio.Translations.New(context.TODO(), openai.AudioTranslationNewParams{
                  File: io.Reader(bytes.NewBuffer([]byte("some file contents"))),
                  Model: openai.AudioModelWhisper1,
                })
                if err != nil {
                  panic(err.Error())
                }
                fmt.Printf("%+v\n", translation)
              }
            java: |-
              package com.openai.example;

              import com.openai.client.OpenAIClient;
              import com.openai.client.okhttp.OpenAIOkHttpClient;
              import com.openai.models.audio.AudioModel;
              import com.openai.models.audio.translations.TranslationCreateParams;
              import com.openai.models.audio.translations.TranslationCreateResponse;
              import java.io.ByteArrayInputStream;

              public final class Main {
                  private Main() {}

                  public static void main(String[] args) {
                      OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                      TranslationCreateParams params = TranslationCreateParams.builder()
                          .file(ByteArrayInputStream("some content".getBytes()))
                          .model(AudioModel.WHISPER_1)
                          .build();
                      TranslationCreateResponse translation = client.audio().translations().create(params);
                  }
              }
            ruby: |-
              require "openai"

              openai = OpenAI::Client.new(api_key: "My API Key")

              translation = openai.audio.translations.create(file: Pathname(__FILE__), model: :"whisper-1")

              puts(translation)
      description: Translates audio into English.
      parameters: []
  /audio/voices:
    post:
      operationId: createVoice
      tags:
        - Audio
      summary: Create voice
      description: Creates a custom voice.
      requestBody:
        required: true
        content:
          multipart/form-data:
            schema:
              $ref: '#/components/schemas/CreateVoiceRequest'
      responses:
        '200':
          description: OK
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/VoiceResource'
      x-oaiMeta:
        name: Create voice
        group: audio
        returns: The created voice.
        examples:
          - title: Default
            request:
              curl: |
                curl https://api.openai.com/v1/audio/voices \
                  -X POST \
                  -H "Authorization: Bearer $OPENAI_API_KEY" \
                  -F "name=My new voice" \
                  -F "consent=cons_1234" \
                  -F "audio_sample=@$HOME/audio_sample.wav;type=audio/x-wav"
      parameters: []
  /realtime/client_secrets:
    post:
      summary: Create client secret
      operationId: create-realtime-client-secret
      tags:
        - Realtime
      requestBody:
        description: Create a client secret with the given session configuration.
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RealtimeCreateClientSecretRequest'
      responses:
        '200':
          description: Client secret created successfully.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RealtimeCreateClientSecretResponse'
      x-oaiMeta:
        name: Create client secret
        group: realtime
        returns: The created client secret and the effective session object. The client secret is a string that looks like `ek_1234`.
        examples:
          response: |
            {
              "value": "ek_68af296e8e408191a1120ab6383263c2",
              "expires_at": 1756310470,
              "session": {
                "type": "realtime",
                "object": "realtime.session",
                "id": "sess_C9CiUVUzUzYIssh3ELY1d",
                "model": "gpt-realtime",
                "output_modalities": [
                  "audio"
                ],
                "instructions": "You are a friendly assistant.",
                "tools": [],
                "tool_choice": "auto",
                "max_output_tokens": "inf",
                "tracing": null,
                "truncation": "auto",
                "prompt": null,
                "expires_at": 0,
                "audio": {
                  "input": {
                    "format": {
                      "type": "audio/pcm",
                      "rate": 24000
                    },
                    "transcription": null,
                    "noise_reduction": null,
                    "turn_detection": {
                      "type": "server_vad",
                    }
                  },
                  "output": {
                    "format": {
                      "type": "audio/pcm",
                      "rate": 24000
                    },
                    "voice": "alloy",
                    "speed": 1.0
                  }
                },
                "include": null
              }
            }
          request:
            curl: |
              curl -X POST https://api.openai.com/v1/realtime/client_secrets \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                  "expires_after": {
                    "anchor": "created_at",
                    "seconds": 600
                  },
                  "session": {
                    "type": "realtime",
                    "model": "gpt-realtime",
                    "instructions": "You are a friendly assistant."
                  }
                }'
            node.js: |-
              import OpenAI from 'openai';

              const client = new OpenAI({
                apiKey: process.env['OPENAI_API_KEY'], // This is the default and can be omitted
              });

              const clientSecret = await client.realtime.clientSecrets.create();

              console.log(clientSecret.expires_at);
            python: |-
              import os
              from openai import OpenAI

              client = OpenAI(
                  api_key=os.environ.get("OPENAI_API_KEY"),  # This is the default and can be omitted
              )
              client_secret = client.realtime.client_secrets.create()
              print(client_secret.expires_at)
            go: |
              package main

              import (
                "context"
                "fmt"

                "github.com/openai/openai-go"
                "github.com/openai/openai-go/option"
                "github.com/openai/openai-go/realtime"
              )

              func main() {
                client := openai.NewClient(
                  option.WithAPIKey("My API Key"),
                )
                clientSecret, err := client.Realtime.ClientSecrets.New(context.TODO(), realtime.ClientSecretNewParams{

                })
                if err != nil {
                  panic(err.Error())
                }
                fmt.Printf("%+v\n", clientSecret.ExpiresAt)
              }
            java: |-
              package com.openai.example;

              import com.openai.client.OpenAIClient;
              import com.openai.client.okhttp.OpenAIOkHttpClient;
              import com.openai.models.realtime.clientsecrets.ClientSecretCreateParams;
              import com.openai.models.realtime.clientsecrets.ClientSecretCreateResponse;

              public final class Main {
                  private Main() {}

                  public static void main(String[] args) {
                      OpenAIClient client = OpenAIOkHttpClient.fromEnv();

                      ClientSecretCreateResponse clientSecret = client.realtime().clientSecrets().create();
                  }
              }
            ruby: |-
              require "openai"

              openai = OpenAI::Client.new(api_key: "My API Key")

              client_secret = openai.realtime.client_secrets.create

              puts(client_secret)
      description: |
        Create a Realtime client secret with an associated session configuration.
      parameters: []
  /realtime/sessions:
    post:
      summary: Create session
      operationId: create-realtime-session
      tags:
        - Realtime
      requestBody:
        description: Create an ephemeral API key with the given session configuration.
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RealtimeSessionCreateRequest'
      responses:
        '200':
          description: Session created successfully.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RealtimeSessionCreateResponse'
      x-oaiMeta:
        name: Create session
        group: realtime
        returns: The created Realtime session object, plus an ephemeral key
        examples:
          request:
            curl: |
              curl -X POST https://api.openai.com/v1/realtime/sessions \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{
                  "model": "gpt-realtime",
                  "modalities": ["audio", "text"],
                  "instructions": "You are a friendly assistant."
                }'
          response: |
            {
              "id": "sess_001",
              "object": "realtime.session",
              "model": "gpt-realtime-2025-08-25",
              "modalities": ["audio", "text"],
              "instructions": "You are a friendly assistant.",
              "voice": "alloy",
              "input_audio_format": "pcm16",
              "output_audio_format": "pcm16",
              "input_audio_transcription": {
                  "model": "whisper-1"
              },
              "turn_detection": null,
              "tools": [],
              "tool_choice": "none",
              "temperature": 0.7,
              "max_response_output_tokens": 200,
              "speed": 1.1,
              "tracing": "auto",
              "client_secret": {
                "value": "ek_abc123", 
                "expires_at": 1234567890
              }
            }
      description: |
        Create an ephemeral API token for use in client-side applications with the
        Realtime API. Can be configured with the same session parameters as the
        `session.update` client event.

        It responds with a session object, plus a `client_secret` key which contains
        a usable ephemeral API token that can be used to authenticate browser clients
        for the Realtime API.
      parameters: []
  /realtime/transcription_sessions:
    post:
      summary: Create transcription session
      operationId: create-realtime-transcription-session
      tags:
        - Realtime
      requestBody:
        description: Create an ephemeral API key with the given session configuration.
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/RealtimeTranscriptionSessionCreateRequest'
      responses:
        '200':
          description: Session created successfully.
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/RealtimeTranscriptionSessionCreateResponse'
      x-oaiMeta:
        name: Create transcription session
        group: realtime
        returns: The created [Realtime transcription session object](https://platform.openai.com/docs/api-reference/realtime-sessions/transcription_session_object), plus an ephemeral key
        examples:
          request:
            curl: |
              curl -X POST https://api.openai.com/v1/realtime/transcription_sessions \
                -H "Authorization: Bearer $OPENAI_API_KEY" \
                -H "Content-Type: application/json" \
                -d '{}'
          response: |
            {
              "id": "sess_BBwZc7cFV3XizEyKGDCGL",
              "object": "realtime.transcription_session",
              "modalities": ["audio", "text"],
              "turn_detection": {
                "type": "server_vad",
                "threshold": 0.5,
                "prefix_padding_ms": 300,
                "silence_duration_ms": 200
              },
              "input_audio_format": "pcm16",
              "input_audio_transcription": {
                "model": "gpt-4o-transcribe",
                "language": null,
                "prompt": ""
              },
              "client_secret": null
            }
      description: |
        Create an ephemeral API token for use in client-side applications with the
        Realtime API specifically for realtime transcriptions. 
        Can be configured with the same session parameters as the `transcription_session.update` client event.

        It responds with a session object, plus a `client_secret` key which contains
        a usable ephemeral API token that can be used to authenticate browser clients
        for the Realtime API.
      parameters: []
webhooks:
  batch_cancelled:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookBatchCancelled'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried.
  batch_completed:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookBatchCompleted'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried.
  batch_expired:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookBatchExpired'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried.
  batch_failed:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookBatchFailed'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried.
  eval_run_canceled:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookEvalRunCanceled'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried. 
  eval_run_failed:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookEvalRunFailed'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried. 
  eval_run_succeeded:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookEvalRunSucceeded'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried. 
  fine_tuning_job_cancelled:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookFineTuningJobCancelled'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried. 
  fine_tuning_job_failed:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookFineTuningJobFailed'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried. 
  fine_tuning_job_succeeded:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookFineTuningJobSucceeded'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried. 
  realtime_call_incoming:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookRealtimeCallIncoming'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200
            status codes will be retried.
  response_cancelled:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookResponseCancelled'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried.
  response_completed:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookResponseCompleted'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried. 
  response_failed:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookResponseFailed'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried.
  response_incomplete:
    post:
      requestBody:
        description: The event payload sent by the API.
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/WebhookResponseIncomplete'
      responses:
        '200':
          description: |
            Return a 200 status code to acknowledge receipt of the event. Non-200 
            status codes will be retried.
components:
  schemas:
    AudioResponseFormat:
      description: |
        The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, `vtt`, or `diarized_json`. For `gpt-4o-transcribe` and `gpt-4o-mini-transcribe`, the only supported format is `json`. For `gpt-4o-transcribe-diarize`, the supported formats are `json`, `text`, and `diarized_json`, with `diarized_json` required to receive speaker annotations.
      type: string
      enum:
        - json
        - text
        - srt
        - verbose_json
        - vtt
        - diarized_json
      default: json
    AudioTranscription:
      type: object
      properties:
        model:
          description: |
            The model to use for transcription. Current options are `whisper-1`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `gpt-4o-transcribe`, and `gpt-4o-transcribe-diarize`. Use `gpt-4o-transcribe-diarize` when you need diarization with speaker labels.
          anyOf:
            - type: string
            - type: string
              enum:
                - whisper-1
                - gpt-4o-mini-transcribe
                - gpt-4o-mini-transcribe-2025-12-15
                - gpt-4o-transcribe
                - gpt-4o-transcribe-diarize
        language:
          type: string
          description: |
            The language of the input audio. Supplying the input language in
            [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format
            will improve accuracy and latency.
        prompt:
          type: string
          description: |
            An optional text to guide the model's style or continue a previous audio
            segment.
            For `whisper-1`, the [prompt is a list of keywords](https://platform.openai.com/docs/guides/speech-to-text#prompting).
            For `gpt-4o-transcribe` models (excluding `gpt-4o-transcribe-diarize`), the prompt is a free text string, for example "expect words related to technology".
    CreateSpeechRequest:
      type: object
      additionalProperties: false
      properties:
        model:
          description: |
            One of the available [TTS models](https://platform.openai.com/docs/models#tts): `tts-1`, `tts-1-hd`, `gpt-4o-mini-tts`, or `gpt-4o-mini-tts-2025-12-15`.
          anyOf:
            - type: string
            - type: string
              enum:
                - tts-1
                - tts-1-hd
                - gpt-4o-mini-tts
                - gpt-4o-mini-tts-2025-12-15
              x-stainless-nominal: false
          x-oaiTypeLabel: string
        input:
          type: string
          description: The text to generate audio for. The maximum length is 4096 characters.
          maxLength: 4096
        instructions:
          type: string
          description: Control the voice of your generated audio with additional instructions. Does not work with `tts-1` or `tts-1-hd`.
          maxLength: 4096
        voice:
          description: The voice to use when generating the audio. Supported built-in voices are `alloy`, `ash`, `ballad`, `coral`, `echo`, `fable`, `onyx`, `nova`, `sage`, `shimmer`, `verse`, `marin`, and `cedar`. Previews of the voices are available in the [Text to speech guide](https://platform.openai.com/docs/guides/text-to-speech#voice-options).
          $ref: '#/components/schemas/VoiceIdsShared'
        response_format:
          description: The format to audio in. Supported formats are `mp3`, `opus`, `aac`, `flac`, `wav`, and `pcm`.
          default: mp3
          type: string
          enum:
            - mp3
            - opus
            - aac
            - flac
            - wav
            - pcm
        speed:
          description: The speed of the generated audio. Select a value from `0.25` to `4.0`. `1.0` is the default.
          type: number
          default: 1
          minimum: 0.25
          maximum: 4
        stream_format:
          description: The format to stream the audio in. Supported formats are `sse` and `audio`. `sse` is not supported for `tts-1` or `tts-1-hd`.
          type: string
          default: audio
          enum:
            - sse
            - audio
      required:
        - model
        - input
        - voice
    CreateSpeechResponseStreamEvent:
      anyOf:
        - $ref: '#/components/schemas/SpeechAudioDeltaEvent'
        - $ref: '#/components/schemas/SpeechAudioDoneEvent'
      discriminator:
        propertyName: type
    CreateTranscriptionRequest:
      type: object
      additionalProperties: false
      properties:
        file:
          description: |
            The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
          type: string
          x-oaiTypeLabel: file
          format: binary
          x-oaiMeta:
            exampleFilePath: speech.mp3
        model:
          description: |
            ID of the model to use. The options are `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, `gpt-4o-mini-transcribe-2025-12-15`, `whisper-1` (which is powered by our open source Whisper V2 model), and `gpt-4o-transcribe-diarize`.
          example: gpt-4o-transcribe
          anyOf:
            - type: string
            - type: string
              enum:
                - whisper-1
                - gpt-4o-transcribe
                - gpt-4o-mini-transcribe
                - gpt-4o-mini-transcribe-2025-12-15
                - gpt-4o-transcribe-diarize
              x-stainless-const: true
              x-stainless-nominal: false
          x-oaiTypeLabel: string
        language:
          description: |
            The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) (e.g. `en`) format will improve accuracy and latency.
          type: string
        prompt:
          description: |
            An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should match the audio language. This field is not supported when using `gpt-4o-transcribe-diarize`.
          type: string
        response_format:
          $ref: '#/components/schemas/AudioResponseFormat'
        temperature:
          description: |
            The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
          type: number
          default: 0
        include:
          description: |
            Additional information to include in the transcription response.
            `logprobs` will return the log probabilities of the tokens in the
            response to understand the model's confidence in the transcription.
            `logprobs` only works with response_format set to `json` and only with
            the models `gpt-4o-transcribe`, `gpt-4o-mini-transcribe`, and `gpt-4o-mini-transcribe-2025-12-15`. This field is not supported when using `gpt-4o-transcribe-diarize`.
          type: array
          items:
            $ref: '#/components/schemas/TranscriptionInclude'
        timestamp_granularities:
          description: |
            The timestamp granularities to populate for this transcription. `response_format` must be set `verbose_json` to use timestamp granularities. Either or both of these options are supported: `word`, or `segment`. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency.
            This option is not available for `gpt-4o-transcribe-diarize`.
          type: array
          items:
            type: string
            enum:
              - word
              - segment
          default:
            - segment
        stream:
          anyOf:
            - description: |
                If set to true, the model response data will be streamed to the client
                as it is generated using [server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#Event_stream_format).
                See the [Streaming section of the Speech-to-Text guide](https://platform.openai.com/docs/guides/speech-to-text?lang=curl#streaming-transcriptions)
                for more information.

                Note: Streaming is not supported for the `whisper-1` model and will be ignored.
              type: boolean
              default: false
            - type: 'null'
        chunking_strategy:
          $ref: '#/components/schemas/TranscriptionChunkingStrategy'
        known_speaker_names:
          description: |
            Optional list of speaker names that correspond to the audio samples provided in `known_speaker_references[]`. Each entry should be a short identifier (for example `customer` or `agent`). Up to 4 speakers are supported.
          type: array
          maxItems: 4
          items:
            type: string
        known_speaker_references:
          description: |
            Optional list of audio samples (as [data URLs](https://developer.mozilla.org/en-US/docs/Web/HTTP/Basics_of_HTTP/Data_URLs)) that contain known speaker references matching `known_speaker_names[]`. Each sample must be between 2 and 10 seconds, and can use any of the same input audio formats supported by `file`.
          type: array
          maxItems: 4
          items:
            type: string
      required:
        - file
        - model
    CreateTranscriptionResponseDiarizedJson:
      type: object
      description: |
        Represents a diarized transcription response returned by the model, including the combined transcript and speaker-segment annotations.
      properties:
        task:
          type: string
          description: The type of task that was run. Always `transcribe`.
          enum:
            - transcribe
          x-stainless-const: true
        duration:
          type: number
          description: Duration of the input audio in seconds.
        text:
          type: string
          description: The concatenated transcript text for the entire audio input.
        segments:
          type: array
          description: Segments of the transcript annotated with timestamps and speaker labels.
          items:
            $ref: '#/components/schemas/TranscriptionDiarizedSegment'
        usage:
          type: object
          description: Token or duration usage statistics for the request.
          discriminator:
            propertyName: type
          anyOf:
            - $ref: '#/components/schemas/TranscriptTextUsageTokens'
              title: Token Usage
            - $ref: '#/components/schemas/TranscriptTextUsageDuration'
              title: Duration Usage
      required:
        - task
        - duration
        - text
        - segments
      x-oaiMeta:
        name: The transcription object (Diarized JSON)
        group: audio
        example: |
          {
            "task": "transcribe",
            "duration": 42.7,
            "text": "Agent: Thanks for calling OpenAI support.\nCustomer: Hi, I need help with diarization.",
            "segments": [
              {
                "type": "transcript.text.segment",
                "id": "seg_001",
                "start": 0.0,
                "end": 5.2,
                "text": "Thanks for calling OpenAI support.",
                "speaker": "agent"
              },
              {
                "type": "transcript.text.segment",
                "id": "seg_002",
                "start": 5.2,
                "end": 12.8,
                "text": "Hi, I need help with diarization.",
                "speaker": "A"
              }
            ],
            "usage": {
              "type": "duration",
              "seconds": 43
            }
          }
    CreateTranscriptionResponseJson:
      type: object
      description: Represents a transcription response returned by model, based on the provided input.
      properties:
        text:
          type: string
          description: The transcribed text.
        logprobs:
          type: array
          optional: true
          description: |
            The log probabilities of the tokens in the transcription. Only returned with the models `gpt-4o-transcribe` and `gpt-4o-mini-transcribe` if `logprobs` is added to the `include` array.
          items:
            type: object
            properties:
              token:
                type: string
                description: The token in the transcription.
              logprob:
                type: number
                description: The log probability of the token.
              bytes:
                type: array
                items:
                  type: number
                description: The bytes of the token.
        usage:
          type: object
          description: Token usage statistics for the request.
          anyOf:
            - $ref: '#/components/schemas/TranscriptTextUsageTokens'
              title: Token Usage
            - $ref: '#/components/schemas/TranscriptTextUsageDuration'
              title: Duration Usage
          discriminator:
            propertyName: type
      required:
        - text
      x-oaiMeta:
        name: The transcription object (JSON)
        group: audio
        example: |
          {
            "text": "Imagine the wildest idea that you've ever had, and you're curious about how it might scale to something that's a 100, a 1,000 times bigger. This is a place where you can get to do that.",
            "usage": {
              "type": "tokens",
              "input_tokens": 14,
              "input_token_details": {
                "text_tokens": 10,
                "audio_tokens": 4
              },
              "output_tokens": 101,
              "total_tokens": 115
            }
          }
    CreateTranscriptionResponseStreamEvent:
      anyOf:
        - $ref: '#/components/schemas/TranscriptTextSegmentEvent'
        - $ref: '#/components/schemas/TranscriptTextDeltaEvent'
        - $ref: '#/components/schemas/TranscriptTextDoneEvent'
      discriminator:
        propertyName: type
    CreateTranscriptionResponseVerboseJson:
      type: object
      description: Represents a verbose json transcription response returned by model, based on the provided input.
      properties:
        language:
          type: string
          description: The language of the input audio.
        duration:
          type: number
          description: The duration of the input audio.
        text:
          type: string
          description: The transcribed text.
        words:
          type: array
          description: Extracted words and their corresponding timestamps.
          items:
            $ref: '#/components/schemas/TranscriptionWord'
        segments:
          type: array
          description: Segments of the transcribed text and their corresponding details.
          items:
            $ref: '#/components/schemas/TranscriptionSegment'
        usage:
          $ref: '#/components/schemas/TranscriptTextUsageDuration'
      required:
        - language
        - duration
        - text
      x-oaiMeta:
        name: The transcription object (Verbose JSON)
        group: audio
        example: |
          {
            "task": "transcribe",
            "language": "english",
            "duration": 8.470000267028809,
            "text": "The beach was a popular spot on a hot summer day. People were swimming in the ocean, building sandcastles, and playing beach volleyball.",
            "segments": [
              {
                "id": 0,
                "seek": 0,
                "start": 0.0,
                "end": 3.319999933242798,
                "text": " The beach was a popular spot on a hot summer day.",
                "tokens": [
                  50364, 440, 7534, 390, 257, 3743, 4008, 322, 257, 2368, 4266, 786, 13, 50530
                ],
                "temperature": 0.0,
                "avg_logprob": -0.2860786020755768,
                "compression_ratio": 1.2363636493682861,
                "no_speech_prob": 0.00985979475080967
              },
              ...
            ],
            "usage": {
              "type": "duration",
              "seconds": 9
            }
          }
    CreateTranslationRequest:
      type: object
      additionalProperties: false
      properties:
        file:
          description: |
            The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm.
          type: string
          x-oaiTypeLabel: file
          format: binary
          x-oaiMeta:
            exampleFilePath: speech.mp3
        model:
          description: |
            ID of the model to use. Only `whisper-1` (which is powered by our open source Whisper V2 model) is currently available.
          example: whisper-1
          anyOf:
            - type: string
            - type: string
              enum:
                - whisper-1
              x-stainless-const: true
          x-oaiTypeLabel: string
        prompt:
          description: |
            An optional text to guide the model's style or continue a previous audio segment. The [prompt](https://platform.openai.com/docs/guides/speech-to-text#prompting) should be in English.
          type: string
        response_format:
          description: |
            The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`.
          type: string
          enum:
            - json
            - text
            - srt
            - verbose_json
            - vtt
          default: json
        temperature:
          description: |
            The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit.
          type: number
          default: 0
      required:
        - file
        - model
    CreateTranslationResponseJson:
      type: object
      properties:
        text:
          type: string
      required:
        - text
    CreateTranslationResponseVerboseJson:
      type: object
      properties:
        language:
          type: string
          description: The language of the output translation (always `english`).
        duration:
          type: number
          description: The duration of the input audio.
        text:
          type: string
          description: The translated text.
        segments:
          type: array
          description: Segments of the translated text and their corresponding details.
          items:
            $ref: '#/components/schemas/TranscriptionSegment'
      required:
        - language
        - duration
        - text
    CreateVoiceRequest:
      type: object
      additionalProperties: false
      properties:
        name:
          type: string
          description: The name of the new voice.
        audio_sample:
          type: string
          format: binary
          x-oaiTypeLabel: file
          description: |
            The sample audio recording file. Maximum size is 10 MiB.

            Supported MIME types:
            `audio/mpeg`, `audio/wav`, `audio/x-wav`, `audio/ogg`, `audio/aac`, `audio/flac`, `audio/webm`, `audio/mp4`.
        consent:
          type: string
          description: The consent recording ID (for example, `cons_1234`).
      required:
        - name
        - audio_sample
        - consent
    MCPTool:
      type: object
      title: MCP tool
      description: |
        Give the model access to additional tools via remote Model Context Protocol
        (MCP) servers. [Learn more about MCP](https://platform.openai.com/docs/guides/tools-remote-mcp).
      properties:
        type:
          type: string
          enum:
            - mcp
          description: The type of the MCP tool. Always `mcp`.
          x-stainless-const: true
        server_label:
          type: string
          description: |
            A label for this MCP server, used to identify it in tool calls.
        server_url:
          type: string
          description: |
            The URL for the MCP server. One of `server_url` or `connector_id` must be
            provided.
        connector_id:
          type: string
          enum:
            - connector_dropbox
            - connector_gmail
            - connector_googlecalendar
            - connector_googledrive
            - connector_microsoftteams
            - connector_outlookcalendar
            - connector_outlookemail
            - connector_sharepoint
          description: |
            Identifier for service connectors, like those available in ChatGPT. One of
            `server_url` or `connector_id` must be provided. Learn more about service
            connectors [here](https://platform.openai.com/docs/guides/tools-remote-mcp#connectors).

            Currently supported `connector_id` values are:

            - Dropbox: `connector_dropbox`
            - Gmail: `connector_gmail`
            - Google Calendar: `connector_googlecalendar`
            - Google Drive: `connector_googledrive`
            - Microsoft Teams: `connector_microsoftteams`
            - Outlook Calendar: `connector_outlookcalendar`
            - Outlook Email: `connector_outlookemail`
            - SharePoint: `connector_sharepoint`
        authorization:
          type: string
          description: |
            An OAuth access token that can be used with a remote MCP server, either
            with a custom MCP server URL or a service connector. Your application
            must handle the OAuth authorization flow and provide the token here.
        server_description:
          type: string
          description: |
            Optional description of the MCP server, used to provide more context.
        headers:
          anyOf:
            - type: object
              additionalProperties:
                type: string
              description: |
                Optional HTTP headers to send to the MCP server. Use for authentication
                or other purposes.
            - type: 'null'
        allowed_tools:
          anyOf:
            - description: |
                List of allowed tool names or a filter object.
              anyOf:
                - type: array
                  title: MCP allowed tools
                  description: A string array of allowed tool names
                  items:
                    type: string
                - $ref: '#/components/schemas/MCPToolFilter'
            - type: 'null'
        require_approval:
          anyOf:
            - description: Specify which of the MCP server's tools require approval.
              default: always
              anyOf:
                - type: object
                  title: MCP tool approval filter
                  description: |
                    Specify which of the MCP server's tools require approval. Can be
                    `always`, `never`, or a filter object associated with tools
                    that require approval.
                  properties:
                    always:
                      $ref: '#/components/schemas/MCPToolFilter'
                    never:
                      $ref: '#/components/schemas/MCPToolFilter'
                  additionalProperties: false
                - type: string
                  title: MCP tool approval setting
                  description: |
                    Specify a single approval policy for all tools. One of `always` or
                    `never`. When set to `always`, all tools will require approval. When
                    set to `never`, all tools will not require approval.
                  enum:
                    - always
                    - never
            - type: 'null'
      required:
        - type
        - server_label
    MCPToolFilter:
      type: object
      title: MCP tool filter
      description: |
        A filter object to specify which tools are allowed.
      properties:
        tool_names:
          type: array
          title: MCP allowed tools
          items:
            type: string
          description: List of allowed tool names.
        read_only:
          type: boolean
          description: |
            Indicates whether or not a tool modifies data or is read-only. If an
            MCP server is [annotated with `readOnlyHint`](https://modelcontextprotocol.io/specification/2025-06-18/schema#toolannotations-readonlyhint),
            it will match this filter.
      required: []
      additionalProperties: false
    NoiseReductionType:
      type: string
      enum:
        - near_field
        - far_field
      description: |
        Type of noise reduction. `near_field` is for close-talking microphones such as headphones, `far_field` is for far-field microphones such as laptop or conference room microphones.
    Prompt:
      anyOf:
        - type: object
          description: |
            Reference to a prompt template and its variables.
            [Learn more](https://platform.openai.com/docs/guides/text?api-mode=responses#reusable-prompts).
          required:
            - id
          properties:
            id:
              type: string
              description: The unique identifier of the prompt template to use.
            version:
              anyOf:
                - type: string
                  description: Optional version of the prompt template.
                - type: 'null'
            variables:
              $ref: '#/components/schemas/ResponsePromptVariables'
        - type: 'null'
    RealtimeAudioFormats:
      anyOf:
        - type: object
          title: PCM audio format
          description: The PCM audio format. Only a 24kHz sample rate is supported.
          properties:
            type:
              type: string
              description: The audio format. Always `audio/pcm`.
              enum:
                - audio/pcm
            rate:
              type: integer
              description: The sample rate of the audio. Always `24000`.
              enum:
                - 24000
        - type: object
          title: PCMU audio format
          description: The G.711 -law format.
          properties:
            type:
              type: string
              description: The audio format. Always `audio/pcmu`.
              enum:
                - audio/pcmu
        - type: object
          title: PCMA audio format
          description: The G.711 A-law format.
          properties:
            type:
              type: string
              description: The audio format. Always `audio/pcma`.
              enum:
                - audio/pcma
      discriminator:
        propertyName: type
    RealtimeCreateClientSecretRequest:
      type: object
      title: Realtime client secret creation request
      description: |
        Create a session and client secret for the Realtime API. The request can specify
        either a realtime or a transcription session configuration.
        [Learn more about the Realtime API](https://platform.openai.com/docs/guides/realtime).
      properties:
        expires_after:
          type: object
          title: Client secret expiration
          description: |
            Configuration for the client secret expiration. Expiration refers to the time after which
            a client secret will no longer be valid for creating sessions. The session itself may
            continue after that time once started. A secret can be used to create multiple sessions
            until it expires.
          properties:
            anchor:
              type: string
              enum:
                - created_at
              description: |
                The anchor point for the client secret expiration, meaning that `seconds` will be added to the `created_at` time of the client secret to produce an expiration timestamp. Only `created_at` is currently supported.
              default: created_at
              x-stainless-const: true
            seconds:
              type: integer
              description: |
                The number of seconds from the anchor point to the expiration. Select a value between `10` and `7200` (2 hours). This default to 600 seconds (10 minutes) if not specified.
              minimum: 10
              maximum: 7200
              default: 600
        session:
          title: Session configuration
          description: |
            Session configuration to use for the client secret. Choose either a realtime
            session or a transcription session.
          anyOf:
            - $ref: '#/components/schemas/RealtimeSessionCreateRequestGA'
            - $ref: '#/components/schemas/RealtimeTranscriptionSessionCreateRequestGA'
          discriminator:
            propertyName: type
    RealtimeCreateClientSecretResponse:
      type: object
      title: Realtime session and client secret
      description: |
        Response from creating a session and client secret for the Realtime API.
      properties:
        value:
          type: string
          description: The generated client secret value.
        expires_at:
          type: integer
          description: Expiration timestamp for the client secret, in seconds since epoch.
        session:
          title: Session configuration
          description: |
            The session configuration for either a realtime or transcription session.
          discriminator:
            propertyName: type
          anyOf:
            - $ref: '#/components/schemas/RealtimeSessionCreateResponseGA'
            - $ref: '#/components/schemas/RealtimeTranscriptionSessionCreateResponseGA'
      required:
        - value
        - expires_at
        - session
      x-oaiMeta:
        name: Session response object
        group: realtime
        example: |
          {
            "value": "ek_68af296e8e408191a1120ab6383263c2",
            "expires_at": 1756310470,
            "session": {
              "type": "realtime",
              "object": "realtime.session",
              "id": "sess_C9CiUVUzUzYIssh3ELY1d",
              "model": "gpt-realtime-2025-08-25",
              "output_modalities": [
                "audio"
              ],
              "instructions": "You are a friendly assistant.",
              "tools": [],
              "tool_choice": "auto",
              "max_output_tokens": "inf",
              "tracing": null,
              "truncation": "auto",
              "prompt": null,
              "expires_at": 0,
              "audio": {
                "input": {
                  "format": {
                    "type": "audio/pcm",
                    "rate": 24000
                  },
                  "transcription": null,
                  "noise_reduction": null,
                  "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 200,
                    "idle_timeout_ms": null,
                    "create_response": true,
                    "interrupt_response": true
                  }
                },
                "output": {
                  "format": {
                    "type": "audio/pcm",
                    "rate": 24000
                  },
                  "voice": "alloy",
                  "speed": 1.0
                }
              },
              "include": null
            }
          }
    RealtimeFunctionTool:
      type: object
      title: Function tool
      properties:
        type:
          type: string
          enum:
            - function
          description: The type of the tool, i.e. `function`.
          x-stainless-const: true
        name:
          type: string
          description: The name of the function.
        description:
          type: string
          description: |
            The description of the function, including guidance on when and how
            to call it, and guidance about what to tell the user when calling
            (if anything).
        parameters:
          type: object
          description: Parameters of the function in JSON Schema.
    RealtimeSessionCreateRequest:
      type: object
      description: |
        A new Realtime session configuration, with an ephemeral key. Default TTL
        for keys is one minute.
      properties:
        client_secret:
          type: object
          description: Ephemeral key returned by the API.
          properties:
            value:
              type: string
              description: |
                Ephemeral key usable in client environments to authenticate connections
                to the Realtime API. Use this in client-side environments rather than
                a standard API token, which should only be used server-side.
            expires_at:
              type: integer
              description: |
                Timestamp for when the token expires. Currently, all tokens expire
                after one minute.
          required:
            - value
            - expires_at
        modalities:
          description: |
            The set of modalities the model can respond with. To disable audio,
            set this to ["text"].
          items:
            type: string
            enum:
              - text
              - audio
        instructions:
          type: string
          description: |
            The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. "be extremely succinct", "act friendly", "here are examples of good responses") and on audio behavior (e.g. "talk quickly", "inject emotion into your voice", "laugh frequently"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.
            Note that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.
        voice:
          $ref: '#/components/schemas/VoiceIdsShared'
          description: The voice the model uses to respond. Supported built-in voices are `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`, and `cedar`. Voice cannot be changed during the session once the model has responded with audio at least once.
        input_audio_format:
          type: string
          description: |
            The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
        output_audio_format:
          type: string
          description: |
            The format of output audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
        input_audio_transcription:
          type: object
          description: |
            Configuration for input audio transcription, defaults to off and can be
            set to `null` to turn off once on. Input audio transcription is not native
            to the model, since the model consumes audio directly. Transcription runs
            asynchronously and should be treated as rough guidance
            rather than the representation understood by the model.
          properties:
            model:
              type: string
              description: |
                The model to use for transcription.
        speed:
          type: number
          default: 1
          maximum: 1.5
          minimum: 0.25
          description: |
            The speed of the model's spoken response. 1.0 is the default speed. 0.25 is
            the minimum speed. 1.5 is the maximum speed. This value can only be changed
            in between model turns, not while a response is in progress.
        tracing:
          title: Tracing Configuration
          description: |
            Configuration options for tracing. Set to null to disable tracing. Once
            tracing is enabled for a session, the configuration cannot be modified.

            `auto` will create a trace for the session with default values for the
            workflow name, group id, and metadata.
          anyOf:
            - type: string
              default: auto
              description: |
                Default tracing mode for the session.
              enum:
                - auto
              x-stainless-const: true
            - type: object
              title: Tracing Configuration
              description: |
                Granular configuration for tracing.
              properties:
                workflow_name:
                  type: string
                  description: |
                    The name of the workflow to attach to this trace. This is used to
                    name the trace in the traces dashboard.
                group_id:
                  type: string
                  description: |
                    The group id to attach to this trace to enable filtering and
                    grouping in the traces dashboard.
                metadata:
                  type: object
                  description: |
                    The arbitrary metadata to attach to this trace to enable
                    filtering in the traces dashboard.
        turn_detection:
          type: object
          description: |
            Configuration for turn detection. Can be set to `null` to turn off. Server
            VAD means that the model will detect the start and end of speech based on
            audio volume and respond at the end of user speech.
          properties:
            type:
              type: string
              description: |
                Type of turn detection, only `server_vad` is currently supported.
            threshold:
              type: number
              description: |
                Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
                higher threshold will require louder audio to activate the model, and
                thus might perform better in noisy environments.
            prefix_padding_ms:
              type: integer
              description: |
                Amount of audio to include before the VAD detected speech (in
                milliseconds). Defaults to 300ms.
            silence_duration_ms:
              type: integer
              description: |
                Duration of silence to detect speech stop (in milliseconds). Defaults
                to 500ms. With shorter values the model will respond more quickly,
                but may jump in on short pauses from the user.
        tools:
          type: array
          description: Tools (functions) available to the model.
          items:
            type: object
            properties:
              type:
                type: string
                enum:
                  - function
                description: The type of the tool, i.e. `function`.
                x-stainless-const: true
              name:
                type: string
                description: The name of the function.
              description:
                type: string
                description: |
                  The description of the function, including guidance on when and how
                  to call it, and guidance about what to tell the user when calling
                  (if anything).
              parameters:
                type: object
                description: Parameters of the function in JSON Schema.
        tool_choice:
          type: string
          description: |
            How the model chooses tools. Options are `auto`, `none`, `required`, or
            specify a function.
        temperature:
          type: number
          description: |
            Sampling temperature for the model, limited to [0.6, 1.2]. Defaults to 0.8.
        max_response_output_tokens:
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
          anyOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
        truncation:
          $ref: '#/components/schemas/RealtimeTruncation'
        prompt:
          $ref: '#/components/schemas/Prompt'
      required:
        - client_secret
      x-oaiMeta:
        name: The session object
        group: realtime
        example: |
          {
            "id": "sess_001",
            "object": "realtime.session",
            "model": "gpt-realtime-2025-08-25",
            "modalities": ["audio", "text"],
            "instructions": "You are a friendly assistant.",
            "voice": "alloy",
            "input_audio_format": "pcm16",
            "output_audio_format": "pcm16",
            "input_audio_transcription": {
                "model": "whisper-1"
            },
            "turn_detection": null,
            "tools": [],
            "tool_choice": "none",
            "temperature": 0.7,
            "speed": 1.1,
            "tracing": "auto",
            "max_response_output_tokens": 200,
            "truncation": "auto",
            "prompt": null,
            "client_secret": {
              "value": "ek_abc123",
              "expires_at": 1234567890
            }
          }
    RealtimeSessionCreateRequestGA:
      type: object
      title: Realtime session configuration
      description: Realtime session object configuration.
      properties:
        type:
          type: string
          description: |
            The type of session to create. Always `realtime` for the Realtime API.
          enum:
            - realtime
          x-stainless-const: true
        output_modalities:
          type: array
          description: |
            The set of modalities the model can respond with. It defaults to `["audio"]`, indicating
            that the model will respond with audio plus a transcript. `["text"]` can be used to make
            the model respond with text only. It is not possible to request both `text` and `audio` at the same time.
          default:
            - audio
          items:
            type: string
            enum:
              - text
              - audio
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt-realtime
                - gpt-realtime-2025-08-28
                - gpt-4o-realtime-preview
                - gpt-4o-realtime-preview-2024-10-01
                - gpt-4o-realtime-preview-2024-12-17
                - gpt-4o-realtime-preview-2025-06-03
                - gpt-4o-mini-realtime-preview
                - gpt-4o-mini-realtime-preview-2024-12-17
                - gpt-realtime-mini
                - gpt-realtime-mini-2025-10-06
                - gpt-realtime-mini-2025-12-15
                - gpt-audio-mini
                - gpt-audio-mini-2025-10-06
                - gpt-audio-mini-2025-12-15
              x-stainless-nominal: false
          description: |
            The Realtime model used for this session.
        instructions:
          type: string
          description: |
            The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. "be extremely succinct", "act friendly", "here are examples of good responses") and on audio behavior (e.g. "talk quickly", "inject emotion into your voice", "laugh frequently"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.

            Note that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.
        audio:
          type: object
          description: |
            Configuration for input and output audio.
          properties:
            input:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                  description: The format of the input audio.
                transcription:
                  description: |
                    Configuration for input audio transcription, defaults to off and can be set to `null` to turn off once on. Input audio transcription is not native to the model, since the model consumes audio directly. Transcription runs asynchronously through [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription) and should be treated as guidance of input audio content rather than precisely what the model heard. The client can optionally set the language and prompt for transcription, these offer additional guidance to the transcription service.
                  $ref: '#/components/schemas/AudioTranscription'
                noise_reduction:
                  type: object
                  description: |
                    Configuration for input audio noise reduction. This can be set to `null` to turn off.
                    Noise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.
                    Filtering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.
                  properties:
                    type:
                      $ref: '#/components/schemas/NoiseReductionType'
                turn_detection:
                  $ref: '#/components/schemas/RealtimeTurnDetection'
            output:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                  description: The format of the output audio.
                voice:
                  $ref: '#/components/schemas/VoiceIdsShared'
                  default: alloy
                  description: The voice the model uses to respond. Supported built-in voices are `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`, `shimmer`, `verse`, `marin`, and `cedar`. Voice cannot be changed during the session once the model has responded with audio at least once. We recommend `marin` and `cedar` for best quality.
                speed:
                  type: number
                  default: 1
                  maximum: 1.5
                  minimum: 0.25
                  description: |
                    The speed of the model's spoken response as a multiple of the original speed.
                    1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.

                    This parameter is a post-processing adjustment to the audio after it is generated, it's
                    also possible to prompt the model to speak faster or slower.
        include:
          type: array
          items:
            type: string
            enum:
              - item.input_audio_transcription.logprobs
          description: |
            Additional fields to include in server outputs.

            `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.
        tracing:
          title: Tracing Configuration
          description: |
            Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once
            tracing is enabled for a session, the configuration cannot be modified.

            `auto` will create a trace for the session with default values for the
            workflow name, group id, and metadata.
          nullable: true
          anyOf:
            - type: string
              title: auto
              default: auto
              description: |
                Enables tracing and sets default values for tracing configuration options. Always `auto`.
              enum:
                - auto
              x-stainless-const: true
            - type: object
              title: Tracing Configuration
              description: |
                Granular configuration for tracing.
              properties:
                workflow_name:
                  type: string
                  description: |
                    The name of the workflow to attach to this trace. This is used to
                    name the trace in the Traces Dashboard.
                group_id:
                  type: string
                  description: |
                    The group id to attach to this trace to enable filtering and
                    grouping in the Traces Dashboard.
                metadata:
                  type: object
                  description: |
                    The arbitrary metadata to attach to this trace to enable
                    filtering in the Traces Dashboard.
        tools:
          type: array
          description: Tools available to the model.
          items:
            anyOf:
              - $ref: '#/components/schemas/RealtimeFunctionTool'
              - $ref: '#/components/schemas/MCPTool'
            discriminator:
              propertyName: type
        tool_choice:
          description: |
            How the model chooses tools. Provide one of the string modes or force a specific
            function/MCP tool.
          default: auto
          anyOf:
            - $ref: '#/components/schemas/ToolChoiceOptions'
            - $ref: '#/components/schemas/ToolChoiceFunction'
            - $ref: '#/components/schemas/ToolChoiceMCP'
        max_output_tokens:
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
          anyOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
        truncation:
          $ref: '#/components/schemas/RealtimeTruncation'
        prompt:
          $ref: '#/components/schemas/Prompt'
      required:
        - type
    RealtimeSessionCreateResponse:
      type: object
      title: Realtime session configuration object
      description: |
        A Realtime session configuration object.
      properties:
        id:
          type: string
          description: |
            Unique identifier for the session that looks like `sess_1234567890abcdef`.
        object:
          type: string
          description: The object type. Always `realtime.session`.
        expires_at:
          type: integer
          description: Expiration timestamp for the session, in seconds since epoch.
        include:
          type: array
          items:
            type: string
            enum:
              - item.input_audio_transcription.logprobs
          description: |
            Additional fields to include in server outputs.
            - `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.
        model:
          type: string
          description: The Realtime model used for this session.
        output_modalities:
          description: |
            The set of modalities the model can respond with. To disable audio,
            set this to ["text"].
          items:
            type: string
            enum:
              - text
              - audio
        instructions:
          type: string
          description: |
            The default system instructions (i.e. system message) prepended to model
            calls. This field allows the client to guide the model on desired
            responses. The model can be instructed on response content and format,
            (e.g. "be extremely succinct", "act friendly", "here are examples of good
            responses") and on audio behavior (e.g. "talk quickly", "inject emotion
            into your voice", "laugh frequently"). The instructions are not guaranteed
            to be followed by the model, but they provide guidance to the model on the
            desired behavior.

            Note that the server sets default instructions which will be used if this
            field is not set and are visible in the `session.created` event at the
            start of the session.
        audio:
          type: object
          description: |
            Configuration for input and output audio for the session.
          properties:
            input:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                transcription:
                  description: |
                    Configuration for input audio transcription.
                  $ref: '#/components/schemas/AudioTranscription'
                noise_reduction:
                  type: object
                  description: |
                    Configuration for input audio noise reduction.
                  properties:
                    type:
                      $ref: '#/components/schemas/NoiseReductionType'
                turn_detection:
                  type: object
                  description: |
                    Configuration for turn detection.
                  properties:
                    type:
                      type: string
                      description: |
                        Type of turn detection, only `server_vad` is currently supported.
                    threshold:
                      type: number
                    prefix_padding_ms:
                      type: integer
                    silence_duration_ms:
                      type: integer
            output:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                voice:
                  $ref: '#/components/schemas/VoiceIdsShared'
                speed:
                  type: number
        tracing:
          title: Tracing Configuration
          description: |
            Configuration options for tracing. Set to null to disable tracing. Once
            tracing is enabled for a session, the configuration cannot be modified.

            `auto` will create a trace for the session with default values for the
            workflow name, group id, and metadata.
          anyOf:
            - type: string
              default: auto
              description: |
                Default tracing mode for the session.
              enum:
                - auto
              x-stainless-const: true
            - type: object
              title: Tracing Configuration
              description: |
                Granular configuration for tracing.
              properties:
                workflow_name:
                  type: string
                  description: |
                    The name of the workflow to attach to this trace. This is used to
                    name the trace in the traces dashboard.
                group_id:
                  type: string
                  description: |
                    The group id to attach to this trace to enable filtering and
                    grouping in the traces dashboard.
                metadata:
                  type: object
                  description: |
                    The arbitrary metadata to attach to this trace to enable
                    filtering in the traces dashboard.
        turn_detection:
          type: object
          description: |
            Configuration for turn detection. Can be set to `null` to turn off. Server
            VAD means that the model will detect the start and end of speech based on
            audio volume and respond at the end of user speech.
          properties:
            type:
              type: string
              description: |
                Type of turn detection, only `server_vad` is currently supported.
            threshold:
              type: number
              description: |
                Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
                higher threshold will require louder audio to activate the model, and
                thus might perform better in noisy environments.
            prefix_padding_ms:
              type: integer
              description: |
                Amount of audio to include before the VAD detected speech (in
                milliseconds). Defaults to 300ms.
            silence_duration_ms:
              type: integer
              description: |
                Duration of silence to detect speech stop (in milliseconds). Defaults
                to 500ms. With shorter values the model will respond more quickly,
                but may jump in on short pauses from the user.
        tools:
          type: array
          description: Tools (functions) available to the model.
          items:
            $ref: '#/components/schemas/RealtimeFunctionTool'
        tool_choice:
          type: string
          description: |
            How the model chooses tools. Options are `auto`, `none`, `required`, or
            specify a function.
        max_output_tokens:
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
          anyOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
      x-oaiMeta:
        name: The session object
        group: realtime
        example: |
          {
            "id": "sess_001",
            "object": "realtime.session",
            "expires_at": 1742188264,
            "model": "gpt-realtime",
            "output_modalities": ["audio"],
            "instructions": "You are a friendly assistant.",
            "tools": [],
            "tool_choice": "none",
            "max_output_tokens": "inf",
            "tracing": "auto",
            "truncation": "auto",
            "prompt": null,
            "audio": {
              "input": {
                "format": {
                  "type": "audio/pcm",
                  "rate": 24000
                },
                "transcription": { "model": "whisper-1" },
                "noise_reduction": null,
                "turn_detection": null
              },
              "output": {
                "format": {
                  "type": "audio/pcm",
                  "rate": 24000
                },
                "voice": "alloy",
                "speed": 1.0
              }
            }
          }
    RealtimeSessionCreateResponseGA:
      type: object
      description: |
        A new Realtime session configuration, with an ephemeral key. Default TTL
        for keys is one minute.
      properties:
        client_secret:
          type: object
          description: Ephemeral key returned by the API.
          properties:
            value:
              type: string
              description: |
                Ephemeral key usable in client environments to authenticate connections to the Realtime API. Use this in client-side environments rather than a standard API token, which should only be used server-side.
            expires_at:
              type: integer
              description: |
                Timestamp for when the token expires. Currently, all tokens expire
                after one minute.
          required:
            - value
            - expires_at
        type:
          type: string
          description: |
            The type of session to create. Always `realtime` for the Realtime API.
          enum:
            - realtime
          x-stainless-const: true
        output_modalities:
          type: array
          description: |
            The set of modalities the model can respond with. It defaults to `["audio"]`, indicating
            that the model will respond with audio plus a transcript. `["text"]` can be used to make
            the model respond with text only. It is not possible to request both `text` and `audio` at the same time.
          default:
            - audio
          items:
            type: string
            enum:
              - text
              - audio
        model:
          anyOf:
            - type: string
            - type: string
              enum:
                - gpt-realtime
                - gpt-realtime-2025-08-28
                - gpt-4o-realtime-preview
                - gpt-4o-realtime-preview-2024-10-01
                - gpt-4o-realtime-preview-2024-12-17
                - gpt-4o-realtime-preview-2025-06-03
                - gpt-4o-mini-realtime-preview
                - gpt-4o-mini-realtime-preview-2024-12-17
                - gpt-realtime-mini
                - gpt-realtime-mini-2025-10-06
                - gpt-realtime-mini-2025-12-15
                - gpt-audio-mini
                - gpt-audio-mini-2025-10-06
                - gpt-audio-mini-2025-12-15
          description: |
            The Realtime model used for this session.
        instructions:
          type: string
          description: |
            The default system instructions (i.e. system message) prepended to model calls. This field allows the client to guide the model on desired responses. The model can be instructed on response content and format, (e.g. "be extremely succinct", "act friendly", "here are examples of good responses") and on audio behavior (e.g. "talk quickly", "inject emotion into your voice", "laugh frequently"). The instructions are not guaranteed to be followed by the model, but they provide guidance to the model on the desired behavior.

            Note that the server sets default instructions which will be used if this field is not set and are visible in the `session.created` event at the start of the session.
        audio:
          type: object
          description: |
            Configuration for input and output audio.
          properties:
            input:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                  description: The format of the input audio.
                transcription:
                  description: |
                    Configuration for input audio transcription, defaults to off and can be set to `null` to turn off once on. Input audio transcription is not native to the model, since the model consumes audio directly. Transcription runs asynchronously through [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription) and should be treated as guidance of input audio content rather than precisely what the model heard. The client can optionally set the language and prompt for transcription, these offer additional guidance to the transcription service.
                  $ref: '#/components/schemas/AudioTranscription'
                noise_reduction:
                  type: object
                  description: |
                    Configuration for input audio noise reduction. This can be set to `null` to turn off.
                    Noise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.
                    Filtering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.
                  properties:
                    type:
                      $ref: '#/components/schemas/NoiseReductionType'
                turn_detection:
                  $ref: '#/components/schemas/RealtimeTurnDetection'
            output:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                  description: The format of the output audio.
                voice:
                  $ref: '#/components/schemas/VoiceIdsShared'
                  default: alloy
                  description: |
                    The voice the model uses to respond. Voice cannot be changed during the
                    session once the model has responded with audio at least once. Current
                    voice options are `alloy`, `ash`, `ballad`, `coral`, `echo`, `sage`,
                    `shimmer`, `verse`, `marin`, and `cedar`. We recommend `marin` and `cedar` for
                    best quality.
                speed:
                  type: number
                  default: 1
                  maximum: 1.5
                  minimum: 0.25
                  description: |
                    The speed of the model's spoken response as a multiple of the original speed.
                    1.0 is the default speed. 0.25 is the minimum speed. 1.5 is the maximum speed. This value can only be changed in between model turns, not while a response is in progress.

                    This parameter is a post-processing adjustment to the audio after it is generated, it's
                    also possible to prompt the model to speak faster or slower.
        include:
          type: array
          items:
            type: string
            enum:
              - item.input_audio_transcription.logprobs
          description: |
            Additional fields to include in server outputs.

            `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.
        tracing:
          anyOf:
            - title: Tracing Configuration
              description: |
                Realtime API can write session traces to the [Traces Dashboard](/logs?api=traces). Set to null to disable tracing. Once
                tracing is enabled for a session, the configuration cannot be modified.

                `auto` will create a trace for the session with default values for the
                workflow name, group id, and metadata.
              anyOf:
                - type: string
                  title: auto
                  default: auto
                  description: |
                    Enables tracing and sets default values for tracing configuration options. Always `auto`.
                  enum:
                    - auto
                  x-stainless-const: true
                - type: object
                  title: Tracing Configuration
                  description: |
                    Granular configuration for tracing.
                  properties:
                    workflow_name:
                      type: string
                      description: |
                        The name of the workflow to attach to this trace. This is used to
                        name the trace in the Traces Dashboard.
                    group_id:
                      type: string
                      description: |
                        The group id to attach to this trace to enable filtering and
                        grouping in the Traces Dashboard.
                    metadata:
                      type: object
                      description: |
                        The arbitrary metadata to attach to this trace to enable
                        filtering in the Traces Dashboard.
            - type: 'null'
        tools:
          type: array
          description: Tools available to the model.
          items:
            anyOf:
              - $ref: '#/components/schemas/RealtimeFunctionTool'
              - $ref: '#/components/schemas/MCPTool'
        tool_choice:
          description: |
            How the model chooses tools. Provide one of the string modes or force a specific
            function/MCP tool.
          default: auto
          anyOf:
            - $ref: '#/components/schemas/ToolChoiceOptions'
            - $ref: '#/components/schemas/ToolChoiceFunction'
            - $ref: '#/components/schemas/ToolChoiceMCP'
        max_output_tokens:
          description: |
            Maximum number of output tokens for a single assistant response,
            inclusive of tool calls. Provide an integer between 1 and 4096 to
            limit output tokens, or `inf` for the maximum available tokens for a
            given model. Defaults to `inf`.
          anyOf:
            - type: integer
            - type: string
              enum:
                - inf
              x-stainless-const: true
        truncation:
          $ref: '#/components/schemas/RealtimeTruncation'
        prompt:
          $ref: '#/components/schemas/Prompt'
      required:
        - client_secret
        - type
      x-oaiMeta:
        name: The session object
        group: realtime
    RealtimeTranscriptionSessionCreateRequest:
      type: object
      title: Realtime transcription session configuration
      description: Realtime transcription session object configuration.
      properties:
        turn_detection:
          type: object
          description: |
            Configuration for turn detection. Can be set to `null` to turn off. Server VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.
          properties:
            type:
              type: string
              description: |
                Type of turn detection. Only `server_vad` is currently supported for transcription sessions.
              enum:
                - server_vad
            threshold:
              type: number
              description: |
                Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
                higher threshold will require louder audio to activate the model, and
                thus might perform better in noisy environments.
            prefix_padding_ms:
              type: integer
              description: |
                Amount of audio to include before the VAD detected speech (in
                milliseconds). Defaults to 300ms.
            silence_duration_ms:
              type: integer
              description: |
                Duration of silence to detect speech stop (in milliseconds). Defaults
                to 500ms. With shorter values the model will respond more quickly,
                but may jump in on short pauses from the user.
        input_audio_noise_reduction:
          type: object
          description: |
            Configuration for input audio noise reduction. This can be set to `null` to turn off.
            Noise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.
            Filtering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.
          properties:
            type:
              $ref: '#/components/schemas/NoiseReductionType'
        input_audio_format:
          type: string
          default: pcm16
          enum:
            - pcm16
            - g711_ulaw
            - g711_alaw
          description: |
            The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
            For `pcm16`, input audio must be 16-bit PCM at a 24kHz sample rate,
            single channel (mono), and little-endian byte order.
        input_audio_transcription:
          description: |
            Configuration for input audio transcription. The client can optionally set the language and prompt for transcription, these offer additional guidance to the transcription service.
          $ref: '#/components/schemas/AudioTranscription'
        include:
          type: array
          items:
            type: string
            enum:
              - item.input_audio_transcription.logprobs
          description: |
            The set of items to include in the transcription. Current available items are:
            `item.input_audio_transcription.logprobs`
    RealtimeTranscriptionSessionCreateRequestGA:
      type: object
      title: Realtime transcription session configuration
      description: Realtime transcription session object configuration.
      properties:
        type:
          type: string
          description: |
            The type of session to create. Always `transcription` for transcription sessions.
          enum:
            - transcription
          x-stainless-const: true
        audio:
          type: object
          description: |
            Configuration for input and output audio.
          properties:
            input:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                transcription:
                  description: |
                    Configuration for input audio transcription, defaults to off and can be set to `null` to turn off once on. Input audio transcription is not native to the model, since the model consumes audio directly. Transcription runs asynchronously through [the /audio/transcriptions endpoint](https://platform.openai.com/docs/api-reference/audio/createTranscription) and should be treated as guidance of input audio content rather than precisely what the model heard. The client can optionally set the language and prompt for transcription, these offer additional guidance to the transcription service.
                  $ref: '#/components/schemas/AudioTranscription'
                noise_reduction:
                  type: object
                  description: |
                    Configuration for input audio noise reduction. This can be set to `null` to turn off.
                    Noise reduction filters audio added to the input audio buffer before it is sent to VAD and the model.
                    Filtering the audio can improve VAD and turn detection accuracy (reducing false positives) and model performance by improving perception of the input audio.
                  properties:
                    type:
                      $ref: '#/components/schemas/NoiseReductionType'
                turn_detection:
                  $ref: '#/components/schemas/RealtimeTurnDetection'
        include:
          type: array
          items:
            type: string
            enum:
              - item.input_audio_transcription.logprobs
          description: |
            Additional fields to include in server outputs.

            `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.
      required:
        - type
    RealtimeTranscriptionSessionCreateResponse:
      type: object
      description: |
        A new Realtime transcription session configuration.

        When a session is created on the server via REST API, the session object
        also contains an ephemeral key. Default TTL for keys is 10 minutes. This
        property is not present when a session is updated via the WebSocket API.
      properties:
        client_secret:
          type: object
          description: |
            Ephemeral key returned by the API. Only present when the session is
            created on the server via REST API.
          properties:
            value:
              type: string
              description: |
                Ephemeral key usable in client environments to authenticate connections
                to the Realtime API. Use this in client-side environments rather than
                a standard API token, which should only be used server-side.
            expires_at:
              type: integer
              description: |
                Timestamp for when the token expires. Currently, all tokens expire
                after one minute.
          required:
            - value
            - expires_at
        modalities:
          description: |
            The set of modalities the model can respond with. To disable audio,
            set this to ["text"].
          items:
            type: string
            enum:
              - text
              - audio
        input_audio_format:
          type: string
          description: |
            The format of input audio. Options are `pcm16`, `g711_ulaw`, or `g711_alaw`.
        input_audio_transcription:
          description: |
            Configuration of the transcription model.
          $ref: '#/components/schemas/AudioTranscription'
        turn_detection:
          type: object
          description: |
            Configuration for turn detection. Can be set to `null` to turn off. Server
            VAD means that the model will detect the start and end of speech based on
            audio volume and respond at the end of user speech.
          properties:
            type:
              type: string
              description: |
                Type of turn detection, only `server_vad` is currently supported.
            threshold:
              type: number
              description: |
                Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
                higher threshold will require louder audio to activate the model, and
                thus might perform better in noisy environments.
            prefix_padding_ms:
              type: integer
              description: |
                Amount of audio to include before the VAD detected speech (in
                milliseconds). Defaults to 300ms.
            silence_duration_ms:
              type: integer
              description: |
                Duration of silence to detect speech stop (in milliseconds). Defaults
                to 500ms. With shorter values the model will respond more quickly,
                but may jump in on short pauses from the user.
      required:
        - client_secret
      x-oaiMeta:
        name: The transcription session object
        group: realtime
        example: |
          {
            "id": "sess_BBwZc7cFV3XizEyKGDCGL",
            "object": "realtime.transcription_session",
            "expires_at": 1742188264,
            "modalities": ["audio", "text"],
            "turn_detection": {
              "type": "server_vad",
              "threshold": 0.5,
              "prefix_padding_ms": 300,
              "silence_duration_ms": 200
            },
            "input_audio_format": "pcm16",
            "input_audio_transcription": {
              "model": "gpt-4o-transcribe",
              "language": null,
              "prompt": ""
            },
            "client_secret": null
          }
    RealtimeTranscriptionSessionCreateResponseGA:
      type: object
      title: Realtime transcription session configuration object
      description: |
        A Realtime transcription session configuration object.
      properties:
        type:
          type: string
          description: |
            The type of session. Always `transcription` for transcription sessions.
          enum:
            - transcription
          x-stainless-const: true
        id:
          type: string
          description: |
            Unique identifier for the session that looks like `sess_1234567890abcdef`.
        object:
          type: string
          description: The object type. Always `realtime.transcription_session`.
        expires_at:
          type: integer
          description: Expiration timestamp for the session, in seconds since epoch.
        include:
          type: array
          items:
            type: string
            enum:
              - item.input_audio_transcription.logprobs
          description: |
            Additional fields to include in server outputs.
            - `item.input_audio_transcription.logprobs`: Include logprobs for input audio transcription.
        audio:
          type: object
          description: |
            Configuration for input audio for the session.
          properties:
            input:
              type: object
              properties:
                format:
                  $ref: '#/components/schemas/RealtimeAudioFormats'
                transcription:
                  description: |
                    Configuration of the transcription model.
                  $ref: '#/components/schemas/AudioTranscription'
                noise_reduction:
                  type: object
                  description: |
                    Configuration for input audio noise reduction.
                  properties:
                    type:
                      $ref: '#/components/schemas/NoiseReductionType'
                turn_detection:
                  type: object
                  description: |
                    Configuration for turn detection. Can be set to `null` to turn off. Server
                    VAD means that the model will detect the start and end of speech based on
                    audio volume and respond at the end of user speech.
                  properties:
                    type:
                      type: string
                      description: |
                        Type of turn detection, only `server_vad` is currently supported.
                    threshold:
                      type: number
                      description: |
                        Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
                        higher threshold will require louder audio to activate the model, and
                        thus might perform better in noisy environments.
                    prefix_padding_ms:
                      type: integer
                      description: |
                        Amount of audio to include before the VAD detected speech (in
                        milliseconds). Defaults to 300ms.
                    silence_duration_ms:
                      type: integer
                      description: |
                        Duration of silence to detect speech stop (in milliseconds). Defaults
                        to 500ms. With shorter values the model will respond more quickly,
                        but may jump in on short pauses from the user.
      required:
        - type
        - id
        - object
      x-oaiMeta:
        name: The transcription session object
        group: realtime
        example: |
          {
            "id": "sess_BBwZc7cFV3XizEyKGDCGL",
            "type": "transcription",
            "object": "realtime.transcription_session",
            "expires_at": 1742188264,
            "include": ["item.input_audio_transcription.logprobs"],
            "audio": {
              "input": {
                "format": "pcm16",
                "transcription": {
                  "model": "gpt-4o-transcribe",
                  "language": null,
                  "prompt": ""
                },
                "noise_reduction": null,
                "turn_detection": {
                  "type": "server_vad",
                  "threshold": 0.5,
                  "prefix_padding_ms": 300,
                  "silence_duration_ms": 200
                }
              }
            }
          }
    RealtimeTruncation:
      title: Realtime Truncation Controls
      description: |
        When the number of tokens in a conversation exceeds the model's input token limit, the conversation be truncated, meaning messages (starting from the oldest) will not be included in the model's context. A 32k context model with 4,096 max output tokens can only include 28,224 tokens in the context before truncation occurs.

        Clients can configure truncation behavior to truncate with a lower max token limit, which is an effective way to control token usage and cost.

        Truncation will reduce the number of cached tokens on the next turn (busting the cache), since messages are dropped from the beginning of the context. However, clients can also configure truncation to retain messages up to a fraction of the maximum context size, which will reduce the need for future truncations and thus improve the cache rate.

        Truncation can be disabled entirely, which means the server will never truncate but would instead return an error if the conversation exceeds the model's input token limit.
      anyOf:
        - type: string
          description: The truncation strategy to use for the session. `auto` is the default truncation strategy. `disabled` will disable truncation and emit errors when the conversation exceeds the input token limit.
          enum:
            - auto
            - disabled
          title: RealtimeTruncationStrategy
        - type: object
          title: Retention ratio truncation
          description: Retain a fraction of the conversation tokens when the conversation exceeds the input token limit. This allows you to amortize truncations across multiple turns, which can help improve cached token usage.
          properties:
            type:
              type: string
              enum:
                - retention_ratio
              description: Use retention ratio truncation.
              x-stainless-const: true
            retention_ratio:
              type: number
              description: |
                Fraction of post-instruction conversation tokens to retain (`0.0` - `1.0`) when the conversation exceeds the input token limit. Setting this to `0.8` means that messages will be dropped until 80% of the maximum allowed tokens are used. This helps reduce the frequency of truncations and improve cache rates.
              minimum: 0
              maximum: 1
            token_limits:
              type: object
              description: Optional custom token limits for this truncation strategy. If not provided, the model's default token limits will be used.
              properties:
                post_instructions:
                  type: integer
                  description: Maximum tokens allowed in the conversation after instructions (which including tool definitions). For example, setting this to 5,000 would mean that truncation would occur when the conversation exceeds 5,000 tokens after instructions. This cannot be higher than the model's context window size minus the maximum output tokens.
                  minimum: 0
          required:
            - type
            - retention_ratio
    RealtimeTurnDetection:
      anyOf:
        - title: Realtime Turn Detection
          description: |
            Configuration for turn detection, ether Server VAD or Semantic VAD. This can be set to `null` to turn off, in which case the client must manually trigger model response.

            Server VAD means that the model will detect the start and end of speech based on audio volume and respond at the end of user speech.

            Semantic VAD is more advanced and uses a turn detection model (in conjunction with VAD) to semantically estimate whether the user has finished speaking, then dynamically sets a timeout based on this probability. For example, if user audio trails off with "uhhm", the model will score a low probability of turn end and wait longer for the user to continue speaking. This can be useful for more natural conversations, but may have a higher latency.
          discriminator:
            propertyName: type
          anyOf:
            - type: object
              title: Server VAD
              description: Server-side voice activity detection (VAD) which flips on when user speech is detected and off after a period of silence.
              required:
                - type
              properties:
                type:
                  type: string
                  default: server_vad
                  const: server_vad
                  description: |
                    Type of turn detection, `server_vad` to turn on simple Server VAD.
                threshold:
                  type: number
                  description: |
                    Used only for `server_vad` mode. Activation threshold for VAD (0.0 to 1.0), this defaults to 0.5. A
                    higher threshold will require louder audio to activate the model, and
                    thus might perform better in noisy environments.
                prefix_padding_ms:
                  type: integer
                  description: |
                    Used only for `server_vad` mode. Amount of audio to include before the VAD detected speech (in
                    milliseconds). Defaults to 300ms.
                silence_duration_ms:
                  type: integer
                  description: |
                    Used only for `server_vad` mode. Duration of silence to detect speech stop (in milliseconds). Defaults
                    to 500ms. With shorter values the model will respond more quickly,
                    but may jump in on short pauses from the user.
                create_response:
                  type: boolean
                  default: true
                  description: |
                    Whether or not to automatically generate a response when a VAD stop event occurs. If `interrupt_response` is set to `false` this may fail to create a response if the model is already responding.

                    If both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.
                interrupt_response:
                  type: boolean
                  default: true
                  description: |
                    Whether or not to automatically interrupt (cancel) any ongoing response with output to the default
                    conversation (i.e. `conversation` of `auto`) when a VAD start event occurs. If `true` then the response will be cancelled, otherwise it will continue until complete.

                    If both `create_response` and `interrupt_response` are set to `false`, the model will never respond automatically but VAD events will still be emitted.
                idle_timeout_ms:
                  anyOf:
                    - type: integer
                      minimum: 5000
                      maximum: 30000
                      description: |
                        Optional timeout after which a model response will be triggered automatically. This is
                        useful for situations in which a long pause from the user is unexpected, such as a phone
                        call. The model will effectively prompt the user to continue the conversation based
                        on the current context.

                        The timeout value will be applied after the last model response's audio has finished playing,
                        i.e. it's set to the `response.done` time plus audio playback duration.

                        An `input_audio_buffer.timeout_triggered` event (plus events
                        associated with the Response) will be emitted when the timeout is reached.
                        Idle timeout is currently only supported for `server_vad` mode.
                    - type: 'null'
            - type: object
              title: Semantic VAD
              description: Server-side semantic turn detection which uses a model to determine when the user has finished speaking.
              required:
                - type
              properties:
                type:
                  type: string
                  const: semantic_vad
                  description: |
                    Type of turn detection, `semantic_vad` to turn on Semantic VAD.
                eagerness:
                  type: string
                  default: auto
                  enum:
                    - low
                    - medium
                    - high
                    - auto
                  description: |
                    Used only for `semantic_vad` mode. The eagerness of the model to respond. `low` will wait longer for the user to continue speaking, `high` will respond more quickly. `auto` is the default and is equivalent to `medium`. `low`, `medium`, and `high` have max timeouts of 8s, 4s, and 2s respectively.
                create_response:
                  type: boolean
                  default: true
                  description: |
                    Whether or not to automatically generate a response when a VAD stop event occurs.
                interrupt_response:
                  type: boolean
                  default: true
                  description: |
                    Whether or not to automatically interrupt any ongoing response with output to the default
                    conversation (i.e. `conversation` of `auto`) when a VAD start event occurs.
        - type: 'null'
    ResponsePromptVariables:
      anyOf:
        - type: object
          title: Prompt Variables
          description: |
            Optional map of values to substitute in for variables in your
            prompt. The substitution values can either be strings, or other
            Response input types like images or files.
          x-oaiExpandable: true
          x-oaiTypeLabel: map
          additionalProperties:
            x-oaiExpandable: true
            x-oaiTypeLabel: map
            anyOf:
              - type: string
              - $ref: '#/components/schemas/InputTextContent'
              - $ref: '#/components/schemas/InputImageContent'
              - $ref: '#/components/schemas/InputFileContent'
        - type: 'null'
    SpeechAudioDeltaEvent:
      type: object
      description: Emitted for each chunk of audio data generated during speech synthesis.
      properties:
        type:
          type: string
          description: |
            The type of the event. Always `speech.audio.delta`.
          enum:
            - speech.audio.delta
          x-stainless-const: true
        audio:
          type: string
          description: |
            A chunk of Base64-encoded audio data.
      required:
        - type
        - audio
      x-oaiMeta:
        name: Stream Event (speech.audio.delta)
        group: speech
        example: |
          {
            "type": "speech.audio.delta",
            "audio": "base64-encoded-audio-data"
          }
    SpeechAudioDoneEvent:
      type: object
      description: Emitted when the speech synthesis is complete and all audio has been streamed.
      properties:
        type:
          type: string
          description: |
            The type of the event. Always `speech.audio.done`.
          enum:
            - speech.audio.done
          x-stainless-const: true
        usage:
          type: object
          description: |
            Token usage statistics for the request.
          properties:
            input_tokens:
              type: integer
              description: Number of input tokens in the prompt.
            output_tokens:
              type: integer
              description: Number of output tokens generated.
            total_tokens:
              type: integer
              description: Total number of tokens used (input + output).
          required:
            - input_tokens
            - output_tokens
            - total_tokens
      required:
        - type
        - usage
      x-oaiMeta:
        name: Stream Event (speech.audio.done)
        group: speech
        example: |
          {
            "type": "speech.audio.done",
            "usage": {
              "input_tokens": 14,
              "output_tokens": 101,
              "total_tokens": 115
            }
          }
    ToolChoiceFunction:
      type: object
      title: Function tool
      description: |
        Use this option to force the model to call a specific function.
      properties:
        type:
          type: string
          enum:
            - function
          description: For function calling, the type is always `function`.
          x-stainless-const: true
        name:
          type: string
          description: The name of the function to call.
      required:
        - type
        - name
    ToolChoiceMCP:
      type: object
      title: MCP tool
      description: |
        Use this option to force the model to call a specific tool on a remote MCP server.
      properties:
        type:
          type: string
          enum:
            - mcp
          description: For MCP tools, the type is always `mcp`.
          x-stainless-const: true
        server_label:
          type: string
          description: |
            The label of the MCP server to use.
        name:
          anyOf:
            - type: string
              description: |
                The name of the tool to call on the server.
            - type: 'null'
      required:
        - type
        - server_label
    ToolChoiceOptions:
      type: string
      title: Tool choice mode
      description: |
        Controls which (if any) tool is called by the model.

        `none` means the model will not call any tool and instead generates a message.

        `auto` means the model can pick between generating a message or calling one or
        more tools.

        `required` means the model must call one or more tools.
      enum:
        - none
        - auto
        - required
    TranscriptTextDeltaEvent:
      type: object
      description: Emitted when there is an additional text delta. This is also the first event emitted when the transcription starts. Only emitted when you [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription) with the `Stream` parameter set to `true`.
      properties:
        type:
          type: string
          description: |
            The type of the event. Always `transcript.text.delta`.
          enum:
            - transcript.text.delta
          x-stainless-const: true
        delta:
          type: string
          description: |
            The text delta that was additionally transcribed.
        logprobs:
          type: array
          description: |
            The log probabilities of the delta. Only included if you [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription) with the `include[]` parameter set to `logprobs`.
          items:
            type: object
            properties:
              token:
                type: string
                description: |
                  The token that was used to generate the log probability.
              logprob:
                type: number
                description: |
                  The log probability of the token.
              bytes:
                type: array
                items:
                  type: integer
                description: |
                  The bytes that were used to generate the log probability.
        segment_id:
          type: string
          description: |
            Identifier of the diarized segment that this delta belongs to. Only present when using `gpt-4o-transcribe-diarize`.
      required:
        - type
        - delta
      x-oaiMeta:
        name: Stream Event (transcript.text.delta)
        group: transcript
        example: |
          {
            "type": "transcript.text.delta",
            "delta": " wonderful"
          }
    TranscriptTextDoneEvent:
      type: object
      description: Emitted when the transcription is complete. Contains the complete transcription text. Only emitted when you [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription) with the `Stream` parameter set to `true`.
      properties:
        type:
          type: string
          description: |
            The type of the event. Always `transcript.text.done`.
          enum:
            - transcript.text.done
          x-stainless-const: true
        text:
          type: string
          description: |
            The text that was transcribed.
        logprobs:
          type: array
          description: |
            The log probabilities of the individual tokens in the transcription. Only included if you [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription) with the `include[]` parameter set to `logprobs`.
          items:
            type: object
            properties:
              token:
                type: string
                description: |
                  The token that was used to generate the log probability.
              logprob:
                type: number
                description: |
                  The log probability of the token.
              bytes:
                type: array
                items:
                  type: integer
                description: |
                  The bytes that were used to generate the log probability.
        usage:
          $ref: '#/components/schemas/TranscriptTextUsageTokens'
      required:
        - type
        - text
      x-oaiMeta:
        name: Stream Event (transcript.text.done)
        group: transcript
        example: |
          {
            "type": "transcript.text.done",
            "text": "I see skies of blue and clouds of white, the bright blessed days, the dark sacred nights, and I think to myself, what a wonderful world.",
            "usage": {
              "type": "tokens",
              "input_tokens": 14,
              "input_token_details": {
                "text_tokens": 10,
                "audio_tokens": 4
              },
              "output_tokens": 31,
              "total_tokens": 45
            }
          }
    TranscriptTextSegmentEvent:
      type: object
      description: |
        Emitted when a diarized transcription returns a completed segment with speaker information. Only emitted when you [create a transcription](https://platform.openai.com/docs/api-reference/audio/create-transcription) with `stream` set to `true` and `response_format` set to `diarized_json`.
      properties:
        type:
          type: string
          description: The type of the event. Always `transcript.text.segment`.
          enum:
            - transcript.text.segment
          x-stainless-const: true
        id:
          type: string
          description: Unique identifier for the segment.
        start:
          type: number
          format: float
          description: Start timestamp of the segment in seconds.
        end:
          type: number
          format: float
          description: End timestamp of the segment in seconds.
        text:
          type: string
          description: Transcript text for this segment.
        speaker:
          type: string
          description: Speaker label for this segment.
      required:
        - type
        - id
        - start
        - end
        - text
        - speaker
      x-oaiMeta:
        name: Stream Event (transcript.text.segment)
        group: transcript
        example: |
          {
            "type": "transcript.text.segment",
            "id": "seg_002",
            "start": 5.2,
            "end": 12.8,
            "text": "Hi, I need help with diarization.",
            "speaker": "A"
          }
    TranscriptTextUsageDuration:
      type: object
      title: TranscriptTextUsageDuration
      description: Usage statistics for models billed by audio input duration.
      properties:
        type:
          type: string
          enum:
            - duration
          description: The type of the usage object. Always `duration` for this variant.
          x-stainless-const: true
        seconds:
          type: number
          description: Duration of the input audio in seconds.
      required:
        - type
        - seconds
    TranscriptTextUsageTokens:
      type: object
      title: TranscriptTextUsageTokens
      description: Usage statistics for models billed by token usage.
      properties:
        type:
          type: string
          enum:
            - tokens
          description: The type of the usage object. Always `tokens` for this variant.
          x-stainless-const: true
        input_tokens:
          type: integer
          description: Number of input tokens billed for this request.
        input_token_details:
          type: object
          description: Details about the input tokens billed for this request.
          properties:
            text_tokens:
              type: integer
              description: Number of text tokens billed for this request.
            audio_tokens:
              type: integer
              description: Number of audio tokens billed for this request.
        output_tokens:
          type: integer
          description: Number of output tokens generated.
        total_tokens:
          type: integer
          description: Total number of tokens used (input + output).
      required:
        - type
        - input_tokens
        - output_tokens
        - total_tokens
    TranscriptionChunkingStrategy:
      anyOf:
        - description: 'Controls how the audio is cut into chunks. When set to `"auto"`, the server first normalizes loudness and then uses voice activity detection (VAD) to choose boundaries. `server_vad` object can be provided to tweak VAD detection parameters manually. If unset, the audio is transcribed as a single block. Required when using `gpt-4o-transcribe-diarize` for inputs longer than 30 seconds. '
          anyOf:
            - type: string
              enum:
                - auto
              default: auto
              description: |
                Automatically set chunking parameters based on the audio. Must be set to `"auto"`.
              x-stainless-const: true
            - $ref: '#/components/schemas/VadConfig'
          x-oaiTypeLabel: string
        - type: 'null'
    TranscriptionDiarizedSegment:
      type: object
      description: A segment of diarized transcript text with speaker metadata.
      properties:
        type:
          type: string
          description: |
            The type of the segment. Always `transcript.text.segment`.
          enum:
            - transcript.text.segment
          x-stainless-const: true
        id:
          type: string
          description: Unique identifier for the segment.
        start:
          type: number
          format: float
          description: Start timestamp of the segment in seconds.
        end:
          type: number
          format: float
          description: End timestamp of the segment in seconds.
        text:
          type: string
          description: Transcript text for this segment.
        speaker:
          type: string
          description: |
            Speaker label for this segment. When known speakers are provided, the label matches `known_speaker_names[]`. Otherwise speakers are labeled sequentially using capital letters (`A`, `B`, ...).
      required:
        - type
        - id
        - start
        - end
        - text
        - speaker
    TranscriptionInclude:
      type: string
      enum:
        - logprobs
    TranscriptionSegment:
      type: object
      properties:
        id:
          type: integer
          description: Unique identifier of the segment.
        seek:
          type: integer
          description: Seek offset of the segment.
        start:
          type: number
          format: float
          description: Start time of the segment in seconds.
        end:
          type: number
          format: float
          description: End time of the segment in seconds.
        text:
          type: string
          description: Text content of the segment.
        tokens:
          type: array
          items:
            type: integer
          description: Array of token IDs for the text content.
        temperature:
          type: number
          format: float
          description: Temperature parameter used for generating the segment.
        avg_logprob:
          type: number
          format: float
          description: Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.
        compression_ratio:
          type: number
          format: float
          description: Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.
        no_speech_prob:
          type: number
          format: float
          description: Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent.
      required:
        - id
        - seek
        - start
        - end
        - text
        - tokens
        - temperature
        - avg_logprob
        - compression_ratio
        - no_speech_prob
    TranscriptionWord:
      type: object
      properties:
        word:
          type: string
          description: The text content of the word.
        start:
          type: number
          format: float
          description: Start time of the word in seconds.
        end:
          type: number
          format: float
          description: End time of the word in seconds.
      required:
        - word
        - start
        - end
    VadConfig:
      type: object
      additionalProperties: false
      required:
        - type
      properties:
        type:
          type: string
          enum:
            - server_vad
          description: Must be set to `server_vad` to enable manual chunking using server side VAD.
        prefix_padding_ms:
          type: integer
          default: 300
          description: |
            Amount of audio to include before the VAD detected speech (in 
            milliseconds).
        silence_duration_ms:
          type: integer
          default: 200
          description: |
            Duration of silence to detect speech stop (in milliseconds).
            With shorter values the model will respond more quickly, 
            but may jump in on short pauses from the user.
        threshold:
          type: number
          default: 0.5
          description: |
            Sensitivity threshold (0.0 to 1.0) for voice activity detection. A 
            higher threshold will require louder audio to activate the model, and 
            thus might perform better in noisy environments.
    VoiceIdsShared:
      example: ash
      anyOf:
        - type: string
        - type: string
          enum:
            - alloy
            - ash
            - ballad
            - coral
            - echo
            - sage
            - shimmer
            - verse
            - marin
            - cedar
    VoiceResource:
      type: object
      title: Voice
      description: A custom voice that can be used for audio output.
      additionalProperties: false
      properties:
        object:
          type: string
          description: The object type, which is always `audio.voice`.
          enum:
            - audio.voice
          x-stainless-const: true
        id:
          type: string
          description: The voice identifier, which can be referenced in API endpoints.
        name:
          type: string
          description: The name of the voice.
        created_at:
          type: integer
          description: The Unix timestamp (in seconds) for when the voice was created.
      required:
        - object
        - id
        - name
        - created_at
      x-oaiMeta:
        name: The voice object
        example: |
          {
            "object": "audio.voice",
            "id": "voice_123abc",
            "name": "My new voice",
            "created_at": 1734220800
          }
    InputTextContent:
      properties:
        type:
          type: string
          enum:
            - input_text
          description: The type of the input item. Always `input_text`.
          default: input_text
          x-stainless-const: true
        text:
          type: string
          description: The text input to the model.
      type: object
      required:
        - type
        - text
      title: Input text
      description: A text input to the model.
    ImageDetail:
      type: string
      enum:
        - low
        - high
        - auto
    InputImageContent:
      properties:
        type:
          type: string
          enum:
            - input_image
          description: The type of the input item. Always `input_image`.
          default: input_image
          x-stainless-const: true
        image_url:
          anyOf:
            - type: string
              description: The URL of the image to be sent to the model. A fully qualified URL or base64 encoded image in a data URL.
            - type: 'null'
        file_id:
          anyOf:
            - type: string
              description: The ID of the file to be sent to the model.
            - type: 'null'
        detail:
          $ref: '#/components/schemas/ImageDetail'
          description: The detail level of the image to be sent to the model. One of `high`, `low`, or `auto`. Defaults to `auto`.
      type: object
      required:
        - type
        - detail
      title: Input image
      description: An image input to the model. Learn about [image inputs](https://platform.openai.com/docs/guides/vision).
    InputFileContent:
      properties:
        type:
          type: string
          enum:
            - input_file
          description: The type of the input item. Always `input_file`.
          default: input_file
          x-stainless-const: true
        file_id:
          anyOf:
            - type: string
              description: The ID of the file to be sent to the model.
            - type: 'null'
        filename:
          type: string
          description: The name of the file to be sent to the model.
        file_url:
          type: string
          description: The URL of the file to be sent to the model.
        file_data:
          type: string
          description: |
            The content of the file to be sent to the model.
      type: object
      required:
        - type
      title: Input file
      description: A file input to the model.
  securitySchemes:
    ApiKeyAuth:
      type: http
      scheme: bearer
x-oaiMeta:
  navigationGroups:
    - id: responses
      title: Responses API
    - id: webhooks
      title: Webhooks
    - id: endpoints
      title: Platform APIs
    - id: vector_stores
      title: Vector stores
    - id: chatkit
      title: ChatKit
      beta: true
    - id: containers
      title: Containers
    - id: realtime
      title: Realtime
    - id: chat
      title: Chat Completions
    - id: assistants
      title: Assistants
      beta: true
    - id: administration
      title: Administration
    - id: legacy
      title: Legacy
  groups:
    - id: responses
      title: Responses
      description: |
        OpenAI's most advanced interface for generating model responses. Supports
        text and image inputs, and text outputs. Create stateful interactions
        with the model, using the output of previous responses as input. Extend
        the model's capabilities with built-in tools for file search, web search,
        computer use, and more. Allow the model access to external systems and data
        using function calling.

        Related guides:
        - [Quickstart](https://platform.openai.com/docs/quickstart?api-mode=responses)
        - [Text inputs and outputs](https://platform.openai.com/docs/guides/text?api-mode=responses)
        - [Image inputs](https://platform.openai.com/docs/guides/images?api-mode=responses)
        - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses)
        - [Function calling](https://platform.openai.com/docs/guides/function-calling?api-mode=responses)
        - [Conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses)
        - [Extend the models with tools](https://platform.openai.com/docs/guides/tools?api-mode=responses)
      navigationGroup: responses
      sections:
        - type: endpoint
          key: createResponse
          path: create
        - type: endpoint
          key: getResponse
          path: get
        - type: endpoint
          key: deleteResponse
          path: delete
        - type: endpoint
          key: cancelResponse
          path: cancel
        - type: endpoint
          key: Compactconversation
          path: compact
        - type: endpoint
          key: listInputItems
          path: input-items
        - type: endpoint
          key: Getinputtokencounts
          path: input-tokens
        - type: object
          key: Response
          path: object
        - type: object
          key: ResponseItemList
          path: list
        - type: object
          key: CompactResource
          path: compacted-object
    - id: conversations
      title: Conversations
      description: |
        Create and manage conversations to store and retrieve conversation state across Response API calls.
      navigationGroup: responses
      sections:
        - type: endpoint
          key: createConversation
          path: create
        - type: endpoint
          key: getConversation
          path: retrieve
        - type: endpoint
          key: updateConversation
          path: update
        - type: endpoint
          key: deleteConversation
          path: delete
        - type: endpoint
          key: listConversationItems
          path: list-items
        - type: endpoint
          key: createConversationItems
          path: create-items
        - type: endpoint
          key: getConversationItem
          path: get-item
        - type: endpoint
          key: deleteConversationItem
          path: delete-item
        - type: object
          key: Conversation
          path: object
        - type: object
          key: ConversationItemList
          path: list-items-object
    - id: responses-streaming
      title: Streaming events
      description: |
        When you [create a Response](https://platform.openai.com/docs/api-reference/responses/create) with
        `stream` set to `true`, the server will emit server-sent events to the
        client as the Response is generated. This section contains the events that
        are emitted by the server.

        [Learn more about streaming responses](https://platform.openai.com/docs/guides/streaming-responses?api-mode=responses).
      navigationGroup: responses
      sections:
        - type: object
          key: ResponseCreatedEvent
          path: <auto>
        - type: object
          key: ResponseInProgressEvent
          path: <auto>
        - type: object
          key: ResponseCompletedEvent
          path: <auto>
        - type: object
          key: ResponseFailedEvent
          path: <auto>
        - type: object
          key: ResponseIncompleteEvent
          path: <auto>
        - type: object
          key: ResponseOutputItemAddedEvent
          path: <auto>
        - type: object
          key: ResponseOutputItemDoneEvent
          path: <auto>
        - type: object
          key: ResponseContentPartAddedEvent
          path: <auto>
        - type: object
          key: ResponseContentPartDoneEvent
          path: <auto>
        - type: object
          key: ResponseTextDeltaEvent
          path: response/output_text/delta
        - type: object
          key: ResponseTextDoneEvent
          path: response/output_text/done
        - type: object
          key: ResponseRefusalDeltaEvent
          path: <auto>
        - type: object
          key: ResponseRefusalDoneEvent
          path: <auto>
        - type: object
          key: ResponseFunctionCallArgumentsDeltaEvent
          path: <auto>
        - type: object
          key: ResponseFunctionCallArgumentsDoneEvent
          path: <auto>
        - type: object
          key: ResponseFileSearchCallInProgressEvent
          path: <auto>
        - type: object
          key: ResponseFileSearchCallSearchingEvent
          path: <auto>
        - type: object
          key: ResponseFileSearchCallCompletedEvent
          path: <auto>
        - type: object
          key: ResponseWebSearchCallInProgressEvent
          path: <auto>
        - type: object
          key: ResponseWebSearchCallSearchingEvent
          path: <auto>
        - type: object
          key: ResponseWebSearchCallCompletedEvent
          path: <auto>
        - type: object
          key: ResponseReasoningSummaryPartAddedEvent
          path: <auto>
        - type: object
          key: ResponseReasoningSummaryPartDoneEvent
          path: <auto>
        - type: object
          key: ResponseReasoningSummaryTextDeltaEvent
          path: <auto>
        - type: object
          key: ResponseReasoningSummaryTextDoneEvent
          path: <auto>
        - type: object
          key: ResponseReasoningTextDeltaEvent
          path: <auto>
        - type: object
          key: ResponseReasoningTextDoneEvent
          path: <auto>
        - type: object
          key: ResponseImageGenCallCompletedEvent
          path: <auto>
        - type: object
          key: ResponseImageGenCallGeneratingEvent
          path: <auto>
        - type: object
          key: ResponseImageGenCallInProgressEvent
          path: <auto>
        - type: object
          key: ResponseImageGenCallPartialImageEvent
          path: <auto>
        - type: object
          key: ResponseMCPCallArgumentsDeltaEvent
          path: <auto>
        - type: object
          key: ResponseMCPCallArgumentsDoneEvent
          path: <auto>
        - type: object
          key: ResponseMCPCallCompletedEvent
          path: <auto>
        - type: object
          key: ResponseMCPCallFailedEvent
          path: <auto>
        - type: object
          key: ResponseMCPCallInProgressEvent
          path: <auto>
        - type: object
          key: ResponseMCPListToolsCompletedEvent
          path: <auto>
        - type: object
          key: ResponseMCPListToolsFailedEvent
          path: <auto>
        - type: object
          key: ResponseMCPListToolsInProgressEvent
          path: <auto>
        - type: object
          key: ResponseCodeInterpreterCallInProgressEvent
          path: <auto>
        - type: object
          key: ResponseCodeInterpreterCallInterpretingEvent
          path: <auto>
        - type: object
          key: ResponseCodeInterpreterCallCompletedEvent
          path: <auto>
        - type: object
          key: ResponseCodeInterpreterCallCodeDeltaEvent
          path: <auto>
        - type: object
          key: ResponseCodeInterpreterCallCodeDoneEvent
          path: <auto>
        - type: object
          key: ResponseOutputTextAnnotationAddedEvent
          path: <auto>
        - type: object
          key: ResponseQueuedEvent
          path: <auto>
        - type: object
          key: ResponseCustomToolCallInputDeltaEvent
          path: <auto>
        - type: object
          key: ResponseCustomToolCallInputDoneEvent
          path: <auto>
        - type: object
          key: ResponseErrorEvent
          path: <auto>
    - id: webhook-events
      title: Webhook Events
      description: |
        Webhooks are HTTP requests sent by OpenAI to a URL you specify when certain
        events happen during the course of API usage.

        [Learn more about webhooks](https://platform.openai.com/docs/guides/webhooks).
      navigationGroup: webhooks
      sections:
        - type: object
          key: WebhookResponseCompleted
          path: <auto>
        - type: object
          key: WebhookResponseCancelled
          path: <auto>
        - type: object
          key: WebhookResponseFailed
          path: <auto>
        - type: object
          key: WebhookResponseIncomplete
          path: <auto>
        - type: object
          key: WebhookBatchCompleted
          path: <auto>
        - type: object
          key: WebhookBatchCancelled
          path: <auto>
        - type: object
          key: WebhookBatchExpired
          path: <auto>
        - type: object
          key: WebhookBatchFailed
          path: <auto>
        - type: object
          key: WebhookFineTuningJobSucceeded
          path: <auto>
        - type: object
          key: WebhookFineTuningJobFailed
          path: <auto>
        - type: object
          key: WebhookFineTuningJobCancelled
          path: <auto>
        - type: object
          key: WebhookEvalRunSucceeded
          path: <auto>
        - type: object
          key: WebhookEvalRunFailed
          path: <auto>
        - type: object
          key: WebhookEvalRunCanceled
          path: <auto>
        - type: object
          key: WebhookRealtimeCallIncoming
          path: <auto>
    - id: audio
      title: Audio
      description: |
        Learn how to turn audio into text or text into audio.

        Related guide: [Speech to text](https://platform.openai.com/docs/guides/speech-to-text)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createSpeech
          path: createSpeech
        - type: endpoint
          key: createVoice
          path: createVoice
        - type: endpoint
          key: createVoiceConsent
          path: createVoiceConsent
        - type: endpoint
          key: listVoiceConsents
          path: listVoiceConsents
        - type: endpoint
          key: getVoiceConsent
          path: getVoiceConsent
        - type: endpoint
          key: updateVoiceConsent
          path: updateVoiceConsent
        - type: endpoint
          key: deleteVoiceConsent
          path: deleteVoiceConsent
        - type: endpoint
          key: createTranscription
          path: createTranscription
        - type: endpoint
          key: createTranslation
          path: createTranslation
        - type: object
          key: VoiceResource
          path: voice-object
        - type: object
          key: VoiceConsentResource
          path: voice-consent-object
        - type: object
          key: VoiceConsentListResource
          path: voice-consent-list
        - type: object
          key: VoiceConsentDeletedResource
          path: voice-consent-deleted
        - type: object
          key: CreateTranscriptionResponseJson
          path: json-object
        - type: object
          key: CreateTranscriptionResponseDiarizedJson
          path: diarized-json-object
        - type: object
          key: CreateTranscriptionResponseVerboseJson
          path: verbose-json-object
        - type: object
          key: SpeechAudioDeltaEvent
          path: speech-audio-delta-event
        - type: object
          key: SpeechAudioDoneEvent
          path: speech-audio-done-event
        - type: object
          key: TranscriptTextDeltaEvent
          path: transcript-text-delta-event
        - type: object
          key: TranscriptTextSegmentEvent
          path: transcript-text-segment-event
        - type: object
          key: TranscriptTextDoneEvent
          path: transcript-text-done-event
    - id: videos
      title: Videos
      description: |
        Generate videos.
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createVideo
          path: create
        - type: endpoint
          key: CreateVideoRemix
          path: remix
        - type: endpoint
          key: ListVideos
          path: list
        - type: endpoint
          key: GetVideo
          path: retrieve
        - type: endpoint
          key: DeleteVideo
          path: delete
        - type: endpoint
          key: RetrieveVideoContent
          path: content
        - type: object
          key: VideoResource
          path: object
    - id: images
      title: Images
      description: |
        Given a prompt and/or an input image, the model will generate a new image.
        Related guide: [Image generation](https://platform.openai.com/docs/guides/images)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createImage
          path: create
        - type: endpoint
          key: createImageEdit
          path: createEdit
        - type: endpoint
          key: createImageVariation
          path: createVariation
        - type: object
          key: ImagesResponse
          path: object
    - id: images-streaming
      title: Image Streaming
      description: |
        Stream image generation and editing in real time with server-sent events.
        [Learn more about image streaming](https://platform.openai.com/docs/guides/image-generation).
      navigationGroup: endpoints
      sections:
        - type: object
          key: ImageGenPartialImageEvent
          path: <auto>
        - type: object
          key: ImageGenCompletedEvent
          path: <auto>
        - type: object
          key: ImageEditPartialImageEvent
          path: <auto>
        - type: object
          key: ImageEditCompletedEvent
          path: <auto>
    - id: embeddings
      title: Embeddings
      description: |
        Get a vector representation of a given input that can be easily consumed by machine learning models and algorithms.
        Related guide: [Embeddings](https://platform.openai.com/docs/guides/embeddings)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createEmbedding
          path: create
        - type: object
          key: Embedding
          path: object
    - id: chatkit
      title: ChatKit
      beta: true
      description: |
        Manage ChatKit sessions, threads, and file uploads for internal integrations.
      navigationGroup: chatkit
      sections:
        - type: endpoint
          key: CreateChatSessionMethod
          beta: true
          path: sessions/create
        - type: endpoint
          key: CancelChatSessionMethod
          beta: true
          path: sessions/cancel
        - type: endpoint
          key: ListThreadsMethod
          beta: true
          path: threads/list
        - type: endpoint
          key: GetThreadMethod
          beta: true
          path: threads/retrieve
        - type: endpoint
          key: DeleteThreadMethod
          beta: true
          path: threads/delete
        - type: endpoint
          key: ListThreadItemsMethod
          beta: true
          path: threads/list-items
        - type: object
          key: ChatSessionResource
          path: sessions/object
        - type: object
          key: ThreadResource
          path: threads/object
        - type: object
          key: ThreadItemListResource
          path: threads/item-list
    - id: evals
      title: Evals
      description: |
        Create, manage, and run evals in the OpenAI platform.
        Related guide: [Evals](https://platform.openai.com/docs/guides/evals)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createEval
          path: create
        - type: endpoint
          key: getEval
          path: get
        - type: endpoint
          key: updateEval
          path: update
        - type: endpoint
          key: deleteEval
          path: delete
        - type: endpoint
          key: listEvals
          path: list
        - type: endpoint
          key: getEvalRuns
          path: getRuns
        - type: endpoint
          key: getEvalRun
          path: getRun
        - type: endpoint
          key: createEvalRun
          path: createRun
        - type: endpoint
          key: cancelEvalRun
          path: cancelRun
        - type: endpoint
          key: deleteEvalRun
          path: deleteRun
        - type: endpoint
          key: getEvalRunOutputItem
          path: getRunOutputItem
        - type: endpoint
          key: getEvalRunOutputItems
          path: getRunOutputItems
        - type: object
          key: Eval
          path: object
        - type: object
          key: EvalRun
          path: run-object
        - type: object
          key: EvalRunOutputItem
          path: run-output-item-object
    - id: fine-tuning
      title: Fine-tuning
      description: |
        Manage fine-tuning jobs to tailor a model to your specific training data.
        Related guide: [Fine-tune models](https://platform.openai.com/docs/guides/fine-tuning)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createFineTuningJob
          path: create
        - type: endpoint
          key: listPaginatedFineTuningJobs
          path: list
        - type: endpoint
          key: listFineTuningEvents
          path: list-events
        - type: endpoint
          key: listFineTuningJobCheckpoints
          path: list-checkpoints
        - type: endpoint
          key: listFineTuningCheckpointPermissions
          path: list-permissions
        - type: endpoint
          key: createFineTuningCheckpointPermission
          path: create-permission
        - type: endpoint
          key: deleteFineTuningCheckpointPermission
          path: delete-permission
        - type: endpoint
          key: retrieveFineTuningJob
          path: retrieve
        - type: endpoint
          key: cancelFineTuningJob
          path: cancel
        - type: endpoint
          key: resumeFineTuningJob
          path: resume
        - type: endpoint
          key: pauseFineTuningJob
          path: pause
        - type: object
          key: FineTuneChatRequestInput
          path: chat-input
        - type: object
          key: FineTunePreferenceRequestInput
          path: preference-input
        - type: object
          key: FineTuneReinforcementRequestInput
          path: reinforcement-input
        - type: object
          key: FineTuningJob
          path: object
        - type: object
          key: FineTuningJobEvent
          path: event-object
        - type: object
          key: FineTuningJobCheckpoint
          path: checkpoint-object
        - type: object
          key: FineTuningCheckpointPermission
          path: permission-object
    - id: graders
      title: Graders
      description: |
        Manage and run graders in the OpenAI platform.
        Related guide: [Graders](https://platform.openai.com/docs/guides/graders)
      navigationGroup: endpoints
      sections:
        - type: object
          key: GraderStringCheck
          path: string-check
        - type: object
          key: GraderTextSimilarity
          path: text-similarity
        - type: object
          key: GraderScoreModel
          path: score-model
        - type: object
          key: GraderLabelModel
          path: label-model
        - type: object
          key: GraderPython
          path: python
        - type: object
          key: GraderMulti
          path: multi
        - type: endpoint
          key: runGrader
          path: run
        - type: endpoint
          key: validateGrader
          path: validate
          beta: true
    - id: batch
      title: Batch
      description: |
        Create large batches of API requests for asynchronous processing. The Batch API returns completions within 24 hours for a 50% discount.
        Related guide: [Batch](https://platform.openai.com/docs/guides/batch)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createBatch
          path: create
        - type: endpoint
          key: retrieveBatch
          path: retrieve
        - type: endpoint
          key: cancelBatch
          path: cancel
        - type: endpoint
          key: listBatches
          path: list
        - type: object
          key: Batch
          path: object
        - type: object
          key: BatchRequestInput
          path: request-input
        - type: object
          key: BatchRequestOutput
          path: request-output
    - id: files
      title: Files
      description: |
        Files are used to upload documents that can be used with features like [Assistants](https://platform.openai.com/docs/api-reference/assistants), [Fine-tuning](https://platform.openai.com/docs/api-reference/fine-tuning), and [Batch API](https://platform.openai.com/docs/guides/batch).
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createFile
          path: create
        - type: endpoint
          key: listFiles
          path: list
        - type: endpoint
          key: retrieveFile
          path: retrieve
        - type: endpoint
          key: deleteFile
          path: delete
        - type: endpoint
          key: downloadFile
          path: retrieve-contents
        - type: object
          key: OpenAIFile
          path: object
    - id: uploads
      title: Uploads
      description: |
        Allows you to upload large files in multiple parts.
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createUpload
          path: create
        - type: endpoint
          key: addUploadPart
          path: add-part
        - type: endpoint
          key: completeUpload
          path: complete
        - type: endpoint
          key: cancelUpload
          path: cancel
        - type: object
          key: Upload
          path: object
        - type: object
          key: UploadPart
          path: part-object
    - id: models
      title: Models
      description: |
        List and describe the various models available in the API. You can refer to the [Models](https://platform.openai.com/docs/models) documentation to understand what models are available and the differences between them.
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: listModels
          path: list
        - type: endpoint
          key: retrieveModel
          path: retrieve
        - type: endpoint
          key: deleteModel
          path: delete
        - type: object
          key: Model
          path: object
    - id: moderations
      title: Moderations
      description: |
        Given text and/or image inputs, classifies if those inputs are potentially harmful across several categories.
        Related guide: [Moderations](https://platform.openai.com/docs/guides/moderation)
      navigationGroup: endpoints
      sections:
        - type: endpoint
          key: createModeration
          path: create
        - type: object
          key: CreateModerationResponse
          path: object
    - id: vector-stores
      title: Vector stores
      description: |
        Vector stores power semantic search for the Retrieval API and the `file_search` tool in the Responses and Assistants APIs.

        Related guide: [File Search](https://platform.openai.com/docs/assistants/tools/file-search)
      navigationGroup: vector_stores
      sections:
        - type: endpoint
          key: createVectorStore
          path: create
        - type: endpoint
          key: listVectorStores
          path: list
        - type: endpoint
          key: getVectorStore
          path: retrieve
        - type: endpoint
          key: modifyVectorStore
          path: modify
        - type: endpoint
          key: deleteVectorStore
          path: delete
        - type: endpoint
          key: searchVectorStore
          path: search
        - type: object
          key: VectorStoreObject
          path: object
    - id: vector-stores-files
      title: Vector store files
      description: |
        Vector store files represent files inside a vector store.

        Related guide: [File Search](https://platform.openai.com/docs/assistants/tools/file-search)
      navigationGroup: vector_stores
      sections:
        - type: endpoint
          key: createVectorStoreFile
          path: createFile
        - type: endpoint
          key: listVectorStoreFiles
          path: listFiles
        - type: endpoint
          key: getVectorStoreFile
          path: getFile
        - type: endpoint
          key: retrieveVectorStoreFileContent
          path: getContent
        - type: endpoint
          key: updateVectorStoreFileAttributes
          path: updateAttributes
        - type: endpoint
          key: deleteVectorStoreFile
          path: deleteFile
        - type: object
          key: VectorStoreFileObject
          path: file-object
    - id: vector-stores-file-batches
      title: Vector store file batches
      description: |
        Vector store file batches represent operations to add multiple files to a vector store.
        Related guide: [File Search](https://platform.openai.com/docs/assistants/tools/file-search)
      navigationGroup: vector_stores
      sections:
        - type: endpoint
          key: createVectorStoreFileBatch
          path: createBatch
        - type: endpoint
          key: getVectorStoreFileBatch
          path: getBatch
        - type: endpoint
          key: cancelVectorStoreFileBatch
          path: cancelBatch
        - type: endpoint
          key: listFilesInVectorStoreBatch
          path: listBatchFiles
        - type: object
          key: VectorStoreFileBatchObject
          path: batch-object
    - id: containers
      title: Containers
      description: |
        Create and manage containers for use with the Code Interpreter tool.
      navigationGroup: containers
      sections:
        - type: endpoint
          key: CreateContainer
          path: createContainers
        - type: endpoint
          key: ListContainers
          path: listContainers
        - type: endpoint
          key: RetrieveContainer
          path: retrieveContainer
        - type: endpoint
          key: DeleteContainer
          path: deleteContainer
        - type: object
          key: ContainerResource
          path: object
    - id: container-files
      title: Container Files
      description: |
        Create and manage container files for use with the Code Interpreter tool.
      navigationGroup: containers
      sections:
        - type: endpoint
          key: CreateContainerFile
          path: createContainerFile
        - type: endpoint
          key: ListContainerFiles
          path: listContainerFiles
        - type: endpoint
          key: RetrieveContainerFile
          path: retrieveContainerFile
        - type: endpoint
          key: RetrieveContainerFileContent
          path: retrieveContainerFileContent
        - type: endpoint
          key: DeleteContainerFile
          path: deleteContainerFile
        - type: object
          key: ContainerFileResource
          path: object
    - id: realtime
      title: Realtime
      description: |
        Communicate with a multimodal model in real time over low latency interfaces
        like WebRTC, WebSocket, and SIP. Natively supports speech-to-speech
        as well as text, image, and audio inputs and outputs.

        [Learn more about the Realtime API](https://platform.openai.com/docs/guides/realtime).
      navigationGroup: realtime
      sections:
        - type: endpoint
          key: create-realtime-call
          path: create-call
    - id: realtime-sessions
      title: Client secrets
      description: |
        REST API endpoint to generate ephemeral client secrets for use in client-side
        applications. Client secrets are short-lived tokens that can be passed to a client app,
        such as a web frontend or mobile client, which grants access to the Realtime API without
        leaking your main API key. You can configure a custom TTL for each client secret.

        You can also attach session configuration options to the client secret, which will be
        applied to any sessions created using that client secret, but these can also be overridden
        by the client connection.

        [Learn more about authentication with client secrets over WebRTC](https://platform.openai.com/docs/guides/realtime-webrtc).
      navigationGroup: realtime
      sections:
        - type: endpoint
          key: create-realtime-client-secret
          path: create-realtime-client-secret
        - type: object
          key: RealtimeCreateClientSecretResponse
          path: create-secret-response
    - id: realtime-calls
      title: Calls
      description: |
        REST endpoints for controlling WebRTC or SIP calls with the Realtime API.
        Accept or reject an incoming call, transfer it to another destination, or hang up the
        call once you are finished.
      navigationGroup: realtime
      sections:
        - type: endpoint
          key: accept-realtime-call
          path: accept-call
        - type: endpoint
          key: reject-realtime-call
          path: reject-call
        - type: endpoint
          key: refer-realtime-call
          path: refer-call
        - type: endpoint
          key: hangup-realtime-call
          path: hangup-call
    - id: realtime-client-events
      title: Client events
      description: |
        These are events that the OpenAI Realtime WebSocket server will accept from the client.
      navigationGroup: realtime
      sections:
        - type: object
          key: RealtimeClientEventSessionUpdate
          path: <auto>
        - type: object
          key: RealtimeClientEventInputAudioBufferAppend
          path: <auto>
        - type: object
          key: RealtimeClientEventInputAudioBufferCommit
          path: <auto>
        - type: object
          key: RealtimeClientEventInputAudioBufferClear
          path: <auto>
        - type: object
          key: RealtimeClientEventConversationItemCreate
          path: <auto>
        - type: object
          key: RealtimeClientEventConversationItemRetrieve
          path: <auto>
        - type: object
          key: RealtimeClientEventConversationItemTruncate
          path: <auto>
        - type: object
          key: RealtimeClientEventConversationItemDelete
          path: <auto>
        - type: object
          key: RealtimeClientEventResponseCreate
          path: <auto>
        - type: object
          key: RealtimeClientEventResponseCancel
          path: <auto>
        - type: object
          key: RealtimeClientEventOutputAudioBufferClear
          path: <auto>
    - id: realtime-server-events
      title: Server events
      description: |
        These are events emitted from the OpenAI Realtime WebSocket server to the client.
      navigationGroup: realtime
      sections:
        - type: object
          key: RealtimeServerEventError
          path: <auto>
        - type: object
          key: RealtimeServerEventSessionCreated
          path: <auto>
        - type: object
          key: RealtimeServerEventSessionUpdated
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemAdded
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemDone
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemRetrieved
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemInputAudioTranscriptionCompleted
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemInputAudioTranscriptionDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemInputAudioTranscriptionSegment
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemInputAudioTranscriptionFailed
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemTruncated
          path: <auto>
        - type: object
          key: RealtimeServerEventConversationItemDeleted
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferCommitted
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferDtmfEventReceived
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferCleared
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferSpeechStarted
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferSpeechStopped
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferTimeoutTriggered
          path: <auto>
        - type: object
          key: RealtimeServerEventOutputAudioBufferStarted
          path: <auto>
        - type: object
          key: RealtimeServerEventOutputAudioBufferStopped
          path: <auto>
        - type: object
          key: RealtimeServerEventOutputAudioBufferCleared
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseCreated
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseOutputItemAdded
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseOutputItemDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseContentPartAdded
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseContentPartDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseTextDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseTextDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioTranscriptDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioTranscriptDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseAudioDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseFunctionCallArgumentsDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseFunctionCallArgumentsDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseMCPCallArgumentsDelta
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseMCPCallArgumentsDone
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseMCPCallInProgress
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseMCPCallCompleted
          path: <auto>
        - type: object
          key: RealtimeServerEventResponseMCPCallFailed
          path: <auto>
        - type: object
          key: RealtimeServerEventMCPListToolsInProgress
          path: <auto>
        - type: object
          key: RealtimeServerEventMCPListToolsCompleted
          path: <auto>
        - type: object
          key: RealtimeServerEventMCPListToolsFailed
          path: <auto>
        - type: object
          key: RealtimeServerEventRateLimitsUpdated
          path: <auto>
    - id: chat
      title: Chat Completions
      description: |
        The Chat Completions API endpoint will generate a model response from a
        list of messages comprising a conversation.

        Related guides:
        - [Quickstart](https://platform.openai.com/docs/quickstart?api-mode=chat)
        - [Text inputs and outputs](https://platform.openai.com/docs/guides/text?api-mode=chat)
        - [Image inputs](https://platform.openai.com/docs/guides/images?api-mode=chat)
        - [Audio inputs and outputs](https://platform.openai.com/docs/guides/audio?api-mode=chat)
        - [Structured Outputs](https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat)
        - [Function calling](https://platform.openai.com/docs/guides/function-calling?api-mode=chat)
        - [Conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=chat)

        **Starting a new project?** We recommend trying [Responses](https://platform.openai.com/docs/api-reference/responses)
        to take advantage of the latest OpenAI platform features. Compare
        [Chat Completions with Responses](https://platform.openai.com/docs/guides/responses-vs-chat-completions?api-mode=responses).
      navigationGroup: chat
      sections:
        - type: endpoint
          key: createChatCompletion
          path: create
        - type: endpoint
          key: getChatCompletion
          path: get
        - type: endpoint
          key: getChatCompletionMessages
          path: getMessages
        - type: endpoint
          key: listChatCompletions
          path: list
        - type: endpoint
          key: updateChatCompletion
          path: update
        - type: endpoint
          key: deleteChatCompletion
          path: delete
        - type: object
          key: CreateChatCompletionResponse
          path: object
        - type: object
          key: ChatCompletionList
          path: list-object
        - type: object
          key: ChatCompletionMessageList
          path: message-list
    - id: chat-streaming
      title: Streaming
      description: |
        Stream Chat Completions in real time. Receive chunks of completions
        returned from the model using server-sent events.
        [Learn more](https://platform.openai.com/docs/guides/streaming-responses?api-mode=chat).
      navigationGroup: chat
      sections:
        - type: object
          key: CreateChatCompletionStreamResponse
          path: streaming
    - id: assistants
      title: Assistants
      beta: true
      description: |
        Build assistants that can call models and use tools to perform tasks.

        [Get started with the Assistants API](https://platform.openai.com/docs/assistants)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createAssistant
          path: createAssistant
        - type: endpoint
          key: listAssistants
          path: listAssistants
        - type: endpoint
          key: getAssistant
          path: getAssistant
        - type: endpoint
          key: modifyAssistant
          path: modifyAssistant
        - type: endpoint
          key: deleteAssistant
          path: deleteAssistant
        - type: object
          key: AssistantObject
          path: object
    - id: threads
      title: Threads
      beta: true
      description: |
        Create threads that assistants can interact with.

        Related guide: [Assistants](https://platform.openai.com/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createThread
          path: createThread
        - type: endpoint
          key: getThread
          path: getThread
        - type: endpoint
          key: modifyThread
          path: modifyThread
        - type: endpoint
          key: deleteThread
          path: deleteThread
        - type: object
          key: ThreadObject
          path: object
    - id: messages
      title: Messages
      beta: true
      description: |
        Create messages within threads

        Related guide: [Assistants](https://platform.openai.com/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createMessage
          path: createMessage
        - type: endpoint
          key: listMessages
          path: listMessages
        - type: endpoint
          key: getMessage
          path: getMessage
        - type: endpoint
          key: modifyMessage
          path: modifyMessage
        - type: endpoint
          key: deleteMessage
          path: deleteMessage
        - type: object
          key: MessageObject
          path: object
    - id: runs
      title: Runs
      beta: true
      description: |
        Represents an execution run on a thread.

        Related guide: [Assistants](https://platform.openai.com/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: createRun
          path: createRun
        - type: endpoint
          key: createThreadAndRun
          path: createThreadAndRun
        - type: endpoint
          key: listRuns
          path: listRuns
        - type: endpoint
          key: getRun
          path: getRun
        - type: endpoint
          key: modifyRun
          path: modifyRun
        - type: endpoint
          key: submitToolOuputsToRun
          path: submitToolOutputs
        - type: endpoint
          key: cancelRun
          path: cancelRun
        - type: object
          key: RunObject
          path: object
    - id: run-steps
      title: Run steps
      beta: true
      description: |
        Represents the steps (model and tool calls) taken during the run.

        Related guide: [Assistants](https://platform.openai.com/docs/assistants/overview)
      navigationGroup: assistants
      sections:
        - type: endpoint
          key: listRunSteps
          path: listRunSteps
        - type: endpoint
          key: getRunStep
          path: getRunStep
        - type: object
          key: RunStepObject
          path: step-object
    - id: assistants-streaming
      title: Streaming
      beta: true
      description: |
        Stream the result of executing a Run or resuming a Run after submitting tool outputs.
        You can stream events from the [Create Thread and Run](https://platform.openai.com/docs/api-reference/runs/createThreadAndRun),
        [Create Run](https://platform.openai.com/docs/api-reference/runs/createRun), and [Submit Tool Outputs](https://platform.openai.com/docs/api-reference/runs/submitToolOutputs)
        endpoints by passing `"stream": true`. The response will be a [Server-Sent events](https://html.spec.whatwg.org/multipage/server-sent-events.html#server-sent-events) stream.
        Our Node and Python SDKs provide helpful utilities to make streaming easy. Reference the
        [Assistants API quickstart](https://platform.openai.com/docs/assistants/overview) to learn more.
      navigationGroup: assistants
      sections:
        - type: object
          key: MessageDeltaObject
          path: message-delta-object
        - type: object
          key: RunStepDeltaObject
          path: run-step-delta-object
        - type: object
          key: AssistantStreamEvent
          path: events
    - id: administration
      title: Administration
      description: |
        Programmatically manage your organization.
        The Audit Logs endpoint provides a log of all actions taken in the organization for security and monitoring purposes.
        To access these endpoints please generate an Admin API Key through the [API Platform Organization overview](/organization/admin-keys). Admin API keys cannot be used for non-administration endpoints.
        For best practices on setting up your organization, please refer to this [guide](https://platform.openai.com/docs/guides/production-best-practices#setting-up-your-organization)
      navigationGroup: administration
    - id: admin-api-keys
      title: Admin API Keys
      description: |
        Admin API keys enable Organization Owners to programmatically manage various aspects of their organization, including users, projects, and API keys. These keys provide administrative capabilities, such as creating, updating, and deleting users; managing projects; and overseeing API key lifecycles.

        Key Features of Admin API Keys:

        - User Management: Invite new users, update roles, and remove users from the organization.

        - Project Management: Create, update, archive projects, and manage user assignments within projects.

        - API Key Oversight: List, retrieve, and delete API keys associated with projects.

        Only Organization Owners have the authority to create and utilize Admin API keys. To manage these keys, Organization Owners can navigate to the Admin Keys section of their API Platform dashboard.

        For direct access to the Admin Keys management page, Organization Owners can use the following link:

        [https://platform.openai.com/settings/organization/admin-keys](https://platform.openai.com/settings/organization/admin-keys)

        It's crucial to handle Admin API keys with care due to their elevated permissions. Adhering to best practices, such as regular key rotation and assigning appropriate permissions, enhances security and ensures proper governance within the organization.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: admin-api-keys-list
          path: list
        - type: endpoint
          key: admin-api-keys-create
          path: create
        - type: endpoint
          key: admin-api-keys-get
          path: listget
        - type: endpoint
          key: admin-api-keys-delete
          path: delete
        - type: object
          key: AdminApiKey
          path: object
    - id: invite
      title: Invites
      description: Invite and manage invitations for an organization.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-invites
          path: list
        - type: endpoint
          key: inviteUser
          path: create
        - type: endpoint
          key: retrieve-invite
          path: retrieve
        - type: endpoint
          key: delete-invite
          path: delete
        - type: object
          key: Invite
          path: object
    - id: users
      title: Users
      description: |
        Manage users and their role in an organization.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-users
          path: list
        - type: endpoint
          key: modify-user
          path: modify
        - type: endpoint
          key: retrieve-user
          path: retrieve
        - type: endpoint
          key: delete-user
          path: delete
        - type: object
          key: User
          path: object
    - id: groups
      title: Groups
      description: |
        Manage reusable collections of users for organization-wide access control and maintain their membership.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-groups
          path: list
        - type: endpoint
          key: create-group
          path: create
        - type: endpoint
          key: update-group
          path: update
        - type: endpoint
          key: delete-group
          path: delete
        - type: endpoint
          key: list-group-users
          path: users/list
        - type: endpoint
          key: add-group-user
          path: users/add
        - type: endpoint
          key: remove-group-user
          path: users/delete
        - type: object
          key: GroupUserAssignment
          path: users/assignment-object
        - type: object
          key: Group
          path: object
    - id: roles
      title: Roles
      description: |
        Create and manage custom roles that can be assigned to groups and users at the organization or project level.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-roles
          path: list
        - type: endpoint
          key: create-role
          path: create
        - type: endpoint
          key: update-role
          path: update
        - type: endpoint
          key: delete-role
          path: delete
        - type: endpoint
          key: list-project-roles
          path: project/list
        - type: endpoint
          key: create-project-role
          path: project/create
        - type: endpoint
          key: update-project-role
          path: project/update
        - type: endpoint
          key: delete-project-role
          path: project/delete
        - type: object
          key: Role
          path: object
    - id: role-assignments
      title: Role assignments
      description: |
        Assign and remove roles for users and groups at the organization or project level.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-group-role-assignments
          path: organization/groups/list
        - type: endpoint
          key: assign-group-role
          path: organization/groups/assign
        - type: endpoint
          key: unassign-group-role
          path: organization/groups/delete
        - type: endpoint
          key: list-user-role-assignments
          path: organization/users/list
        - type: endpoint
          key: assign-user-role
          path: organization/users/assign
        - type: endpoint
          key: unassign-user-role
          path: organization/users/delete
        - type: endpoint
          key: list-project-group-role-assignments
          path: projects/groups/list
        - type: endpoint
          key: assign-project-group-role
          path: projects/groups/assign
        - type: endpoint
          key: unassign-project-group-role
          path: projects/groups/delete
        - type: endpoint
          key: list-project-user-role-assignments
          path: projects/users/list
        - type: endpoint
          key: assign-project-user-role
          path: projects/users/assign
        - type: endpoint
          key: unassign-project-user-role
          path: projects/users/delete
        - type: object
          key: GroupRoleAssignment
          path: objects/group
        - type: object
          key: UserRoleAssignment
          path: objects/user
    - id: projects
      title: Projects
      description: |
        Manage the projects within an orgnanization includes creation, updating, and archiving or projects.
        The Default project cannot be archived.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-projects
          path: list
        - type: endpoint
          key: create-project
          path: create
        - type: endpoint
          key: retrieve-project
          path: retrieve
        - type: endpoint
          key: modify-project
          path: modify
        - type: endpoint
          key: archive-project
          path: archive
        - type: object
          key: Project
          path: object
    - id: project-users
      title: Project users
      description: |
        Manage users within a project, including adding, updating roles, and removing users.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-users
          path: list
        - type: endpoint
          key: create-project-user
          path: create
        - type: endpoint
          key: retrieve-project-user
          path: retrieve
        - type: endpoint
          key: modify-project-user
          path: modify
        - type: endpoint
          key: delete-project-user
          path: delete
        - type: object
          key: ProjectUser
          path: object
    - id: project-groups
      title: Project groups
      description: |
        Manage which groups have access to a project and the role they receive.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-groups
          path: list
        - type: endpoint
          key: add-project-group
          path: add
        - type: endpoint
          key: remove-project-group
          path: delete
        - type: object
          key: ProjectGroup
          path: object
    - id: project-service-accounts
      title: Project service accounts
      description: |
        Manage service accounts within a project. A service account is a bot user that is not associated with a user.
        If a user leaves an organization, their keys and membership in projects will no longer work. Service accounts
        do not have this limitation. However, service accounts can also be deleted from a project.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-service-accounts
          path: list
        - type: endpoint
          key: create-project-service-account
          path: create
        - type: endpoint
          key: retrieve-project-service-account
          path: retrieve
        - type: endpoint
          key: delete-project-service-account
          path: delete
        - type: object
          key: ProjectServiceAccount
          path: object
    - id: project-api-keys
      title: Project API keys
      description: |
        Manage API keys for a given project. Supports listing and deleting keys for users.
        This API does not allow issuing keys for users, as users need to authorize themselves to generate keys.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-api-keys
          path: list
        - type: endpoint
          key: retrieve-project-api-key
          path: retrieve
        - type: endpoint
          key: delete-project-api-key
          path: delete
        - type: object
          key: ProjectApiKey
          path: object
    - id: project-rate-limits
      title: Project rate limits
      description: |
        Manage rate limits per model for projects. Rate limits may be configured to be equal to or lower than the organization's rate limits.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-project-rate-limits
          path: list
        - type: endpoint
          key: update-project-rate-limits
          path: update
        - type: object
          key: ProjectRateLimit
          path: object
    - id: audit-logs
      title: Audit logs
      description: |
        Logs of user actions and configuration changes within this organization.
        To log events, an Organization Owner must activate logging in the [Data Controls Settings](/settings/organization/data-controls/data-retention).
        Once activated, for security reasons, logging cannot be deactivated.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: list-audit-logs
          path: list
        - type: object
          key: AuditLog
          path: object
    - id: usage
      title: Usage
      description: |
        The **Usage API** provides detailed insights into your activity across the OpenAI API. It also includes a separate [Costs endpoint](https://platform.openai.com/docs/api-reference/usage/costs), which offers visibility into your spend, breaking down consumption by invoice line items and project IDs.

        While the Usage API delivers granular usage data, it may not always reconcile perfectly with the Costs due to minor differences in how usage and spend are recorded. For financial purposes, we recommend using the [Costs endpoint](https://platform.openai.com/docs/api-reference/usage/costs) or the [Costs tab](/settings/organization/usage) in the Usage Dashboard, which will reconcile back to your billing invoice.
      navigationGroup: administration
      sections:
        - type: endpoint
          key: usage-completions
          path: completions
        - type: object
          key: UsageCompletionsResult
          path: completions_object
        - type: endpoint
          key: usage-embeddings
          path: embeddings
        - type: object
          key: UsageEmbeddingsResult
          path: embeddings_object
        - type: endpoint
          key: usage-moderations
          path: moderations
        - type: object
          key: UsageModerationsResult
          path: moderations_object
        - type: endpoint
          key: usage-images
          path: images
        - type: object
          key: UsageImagesResult
          path: images_object
        - type: endpoint
          key: usage-audio-speeches
          path: audio_speeches
        - type: object
          key: UsageAudioSpeechesResult
          path: audio_speeches_object
        - type: endpoint
          key: usage-audio-transcriptions
          path: audio_transcriptions
        - type: object
          key: UsageAudioTranscriptionsResult
          path: audio_transcriptions_object
        - type: endpoint
          key: usage-vector-stores
          path: vector_stores
        - type: object
          key: UsageVectorStoresResult
          path: vector_stores_object
        - type: endpoint
          key: usage-code-interpreter-sessions
          path: code_interpreter_sessions
        - type: object
          key: UsageCodeInterpreterSessionsResult
          path: code_interpreter_sessions_object
        - type: endpoint
          key: usage-costs
          path: costs
        - type: object
          key: CostsResult
          path: costs_object
    - id: certificates
      beta: true
      title: Certificates
      description: |
        Manage Mutual TLS certificates across your organization and projects.

        [Learn more about Mutual TLS.](https://help.openai.com/en/articles/10876024-openai-mutual-tls-beta-program)
      navigationGroup: administration
      sections:
        - type: endpoint
          key: uploadCertificate
          path: uploadCertificate
        - type: endpoint
          key: getCertificate
          path: getCertificate
        - type: endpoint
          key: modifyCertificate
          path: modifyCertificate
        - type: endpoint
          key: deleteCertificate
          path: deleteCertificate
        - type: endpoint
          key: listOrganizationCertificates
          path: listOrganizationCertificates
        - type: endpoint
          key: listProjectCertificates
          path: listProjectCertificates
        - type: endpoint
          key: activateOrganizationCertificates
          path: activateOrganizationCertificates
        - type: endpoint
          key: deactivateOrganizationCertificates
          path: deactivateOrganizationCertificates
        - type: endpoint
          key: activateProjectCertificates
          path: activateProjectCertificates
        - type: endpoint
          key: deactivateProjectCertificates
          path: deactivateProjectCertificates
        - type: object
          key: Certificate
          path: object
    - id: completions
      title: Completions
      legacy: true
      navigationGroup: legacy
      description: |
        Given a prompt, the model will return one or more predicted completions along with the probabilities of alternative tokens at each position. Most developer should use our [Chat Completions API](https://platform.openai.com/docs/guides/text-generation#text-generation-models) to leverage our best and newest models.
      sections:
        - type: endpoint
          key: createCompletion
          path: create
        - type: object
          key: CreateCompletionResponse
          path: object
    - id: realtime_beta
      title: Realtime Beta
      legacy: true
      navigationGroup: legacy
      description: |
        Communicate with a multimodal model in real time over low latency interfaces like WebRTC, WebSocket, and SIP. Natively supports speech-to-speech as well as text, image, and audio inputs and outputs.
        [Learn more about the Realtime API](https://platform.openai.com/docs/guides/realtime).
    - id: realtime-beta-sessions
      title: Realtime Beta session tokens
      description: |
        REST API endpoint to generate ephemeral session tokens for use in client-side
        applications.
      navigationGroup: legacy
      sections:
        - type: endpoint
          key: create-realtime-session
          path: create
        - type: endpoint
          key: create-realtime-transcription-session
          path: create-transcription
        - type: object
          key: RealtimeSessionCreateResponse
          path: session_object
        - type: object
          key: RealtimeTranscriptionSessionCreateResponse
          path: transcription_session_object
    - id: realtime-beta-client-events
      title: Realtime Beta client events
      description: |
        These are events that the OpenAI Realtime WebSocket server will accept from the client.
      navigationGroup: legacy
      sections:
        - type: object
          key: RealtimeBetaClientEventSessionUpdate
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventInputAudioBufferAppend
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventInputAudioBufferCommit
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventInputAudioBufferClear
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventConversationItemCreate
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventConversationItemRetrieve
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventConversationItemTruncate
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventConversationItemDelete
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventResponseCreate
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventResponseCancel
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventTranscriptionSessionUpdate
          path: <auto>
        - type: object
          key: RealtimeBetaClientEventOutputAudioBufferClear
          path: <auto>
    - id: realtime-beta-server-events
      title: Realtime Beta server events
      description: |
        These are events emitted from the OpenAI Realtime WebSocket server to the client.
      navigationGroup: legacy
      sections:
        - type: object
          key: RealtimeBetaServerEventError
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventSessionCreated
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventSessionUpdated
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventTranscriptionSessionCreated
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventTranscriptionSessionUpdated
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemCreated
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemRetrieved
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemInputAudioTranscriptionCompleted
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemInputAudioTranscriptionDelta
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemInputAudioTranscriptionSegment
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemInputAudioTranscriptionFailed
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemTruncated
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventConversationItemDeleted
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventInputAudioBufferCommitted
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventInputAudioBufferCleared
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventInputAudioBufferSpeechStarted
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventInputAudioBufferSpeechStopped
          path: <auto>
        - type: object
          key: RealtimeServerEventInputAudioBufferTimeoutTriggered
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseCreated
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseOutputItemAdded
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseOutputItemDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseContentPartAdded
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseContentPartDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseTextDelta
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseTextDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseAudioTranscriptDelta
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseAudioTranscriptDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseAudioDelta
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseAudioDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseFunctionCallArgumentsDelta
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseFunctionCallArgumentsDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseMCPCallArgumentsDelta
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseMCPCallArgumentsDone
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseMCPCallInProgress
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseMCPCallCompleted
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventResponseMCPCallFailed
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventMCPListToolsInProgress
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventMCPListToolsCompleted
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventMCPListToolsFailed
          path: <auto>
        - type: object
          key: RealtimeBetaServerEventRateLimitsUpdated
          path: <auto>
